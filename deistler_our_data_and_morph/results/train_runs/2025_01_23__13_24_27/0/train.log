[2025-01-23 13:24:28,042][rgc][INFO] - Recording ids [1]
[2025-01-23 13:24:29,003][jax._src.xla_bridge][INFO] - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
[2025-01-23 13:24:29,016][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-01-23 13:24:33,519][rgc][INFO] - Recomputing avg_recordings
[2025-01-23 13:24:33,541][rgc][INFO] - number_of_recordings_each_scanfield [5]
[2025-01-23 13:24:34,108][rgc][INFO] - Inserted 5 recordings
[2025-01-23 13:24:34,108][rgc][INFO] - number_of_recordings_each_scanfield [5]
[2025-01-23 13:24:34,110][rgc][INFO] - currents.shape (10, 179)
[2025-01-23 13:24:34,110][rgc][INFO] - labels.shape (10, 5)
[2025-01-23 13:24:34,110][rgc][INFO] - loss_weights.shape (10, 5)
[2025-01-23 13:24:45,192][rgc][INFO] - Weight mean 0.04815898814742354
[2025-01-23 13:24:45,603][rgc][INFO] - Num train 7, num val 2, num test 1
[2025-01-23 13:24:47,958][rgc][INFO] - noise_full (10, 15, 20)
[2025-01-23 13:24:47,959][rgc][INFO] - number of training batches 7
[2025-01-23 13:24:47,959][rgc][INFO] - lr scheduling dict: {}
[2025-01-23 13:24:48,085][rgc][INFO] - Starting to train
[2025-01-23 13:24:48,086][rgc][INFO] - Number of epochs 2
[2025-01-23 13:24:48,156][rgc][INFO] - 	Applying batch grad function of batch 0
[2025-01-23 13:35:26,810][rgc][INFO] - 	Updating weights of batch 0
[2025-01-23 13:35:27,209][rgc][INFO] - 	Updating weights of batch 0
[2025-01-23 13:35:27,545][rgc][INFO] - Batch 0, avg loss per batch: 4.9884926896908555
[2025-01-23 13:35:27,547][rgc][INFO] - 	Applying batch grad function of batch 1
[2025-01-23 13:46:04,900][rgc][INFO] - 	Updating weights of batch 1
[2025-01-23 13:46:04,992][rgc][INFO] - 	Updating weights of batch 1
[2025-01-23 13:46:05,032][rgc][INFO] - Batch 1, avg loss per batch: 6.3090187615236495
[2025-01-23 13:46:05,034][rgc][INFO] - 	Applying batch grad function of batch 2
[2025-01-23 13:47:00,660][rgc][INFO] - 	Updating weights of batch 2
[2025-01-23 13:47:00,673][rgc][INFO] - 	Updating weights of batch 2
[2025-01-23 13:47:00,693][rgc][INFO] - Batch 2, avg loss per batch: 9.23457529933923
[2025-01-23 13:47:00,694][rgc][INFO] - 	Applying batch grad function of batch 3
[2025-01-23 13:47:56,297][rgc][INFO] - 	Updating weights of batch 3
[2025-01-23 13:47:56,306][rgc][INFO] - 	Updating weights of batch 3
[2025-01-23 13:47:56,323][rgc][INFO] - Batch 3, avg loss per batch: 5.469732135478193
[2025-01-23 13:47:56,324][rgc][INFO] - 	Applying batch grad function of batch 4
[2025-01-23 13:48:51,531][rgc][INFO] - 	Updating weights of batch 4
[2025-01-23 13:48:51,540][rgc][INFO] - 	Updating weights of batch 4
[2025-01-23 13:48:51,557][rgc][INFO] - Batch 4, avg loss per batch: 4.524151676134775
[2025-01-23 13:48:51,559][rgc][INFO] - 	Applying batch grad function of batch 5
[2025-01-23 13:49:47,263][rgc][INFO] - 	Updating weights of batch 5
[2025-01-23 13:49:47,274][rgc][INFO] - 	Updating weights of batch 5
[2025-01-23 13:49:47,293][rgc][INFO] - Batch 5, avg loss per batch: 7.530006395102605
[2025-01-23 13:49:47,295][rgc][INFO] - 	Applying batch grad function of batch 6
[2025-01-23 13:50:40,942][rgc][INFO] - 	Updating weights of batch 6
[2025-01-23 13:50:40,952][rgc][INFO] - 	Updating weights of batch 6
[2025-01-23 13:50:40,969][rgc][INFO] - Batch 6, avg loss per batch: 7.652526599534779
[2025-01-23 13:50:41,104][rgc][INFO] - ================= Epoch 0, loss: 45.70850355680409 ===============
[2025-01-23 13:54:12,484][rgc][INFO] - AVG rho on val data: 0.19999999999999996
[2025-01-23 13:57:32,768][rgc][INFO] - AVG rho on test data: nan
[2025-01-23 14:01:14,439][rgc][INFO] - AVG rho on train data: 0.31303215473475804
[2025-01-23 14:01:14,439][rgc][INFO] - Current best rhos: train 0.31303215473475804, val 0.19999999999999996, test nan
[2025-01-23 14:01:14,471][rgc][INFO] - 	Applying batch grad function of batch 0
[2025-01-23 14:02:09,931][rgc][INFO] - 	Updating weights of batch 0
[2025-01-23 14:02:09,942][rgc][INFO] - 	Updating weights of batch 0
[2025-01-23 14:02:09,959][rgc][INFO] - Batch 0, avg loss per batch: 7.530002327554442
[2025-01-23 14:02:09,960][rgc][INFO] - 	Applying batch grad function of batch 1
[2025-01-23 14:03:02,723][rgc][INFO] - 	Updating weights of batch 1
[2025-01-23 14:03:02,734][rgc][INFO] - 	Updating weights of batch 1
[2025-01-23 14:03:02,752][rgc][INFO] - Batch 1, avg loss per batch: 4.470374421364095
[2025-01-23 14:03:02,753][rgc][INFO] - 	Applying batch grad function of batch 2
[2025-01-23 14:03:57,465][rgc][INFO] - 	Updating weights of batch 2
[2025-01-23 14:03:57,480][rgc][INFO] - 	Updating weights of batch 2
[2025-01-23 14:03:57,507][rgc][INFO] - Batch 2, avg loss per batch: 4.944699160229
[2025-01-23 14:03:57,509][rgc][INFO] - 	Applying batch grad function of batch 3
[2025-01-23 14:04:52,892][rgc][INFO] - 	Updating weights of batch 3
[2025-01-23 14:04:52,901][rgc][INFO] - 	Updating weights of batch 3
[2025-01-23 14:04:52,918][rgc][INFO] - Batch 3, avg loss per batch: 9.923683871442629
[2025-01-23 14:04:52,920][rgc][INFO] - 	Applying batch grad function of batch 4
[2025-01-23 14:05:46,418][rgc][INFO] - 	Updating weights of batch 4
[2025-01-23 14:05:46,429][rgc][INFO] - 	Updating weights of batch 4
[2025-01-23 14:05:46,447][rgc][INFO] - Batch 4, avg loss per batch: 7.232463968618622
[2025-01-23 14:05:46,448][rgc][INFO] - 	Applying batch grad function of batch 5
[2025-01-23 14:06:40,914][rgc][INFO] - 	Updating weights of batch 5
[2025-01-23 14:06:40,924][rgc][INFO] - 	Updating weights of batch 5
[2025-01-23 14:06:40,941][rgc][INFO] - Batch 5, avg loss per batch: 7.652517423521888
[2025-01-23 14:06:40,942][rgc][INFO] - 	Applying batch grad function of batch 6
[2025-01-23 14:07:36,507][rgc][INFO] - 	Updating weights of batch 6
[2025-01-23 14:07:36,517][rgc][INFO] - 	Updating weights of batch 6
[2025-01-23 14:07:36,534][rgc][INFO] - Batch 6, avg loss per batch: 4.079946928223822
[2025-01-23 14:07:36,578][rgc][INFO] - ================= Epoch 1, loss: 45.8336881009545 ===============
[2025-01-23 14:07:53,305][rgc][INFO] - AVG rho on val data: 0.19999999999999996
[2025-01-23 14:08:05,959][rgc][INFO] - AVG rho on test data: nan
[2025-01-23 14:08:28,862][rgc][INFO] - AVG rho on train data: 0.40144835747965824
[2025-01-23 14:08:28,863][rgc][INFO] - Current best rhos: train 0.31303215473475804, val 0.19999999999999996, test nan
[2025-01-23 14:08:28,890][rgc][INFO] - Finished
