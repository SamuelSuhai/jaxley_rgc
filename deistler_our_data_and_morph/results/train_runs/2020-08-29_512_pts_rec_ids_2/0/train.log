[2025-02-13 09:16:20,324][rgc][INFO] - Recording ids [2]
[2025-02-13 09:16:21,076][jax._src.xla_bridge][INFO] - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
[2025-02-13 09:16:21,077][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-02-13 09:16:29,630][rgc][INFO] - Recomputing avg_recordings
[2025-02-13 09:16:32,264][rgc][INFO] - number_of_recordings_each_scanfield [5]
[2025-02-13 09:16:33,208][rgc][INFO] - Inserted 5 recordings
[2025-02-13 09:16:33,209][rgc][INFO] - number_of_recordings_each_scanfield [5]
[2025-02-13 09:16:33,220][rgc][INFO] - currents.shape (512, 353)
[2025-02-13 09:16:33,220][rgc][INFO] - labels.shape (512, 5)
[2025-02-13 09:16:33,220][rgc][INFO] - loss_weights.shape (512, 5)
[2025-02-13 09:16:43,011][rgc][INFO] - Weight mean of w_bc_to_rgc: 0.09999999999999999
[2025-02-13 09:16:43,705][rgc][INFO] - Num train 358, num val 90, num test 64
[2025-02-13 09:16:45,104][rgc][INFO] - noise_full (512, 15, 20)
[2025-02-13 09:16:45,104][rgc][INFO] - number of training batches 358
[2025-02-13 09:16:45,104][rgc][INFO] - lr scheduling dict: {800: 0.2}
[2025-02-13 09:16:45,187][rgc][INFO] - Starting to train
[2025-02-13 09:16:45,187][rgc][INFO] - Number of epochs 5
[2025-02-13 09:16:45,203][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 0
[2025-02-13 09:26:30,135][rgc][INFO] - 	Updating weights of batch 0
[2025-02-13 09:26:30,928][rgc][INFO] - Batch 0, avg loss per batch: 3.5946821396201893
[2025-02-13 09:26:30,929][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 1
[2025-02-13 09:36:50,090][rgc][INFO] - 	Updating weights of batch 1
[2025-02-13 09:36:50,165][rgc][INFO] - Batch 1, avg loss per batch: 8.076011671359861
[2025-02-13 09:36:50,166][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 2
[2025-02-13 09:37:10,871][rgc][INFO] - 	Updating weights of batch 2
[2025-02-13 09:37:10,933][rgc][INFO] - Batch 2, avg loss per batch: 10.711786802867014
[2025-02-13 09:37:10,934][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 3
[2025-02-13 09:37:31,695][rgc][INFO] - 	Updating weights of batch 3
[2025-02-13 09:37:31,753][rgc][INFO] - Batch 3, avg loss per batch: 12.270711693107229
[2025-02-13 09:37:31,754][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 4
[2025-02-13 09:37:52,550][rgc][INFO] - 	Updating weights of batch 4
[2025-02-13 09:37:52,609][rgc][INFO] - Batch 4, avg loss per batch: 8.866356976599668
[2025-02-13 09:37:52,610][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 5
[2025-02-13 09:38:13,317][rgc][INFO] - 	Updating weights of batch 5
[2025-02-13 09:38:13,377][rgc][INFO] - Batch 5, avg loss per batch: 2.747057963485066
[2025-02-13 09:38:13,378][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 6
[2025-02-13 09:38:34,123][rgc][INFO] - 	Updating weights of batch 6
[2025-02-13 09:38:34,185][rgc][INFO] - Batch 6, avg loss per batch: 7.421980064278987
[2025-02-13 09:38:34,186][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 7
[2025-02-13 09:38:54,933][rgc][INFO] - 	Updating weights of batch 7
[2025-02-13 09:38:54,994][rgc][INFO] - Batch 7, avg loss per batch: 3.925762414045205
[2025-02-13 09:38:54,995][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 8
[2025-02-13 09:39:15,751][rgc][INFO] - 	Updating weights of batch 8
[2025-02-13 09:39:15,808][rgc][INFO] - Batch 8, avg loss per batch: 1.592155178942444
[2025-02-13 09:39:15,809][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 9
[2025-02-13 09:39:36,553][rgc][INFO] - 	Updating weights of batch 9
[2025-02-13 09:39:36,609][rgc][INFO] - Batch 9, avg loss per batch: 8.363341491325478
[2025-02-13 09:39:36,610][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 10
[2025-02-13 09:39:57,315][rgc][INFO] - 	Updating weights of batch 10
[2025-02-13 09:39:57,373][rgc][INFO] - Batch 10, avg loss per batch: 7.256227026994261
[2025-02-13 09:39:57,374][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 11
[2025-02-13 09:40:18,156][rgc][INFO] - 	Updating weights of batch 11
[2025-02-13 09:40:18,216][rgc][INFO] - Batch 11, avg loss per batch: 4.069859609420114
[2025-02-13 09:40:18,217][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 12
[2025-02-13 09:40:38,999][rgc][INFO] - 	Updating weights of batch 12
[2025-02-13 09:40:39,056][rgc][INFO] - Batch 12, avg loss per batch: 2.919922694791139
[2025-02-13 09:40:39,056][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 13
[2025-02-13 09:40:59,758][rgc][INFO] - 	Updating weights of batch 13
[2025-02-13 09:40:59,821][rgc][INFO] - Batch 13, avg loss per batch: 5.5871795934349455
[2025-02-13 09:40:59,822][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 14
[2025-02-13 09:41:20,503][rgc][INFO] - 	Updating weights of batch 14
[2025-02-13 09:41:20,563][rgc][INFO] - Batch 14, avg loss per batch: 1.5478309467574705
[2025-02-13 09:41:20,564][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 15
[2025-02-13 09:41:41,332][rgc][INFO] - 	Updating weights of batch 15
[2025-02-13 09:41:41,394][rgc][INFO] - Batch 15, avg loss per batch: 4.11084927379691
[2025-02-13 09:41:41,395][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 16
[2025-02-13 09:42:02,096][rgc][INFO] - 	Updating weights of batch 16
[2025-02-13 09:42:02,158][rgc][INFO] - Batch 16, avg loss per batch: 6.769542348959391
[2025-02-13 09:42:02,159][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 17
[2025-02-13 09:42:22,872][rgc][INFO] - 	Updating weights of batch 17
[2025-02-13 09:42:22,934][rgc][INFO] - Batch 17, avg loss per batch: 3.004822160891671
[2025-02-13 09:42:22,935][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 18
[2025-02-13 09:42:43,669][rgc][INFO] - 	Updating weights of batch 18
[2025-02-13 09:42:43,729][rgc][INFO] - Batch 18, avg loss per batch: 2.746654456527804
[2025-02-13 09:42:43,729][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 19
[2025-02-13 09:43:04,411][rgc][INFO] - 	Updating weights of batch 19
[2025-02-13 09:43:04,469][rgc][INFO] - Batch 19, avg loss per batch: 3.2803290731873505
[2025-02-13 09:43:04,470][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 20
[2025-02-13 09:43:25,225][rgc][INFO] - 	Updating weights of batch 20
[2025-02-13 09:43:25,281][rgc][INFO] - Batch 20, avg loss per batch: 3.0623361917680887
[2025-02-13 09:43:25,282][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 21
[2025-02-13 09:43:46,025][rgc][INFO] - 	Updating weights of batch 21
[2025-02-13 09:43:46,081][rgc][INFO] - Batch 21, avg loss per batch: 1.6492828430568387
[2025-02-13 09:43:46,082][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 22
[2025-02-13 09:44:06,814][rgc][INFO] - 	Updating weights of batch 22
[2025-02-13 09:44:06,872][rgc][INFO] - Batch 22, avg loss per batch: 4.953436245106646
[2025-02-13 09:44:06,872][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 23
[2025-02-13 09:44:27,606][rgc][INFO] - 	Updating weights of batch 23
[2025-02-13 09:44:27,662][rgc][INFO] - Batch 23, avg loss per batch: 4.131948247139366
[2025-02-13 09:44:27,663][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 24
[2025-02-13 09:44:48,378][rgc][INFO] - 	Updating weights of batch 24
[2025-02-13 09:44:48,436][rgc][INFO] - Batch 24, avg loss per batch: 2.884997988504507
[2025-02-13 09:44:48,436][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 25
[2025-02-13 09:45:09,135][rgc][INFO] - 	Updating weights of batch 25
[2025-02-13 09:45:09,190][rgc][INFO] - Batch 25, avg loss per batch: 2.242557329126412
[2025-02-13 09:45:09,191][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 26
[2025-02-13 09:45:29,932][rgc][INFO] - 	Updating weights of batch 26
[2025-02-13 09:45:29,991][rgc][INFO] - Batch 26, avg loss per batch: 3.9390276559853343
[2025-02-13 09:45:29,992][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 27
[2025-02-13 09:45:50,672][rgc][INFO] - 	Updating weights of batch 27
[2025-02-13 09:45:50,731][rgc][INFO] - Batch 27, avg loss per batch: 3.567611941553359
[2025-02-13 09:45:50,731][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 28
[2025-02-13 09:46:11,497][rgc][INFO] - 	Updating weights of batch 28
[2025-02-13 09:46:11,558][rgc][INFO] - Batch 28, avg loss per batch: 2.0074988228341923
[2025-02-13 09:46:11,558][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 29
[2025-02-13 09:46:32,298][rgc][INFO] - 	Updating weights of batch 29
[2025-02-13 09:46:32,355][rgc][INFO] - Batch 29, avg loss per batch: 3.264399667044204
[2025-02-13 09:46:32,356][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 30
[2025-02-13 09:46:53,134][rgc][INFO] - 	Updating weights of batch 30
[2025-02-13 09:46:53,196][rgc][INFO] - Batch 30, avg loss per batch: 3.8322829126223157
[2025-02-13 09:46:53,197][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 31
[2025-02-13 09:47:13,934][rgc][INFO] - 	Updating weights of batch 31
[2025-02-13 09:47:13,993][rgc][INFO] - Batch 31, avg loss per batch: 1.9456064554218384
[2025-02-13 09:47:13,993][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 32
[2025-02-13 09:47:34,666][rgc][INFO] - 	Updating weights of batch 32
[2025-02-13 09:47:34,725][rgc][INFO] - Batch 32, avg loss per batch: 3.9815052190140783
[2025-02-13 09:47:34,726][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 33
[2025-02-13 09:47:55,404][rgc][INFO] - 	Updating weights of batch 33
[2025-02-13 09:47:55,457][rgc][INFO] - Batch 33, avg loss per batch: 4.018921706228215
[2025-02-13 09:47:55,458][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 34
[2025-02-13 09:48:16,218][rgc][INFO] - 	Updating weights of batch 34
[2025-02-13 09:48:16,275][rgc][INFO] - Batch 34, avg loss per batch: 4.338743902977216
[2025-02-13 09:48:16,276][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 35
[2025-02-13 09:48:37,041][rgc][INFO] - 	Updating weights of batch 35
[2025-02-13 09:48:37,101][rgc][INFO] - Batch 35, avg loss per batch: 0.5555698370183103
[2025-02-13 09:48:37,102][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 36
[2025-02-13 09:48:57,869][rgc][INFO] - 	Updating weights of batch 36
[2025-02-13 09:48:57,926][rgc][INFO] - Batch 36, avg loss per batch: 5.698731276407673
[2025-02-13 09:48:57,926][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 37
[2025-02-13 09:49:18,629][rgc][INFO] - 	Updating weights of batch 37
[2025-02-13 09:49:18,686][rgc][INFO] - Batch 37, avg loss per batch: 2.404630776655649
[2025-02-13 09:49:18,687][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 38
[2025-02-13 09:49:39,434][rgc][INFO] - 	Updating weights of batch 38
[2025-02-13 09:49:39,496][rgc][INFO] - Batch 38, avg loss per batch: 6.839945265440436
[2025-02-13 09:49:39,496][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 39
[2025-02-13 09:50:00,244][rgc][INFO] - 	Updating weights of batch 39
[2025-02-13 09:50:00,305][rgc][INFO] - Batch 39, avg loss per batch: 3.0303462255739215
[2025-02-13 09:50:00,305][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 40
[2025-02-13 09:50:21,024][rgc][INFO] - 	Updating weights of batch 40
[2025-02-13 09:50:21,081][rgc][INFO] - Batch 40, avg loss per batch: 2.3014876402398983
[2025-02-13 09:50:21,082][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 41
[2025-02-13 09:50:41,831][rgc][INFO] - 	Updating weights of batch 41
[2025-02-13 09:50:41,890][rgc][INFO] - Batch 41, avg loss per batch: 3.2443794498101437
[2025-02-13 09:50:41,891][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 42
[2025-02-13 09:51:02,643][rgc][INFO] - 	Updating weights of batch 42
[2025-02-13 09:51:02,703][rgc][INFO] - Batch 42, avg loss per batch: 3.6031868682428225
[2025-02-13 09:51:02,704][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 43
[2025-02-13 09:51:23,484][rgc][INFO] - 	Updating weights of batch 43
[2025-02-13 09:51:23,543][rgc][INFO] - Batch 43, avg loss per batch: 2.3637758513951184
[2025-02-13 09:51:23,544][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 44
[2025-02-13 09:51:44,321][rgc][INFO] - 	Updating weights of batch 44
[2025-02-13 09:51:44,380][rgc][INFO] - Batch 44, avg loss per batch: 2.588515228191881
[2025-02-13 09:51:44,381][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 45
[2025-02-13 09:52:05,067][rgc][INFO] - 	Updating weights of batch 45
[2025-02-13 09:52:05,127][rgc][INFO] - Batch 45, avg loss per batch: 3.260589862064005
[2025-02-13 09:52:05,128][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 46
[2025-02-13 09:52:25,899][rgc][INFO] - 	Updating weights of batch 46
[2025-02-13 09:52:25,955][rgc][INFO] - Batch 46, avg loss per batch: 4.103744004106012
[2025-02-13 09:52:25,956][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 47
[2025-02-13 09:52:46,723][rgc][INFO] - 	Updating weights of batch 47
[2025-02-13 09:52:46,780][rgc][INFO] - Batch 47, avg loss per batch: 2.143398559700273
[2025-02-13 09:52:46,780][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 48
[2025-02-13 09:53:07,529][rgc][INFO] - 	Updating weights of batch 48
[2025-02-13 09:53:07,585][rgc][INFO] - Batch 48, avg loss per batch: 0.8596145045809638
[2025-02-13 09:53:07,586][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 49
[2025-02-13 09:53:28,345][rgc][INFO] - 	Updating weights of batch 49
[2025-02-13 09:53:28,403][rgc][INFO] - Batch 49, avg loss per batch: 2.99645132180185
[2025-02-13 09:53:28,404][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 50
[2025-02-13 09:53:49,152][rgc][INFO] - 	Updating weights of batch 50
[2025-02-13 09:53:49,210][rgc][INFO] - Batch 50, avg loss per batch: 2.9557981613804896
[2025-02-13 09:53:49,211][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 51
[2025-02-13 09:54:09,953][rgc][INFO] - 	Updating weights of batch 51
[2025-02-13 09:54:10,007][rgc][INFO] - Batch 51, avg loss per batch: 1.3962085638454242
[2025-02-13 09:54:10,007][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 52
[2025-02-13 09:54:30,692][rgc][INFO] - 	Updating weights of batch 52
[2025-02-13 09:54:30,750][rgc][INFO] - Batch 52, avg loss per batch: 5.268280896983443
[2025-02-13 09:54:30,751][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 53
[2025-02-13 09:54:51,512][rgc][INFO] - 	Updating weights of batch 53
[2025-02-13 09:54:51,569][rgc][INFO] - Batch 53, avg loss per batch: 4.028648761381558
[2025-02-13 09:54:51,569][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 54
[2025-02-13 09:55:12,343][rgc][INFO] - 	Updating weights of batch 54
[2025-02-13 09:55:12,401][rgc][INFO] - Batch 54, avg loss per batch: 1.704588810170836
[2025-02-13 09:55:12,401][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 55
[2025-02-13 09:55:33,147][rgc][INFO] - 	Updating weights of batch 55
[2025-02-13 09:55:33,204][rgc][INFO] - Batch 55, avg loss per batch: 2.563819547022597
[2025-02-13 09:55:33,205][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 56
[2025-02-13 09:55:53,968][rgc][INFO] - 	Updating weights of batch 56
[2025-02-13 09:55:54,025][rgc][INFO] - Batch 56, avg loss per batch: 4.77756933533991
[2025-02-13 09:55:54,025][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 57
[2025-02-13 09:56:14,705][rgc][INFO] - 	Updating weights of batch 57
[2025-02-13 09:56:14,759][rgc][INFO] - Batch 57, avg loss per batch: 2.6159283976248258
[2025-02-13 09:56:14,760][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 58
[2025-02-13 09:56:35,460][rgc][INFO] - 	Updating weights of batch 58
[2025-02-13 09:56:35,518][rgc][INFO] - Batch 58, avg loss per batch: 2.402732950600066
[2025-02-13 09:56:35,518][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 59
[2025-02-13 09:56:56,278][rgc][INFO] - 	Updating weights of batch 59
[2025-02-13 09:56:56,332][rgc][INFO] - Batch 59, avg loss per batch: 2.431905907694004
[2025-02-13 09:56:56,333][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 60
[2025-02-13 09:57:17,011][rgc][INFO] - 	Updating weights of batch 60
[2025-02-13 09:57:17,070][rgc][INFO] - Batch 60, avg loss per batch: 2.9311821919049836
[2025-02-13 09:57:17,071][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 61
[2025-02-13 09:57:37,764][rgc][INFO] - 	Updating weights of batch 61
[2025-02-13 09:57:37,821][rgc][INFO] - Batch 61, avg loss per batch: 1.805044861776074
[2025-02-13 09:57:37,821][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 62
[2025-02-13 09:57:58,503][rgc][INFO] - 	Updating weights of batch 62
[2025-02-13 09:57:58,557][rgc][INFO] - Batch 62, avg loss per batch: 2.463103530852248
[2025-02-13 09:57:58,558][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 63
[2025-02-13 09:58:19,309][rgc][INFO] - 	Updating weights of batch 63
[2025-02-13 09:58:19,365][rgc][INFO] - Batch 63, avg loss per batch: 3.9079179334206433
[2025-02-13 09:58:19,366][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 64
[2025-02-13 09:58:40,071][rgc][INFO] - 	Updating weights of batch 64
[2025-02-13 09:58:40,126][rgc][INFO] - Batch 64, avg loss per batch: 2.4701250018533525
[2025-02-13 09:58:40,127][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 65
[2025-02-13 09:59:00,808][rgc][INFO] - 	Updating weights of batch 65
[2025-02-13 09:59:00,865][rgc][INFO] - Batch 65, avg loss per batch: 2.5227217036115412
[2025-02-13 09:59:00,866][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 66
[2025-02-13 09:59:21,550][rgc][INFO] - 	Updating weights of batch 66
[2025-02-13 09:59:21,608][rgc][INFO] - Batch 66, avg loss per batch: 3.1073921009692755
[2025-02-13 09:59:21,608][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 67
[2025-02-13 09:59:42,303][rgc][INFO] - 	Updating weights of batch 67
[2025-02-13 09:59:42,357][rgc][INFO] - Batch 67, avg loss per batch: 2.7501871784974474
[2025-02-13 09:59:42,358][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 68
[2025-02-13 10:00:03,043][rgc][INFO] - 	Updating weights of batch 68
[2025-02-13 10:00:03,099][rgc][INFO] - Batch 68, avg loss per batch: 3.103555412492993
[2025-02-13 10:00:03,100][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 69
[2025-02-13 10:00:23,801][rgc][INFO] - 	Updating weights of batch 69
[2025-02-13 10:00:23,859][rgc][INFO] - Batch 69, avg loss per batch: 1.4971964035845926
[2025-02-13 10:00:23,860][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 70
[2025-02-13 10:00:44,551][rgc][INFO] - 	Updating weights of batch 70
[2025-02-13 10:00:44,612][rgc][INFO] - Batch 70, avg loss per batch: 3.4377707130639275
[2025-02-13 10:00:44,614][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 71
[2025-02-13 10:01:05,296][rgc][INFO] - 	Updating weights of batch 71
[2025-02-13 10:01:05,356][rgc][INFO] - Batch 71, avg loss per batch: 1.744087202417698
[2025-02-13 10:01:05,356][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 72
[2025-02-13 10:01:26,061][rgc][INFO] - 	Updating weights of batch 72
[2025-02-13 10:01:26,120][rgc][INFO] - Batch 72, avg loss per batch: 4.804368862144633
[2025-02-13 10:01:26,121][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 73
[2025-02-13 10:01:46,803][rgc][INFO] - 	Updating weights of batch 73
[2025-02-13 10:01:46,859][rgc][INFO] - Batch 73, avg loss per batch: 2.106500946796319
[2025-02-13 10:01:46,860][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 74
[2025-02-13 10:02:07,541][rgc][INFO] - 	Updating weights of batch 74
[2025-02-13 10:02:07,600][rgc][INFO] - Batch 74, avg loss per batch: 6.429996355718849
[2025-02-13 10:02:07,600][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 75
[2025-02-13 10:02:28,290][rgc][INFO] - 	Updating weights of batch 75
[2025-02-13 10:02:28,347][rgc][INFO] - Batch 75, avg loss per batch: 5.555855430112024
[2025-02-13 10:02:28,348][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 76
[2025-02-13 10:02:49,091][rgc][INFO] - 	Updating weights of batch 76
[2025-02-13 10:02:49,149][rgc][INFO] - Batch 76, avg loss per batch: 2.117130427777705
[2025-02-13 10:02:49,150][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 77
[2025-02-13 10:03:09,825][rgc][INFO] - 	Updating weights of batch 77
[2025-02-13 10:03:09,882][rgc][INFO] - Batch 77, avg loss per batch: 2.3963435111641314
[2025-02-13 10:03:09,882][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 78
[2025-02-13 10:03:30,568][rgc][INFO] - 	Updating weights of batch 78
[2025-02-13 10:03:30,625][rgc][INFO] - Batch 78, avg loss per batch: 3.1145705286584757
[2025-02-13 10:03:30,626][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 79
[2025-02-13 10:03:51,365][rgc][INFO] - 	Updating weights of batch 79
[2025-02-13 10:03:51,423][rgc][INFO] - Batch 79, avg loss per batch: 1.7614392505396226
[2025-02-13 10:03:51,423][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 80
[2025-02-13 10:04:12,173][rgc][INFO] - 	Updating weights of batch 80
[2025-02-13 10:04:12,232][rgc][INFO] - Batch 80, avg loss per batch: 1.767201458693423
[2025-02-13 10:04:12,234][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 81
[2025-02-13 10:04:32,909][rgc][INFO] - 	Updating weights of batch 81
[2025-02-13 10:04:32,968][rgc][INFO] - Batch 81, avg loss per batch: 1.4830974516097553
[2025-02-13 10:04:32,969][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 82
[2025-02-13 10:04:53,655][rgc][INFO] - 	Updating weights of batch 82
[2025-02-13 10:04:53,710][rgc][INFO] - Batch 82, avg loss per batch: 2.9865368989376364
[2025-02-13 10:04:53,711][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 83
[2025-02-13 10:05:14,384][rgc][INFO] - 	Updating weights of batch 83
[2025-02-13 10:05:14,439][rgc][INFO] - Batch 83, avg loss per batch: 2.421578674274277
[2025-02-13 10:05:14,440][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 84
[2025-02-13 10:05:35,111][rgc][INFO] - 	Updating weights of batch 84
[2025-02-13 10:05:35,167][rgc][INFO] - Batch 84, avg loss per batch: 3.5698348465592984
[2025-02-13 10:05:35,167][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 85
[2025-02-13 10:05:55,921][rgc][INFO] - 	Updating weights of batch 85
[2025-02-13 10:05:55,979][rgc][INFO] - Batch 85, avg loss per batch: 2.886167973625072
[2025-02-13 10:05:55,980][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 86
[2025-02-13 10:06:16,670][rgc][INFO] - 	Updating weights of batch 86
[2025-02-13 10:06:16,727][rgc][INFO] - Batch 86, avg loss per batch: 2.5509238791428457
[2025-02-13 10:06:16,727][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 87
[2025-02-13 10:06:37,402][rgc][INFO] - 	Updating weights of batch 87
[2025-02-13 10:06:37,452][rgc][INFO] - Batch 87, avg loss per batch: 2.204342528718053
[2025-02-13 10:06:37,452][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 88
[2025-02-13 10:06:58,207][rgc][INFO] - 	Updating weights of batch 88
[2025-02-13 10:06:58,264][rgc][INFO] - Batch 88, avg loss per batch: 5.930369288761873
[2025-02-13 10:06:58,264][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 89
[2025-02-13 10:07:18,943][rgc][INFO] - 	Updating weights of batch 89
[2025-02-13 10:07:19,000][rgc][INFO] - Batch 89, avg loss per batch: 3.100041435253683
[2025-02-13 10:07:19,001][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 90
[2025-02-13 10:07:39,751][rgc][INFO] - 	Updating weights of batch 90
[2025-02-13 10:07:39,813][rgc][INFO] - Batch 90, avg loss per batch: 2.876108664611762
[2025-02-13 10:07:39,814][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 91
[2025-02-13 10:08:00,565][rgc][INFO] - 	Updating weights of batch 91
[2025-02-13 10:08:00,622][rgc][INFO] - Batch 91, avg loss per batch: 2.096370582842351
[2025-02-13 10:08:00,623][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 92
[2025-02-13 10:08:21,338][rgc][INFO] - 	Updating weights of batch 92
[2025-02-13 10:08:21,396][rgc][INFO] - Batch 92, avg loss per batch: 4.050361813889785
[2025-02-13 10:08:21,397][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 93
[2025-02-13 10:08:42,140][rgc][INFO] - 	Updating weights of batch 93
[2025-02-13 10:08:42,196][rgc][INFO] - Batch 93, avg loss per batch: 2.5843631913795773
[2025-02-13 10:08:42,197][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 94
[2025-02-13 10:09:02,858][rgc][INFO] - 	Updating weights of batch 94
[2025-02-13 10:09:02,914][rgc][INFO] - Batch 94, avg loss per batch: 1.6511293463696932
[2025-02-13 10:09:02,915][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 95
[2025-02-13 10:09:23,601][rgc][INFO] - 	Updating weights of batch 95
[2025-02-13 10:09:23,658][rgc][INFO] - Batch 95, avg loss per batch: 3.841365359591339
[2025-02-13 10:09:23,659][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 96
[2025-02-13 10:09:44,428][rgc][INFO] - 	Updating weights of batch 96
[2025-02-13 10:09:44,486][rgc][INFO] - Batch 96, avg loss per batch: 3.4118959350117875
[2025-02-13 10:09:44,487][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 97
[2025-02-13 10:10:05,177][rgc][INFO] - 	Updating weights of batch 97
[2025-02-13 10:10:05,235][rgc][INFO] - Batch 97, avg loss per batch: 3.0221032116922038
[2025-02-13 10:10:05,235][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 98
[2025-02-13 10:10:25,904][rgc][INFO] - 	Updating weights of batch 98
[2025-02-13 10:10:25,960][rgc][INFO] - Batch 98, avg loss per batch: 4.7615106496677075
[2025-02-13 10:10:25,961][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 99
[2025-02-13 10:10:46,635][rgc][INFO] - 	Updating weights of batch 99
[2025-02-13 10:10:46,691][rgc][INFO] - Batch 99, avg loss per batch: 1.4709202191808852
[2025-02-13 10:14:18,550][rgc][INFO] - AVG rho on val data: 0.07815010454033912
[2025-02-13 10:14:18,550][rgc][INFO] - AVG mae on val data: 0.6974247756552702
[2025-02-13 10:17:57,295][rgc][INFO] - AVG rho on test data: 0.1850810738650743
[2025-02-13 10:17:57,295][rgc][INFO] - AVG mae on test data: 0.6467846074434664
[2025-02-13 10:21:03,126][rgc][INFO] - AVG rho on train data: 0.08280787324347
[2025-02-13 10:21:03,127][rgc][INFO] - AVG mae on train data: 0.7016636681375927
[2025-02-13 10:21:03,128][rgc][INFO] - Current best rhos: train 0.08280787324347, val 0.07815010454033912, test 0.1850810738650743
[2025-02-13 10:21:03,131][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 100
[2025-02-13 10:21:23,916][rgc][INFO] - 	Updating weights of batch 100
[2025-02-13 10:21:23,974][rgc][INFO] - Batch 100, avg loss per batch: 3.725486622065038
[2025-02-13 10:21:23,975][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 101
[2025-02-13 10:21:44,680][rgc][INFO] - 	Updating weights of batch 101
[2025-02-13 10:21:44,739][rgc][INFO] - Batch 101, avg loss per batch: 2.584910322850525
[2025-02-13 10:21:44,741][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 102
[2025-02-13 10:22:05,436][rgc][INFO] - 	Updating weights of batch 102
[2025-02-13 10:22:05,495][rgc][INFO] - Batch 102, avg loss per batch: 3.5513117614785563
[2025-02-13 10:22:05,495][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 103
[2025-02-13 10:22:26,180][rgc][INFO] - 	Updating weights of batch 103
[2025-02-13 10:22:26,234][rgc][INFO] - Batch 103, avg loss per batch: 2.911143753226951
[2025-02-13 10:22:26,234][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 104
[2025-02-13 10:22:46,907][rgc][INFO] - 	Updating weights of batch 104
[2025-02-13 10:22:46,964][rgc][INFO] - Batch 104, avg loss per batch: 2.2167324140465334
[2025-02-13 10:22:46,965][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 105
[2025-02-13 10:23:07,717][rgc][INFO] - 	Updating weights of batch 105
[2025-02-13 10:23:07,777][rgc][INFO] - Batch 105, avg loss per batch: 1.3995100589667921
[2025-02-13 10:23:07,778][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 106
[2025-02-13 10:23:28,560][rgc][INFO] - 	Updating weights of batch 106
[2025-02-13 10:23:28,620][rgc][INFO] - Batch 106, avg loss per batch: 3.3958807902794086
[2025-02-13 10:23:28,621][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 107
[2025-02-13 10:23:49,381][rgc][INFO] - 	Updating weights of batch 107
[2025-02-13 10:23:49,440][rgc][INFO] - Batch 107, avg loss per batch: 1.2107279338986499
[2025-02-13 10:23:49,441][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 108
[2025-02-13 10:24:10,200][rgc][INFO] - 	Updating weights of batch 108
[2025-02-13 10:24:10,258][rgc][INFO] - Batch 108, avg loss per batch: 3.5009433045036413
[2025-02-13 10:24:10,259][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 109
[2025-02-13 10:24:30,955][rgc][INFO] - 	Updating weights of batch 109
[2025-02-13 10:24:31,010][rgc][INFO] - Batch 109, avg loss per batch: 3.2613523555752013
[2025-02-13 10:24:31,011][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 110
[2025-02-13 10:24:51,684][rgc][INFO] - 	Updating weights of batch 110
[2025-02-13 10:24:51,744][rgc][INFO] - Batch 110, avg loss per batch: 2.518805172064156
[2025-02-13 10:24:51,745][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 111
[2025-02-13 10:25:12,474][rgc][INFO] - 	Updating weights of batch 111
[2025-02-13 10:25:12,529][rgc][INFO] - Batch 111, avg loss per batch: 2.8266057160291873
[2025-02-13 10:25:12,530][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 112
[2025-02-13 10:25:33,264][rgc][INFO] - 	Updating weights of batch 112
[2025-02-13 10:25:33,323][rgc][INFO] - Batch 112, avg loss per batch: 3.601272316709244
[2025-02-13 10:25:33,324][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 113
[2025-02-13 10:25:54,011][rgc][INFO] - 	Updating weights of batch 113
[2025-02-13 10:25:54,069][rgc][INFO] - Batch 113, avg loss per batch: 6.477108307364418
[2025-02-13 10:25:54,070][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 114
[2025-02-13 10:26:14,827][rgc][INFO] - 	Updating weights of batch 114
[2025-02-13 10:26:14,887][rgc][INFO] - Batch 114, avg loss per batch: 3.287065661111836
[2025-02-13 10:26:14,887][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 115
[2025-02-13 10:26:35,654][rgc][INFO] - 	Updating weights of batch 115
[2025-02-13 10:26:35,715][rgc][INFO] - Batch 115, avg loss per batch: 3.3806714285811017
[2025-02-13 10:26:35,716][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 116
[2025-02-13 10:26:56,396][rgc][INFO] - 	Updating weights of batch 116
[2025-02-13 10:26:56,456][rgc][INFO] - Batch 116, avg loss per batch: 3.0048459576496365
[2025-02-13 10:26:56,457][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 117
[2025-02-13 10:27:17,213][rgc][INFO] - 	Updating weights of batch 117
[2025-02-13 10:27:17,274][rgc][INFO] - Batch 117, avg loss per batch: 3.6932494623344603
[2025-02-13 10:27:17,275][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 118
[2025-02-13 10:27:37,962][rgc][INFO] - 	Updating weights of batch 118
[2025-02-13 10:27:38,023][rgc][INFO] - Batch 118, avg loss per batch: 1.325032587326131
[2025-02-13 10:27:38,024][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 119
[2025-02-13 10:27:58,749][rgc][INFO] - 	Updating weights of batch 119
[2025-02-13 10:27:58,809][rgc][INFO] - Batch 119, avg loss per batch: 3.849004065020041
[2025-02-13 10:27:58,810][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 120
[2025-02-13 10:28:19,539][rgc][INFO] - 	Updating weights of batch 120
[2025-02-13 10:28:19,599][rgc][INFO] - Batch 120, avg loss per batch: 5.073659243805774
[2025-02-13 10:28:19,600][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 121
[2025-02-13 10:28:40,354][rgc][INFO] - 	Updating weights of batch 121
[2025-02-13 10:28:40,414][rgc][INFO] - Batch 121, avg loss per batch: 3.834198307129334
[2025-02-13 10:28:40,415][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 122
[2025-02-13 10:29:01,162][rgc][INFO] - 	Updating weights of batch 122
[2025-02-13 10:29:01,220][rgc][INFO] - Batch 122, avg loss per batch: 2.9997574003469234
[2025-02-13 10:29:01,221][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 123
[2025-02-13 10:29:21,951][rgc][INFO] - 	Updating weights of batch 123
[2025-02-13 10:29:22,009][rgc][INFO] - Batch 123, avg loss per batch: 3.01972006128542
[2025-02-13 10:29:22,010][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 124
[2025-02-13 10:29:42,683][rgc][INFO] - 	Updating weights of batch 124
[2025-02-13 10:29:42,742][rgc][INFO] - Batch 124, avg loss per batch: 1.4136260234036058
[2025-02-13 10:29:42,742][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 125
[2025-02-13 10:30:03,424][rgc][INFO] - 	Updating weights of batch 125
[2025-02-13 10:30:03,484][rgc][INFO] - Batch 125, avg loss per batch: 4.302497889858108
[2025-02-13 10:30:03,485][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 126
[2025-02-13 10:30:24,256][rgc][INFO] - 	Updating weights of batch 126
[2025-02-13 10:30:24,314][rgc][INFO] - Batch 126, avg loss per batch: 2.760426172814968
[2025-02-13 10:30:24,315][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 127
[2025-02-13 10:30:44,992][rgc][INFO] - 	Updating weights of batch 127
[2025-02-13 10:30:45,051][rgc][INFO] - Batch 127, avg loss per batch: 2.1439874574184334
[2025-02-13 10:30:45,052][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 128
[2025-02-13 10:31:05,752][rgc][INFO] - 	Updating weights of batch 128
[2025-02-13 10:31:05,811][rgc][INFO] - Batch 128, avg loss per batch: 4.557324336679432
[2025-02-13 10:31:05,812][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 129
[2025-02-13 10:31:26,478][rgc][INFO] - 	Updating weights of batch 129
[2025-02-13 10:31:26,537][rgc][INFO] - Batch 129, avg loss per batch: 2.973914139047772
[2025-02-13 10:31:26,538][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 130
[2025-02-13 10:31:47,293][rgc][INFO] - 	Updating weights of batch 130
[2025-02-13 10:31:47,352][rgc][INFO] - Batch 130, avg loss per batch: 3.039881173070979
[2025-02-13 10:31:47,353][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 131
[2025-02-13 10:32:08,102][rgc][INFO] - 	Updating weights of batch 131
[2025-02-13 10:32:08,156][rgc][INFO] - Batch 131, avg loss per batch: 2.0298177484435302
[2025-02-13 10:32:08,157][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 132
[2025-02-13 10:32:28,909][rgc][INFO] - 	Updating weights of batch 132
[2025-02-13 10:32:28,963][rgc][INFO] - Batch 132, avg loss per batch: 3.100711977554047
[2025-02-13 10:32:28,964][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 133
[2025-02-13 10:32:49,716][rgc][INFO] - 	Updating weights of batch 133
[2025-02-13 10:32:49,771][rgc][INFO] - Batch 133, avg loss per batch: 3.2176417515343623
[2025-02-13 10:32:49,771][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 134
[2025-02-13 10:33:10,530][rgc][INFO] - 	Updating weights of batch 134
[2025-02-13 10:33:10,589][rgc][INFO] - Batch 134, avg loss per batch: 5.032860684972057
[2025-02-13 10:33:10,589][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 135
[2025-02-13 10:33:31,347][rgc][INFO] - 	Updating weights of batch 135
[2025-02-13 10:33:31,407][rgc][INFO] - Batch 135, avg loss per batch: 3.4938196114266753
[2025-02-13 10:33:31,408][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 136
[2025-02-13 10:33:52,163][rgc][INFO] - 	Updating weights of batch 136
[2025-02-13 10:33:52,221][rgc][INFO] - Batch 136, avg loss per batch: 4.104828000753286
[2025-02-13 10:33:52,221][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 137
[2025-02-13 10:34:12,951][rgc][INFO] - 	Updating weights of batch 137
[2025-02-13 10:34:13,009][rgc][INFO] - Batch 137, avg loss per batch: 2.6418443767979234
[2025-02-13 10:34:13,010][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 138
[2025-02-13 10:34:33,775][rgc][INFO] - 	Updating weights of batch 138
[2025-02-13 10:34:33,833][rgc][INFO] - Batch 138, avg loss per batch: 3.2408886605905693
[2025-02-13 10:34:33,834][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 139
[2025-02-13 10:34:54,576][rgc][INFO] - 	Updating weights of batch 139
[2025-02-13 10:34:54,635][rgc][INFO] - Batch 139, avg loss per batch: 2.9243654070358946
[2025-02-13 10:34:54,635][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 140
[2025-02-13 10:35:15,304][rgc][INFO] - 	Updating weights of batch 140
[2025-02-13 10:35:15,361][rgc][INFO] - Batch 140, avg loss per batch: 4.2703337600146885
[2025-02-13 10:35:15,361][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 141
[2025-02-13 10:35:36,114][rgc][INFO] - 	Updating weights of batch 141
[2025-02-13 10:35:36,173][rgc][INFO] - Batch 141, avg loss per batch: 4.201398988099916
[2025-02-13 10:35:36,174][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 142
[2025-02-13 10:35:56,893][rgc][INFO] - 	Updating weights of batch 142
[2025-02-13 10:35:56,949][rgc][INFO] - Batch 142, avg loss per batch: 5.052482525150766
[2025-02-13 10:35:56,949][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 143
[2025-02-13 10:36:17,710][rgc][INFO] - 	Updating weights of batch 143
[2025-02-13 10:36:17,766][rgc][INFO] - Batch 143, avg loss per batch: 2.472300296318747
[2025-02-13 10:36:17,767][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 144
[2025-02-13 10:36:38,451][rgc][INFO] - 	Updating weights of batch 144
[2025-02-13 10:36:38,507][rgc][INFO] - Batch 144, avg loss per batch: 3.9341008977055454
[2025-02-13 10:36:38,508][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 145
[2025-02-13 10:36:59,255][rgc][INFO] - 	Updating weights of batch 145
[2025-02-13 10:36:59,313][rgc][INFO] - Batch 145, avg loss per batch: 5.942015347946184
[2025-02-13 10:36:59,313][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 146
[2025-02-13 10:37:20,055][rgc][INFO] - 	Updating weights of batch 146
[2025-02-13 10:37:20,108][rgc][INFO] - Batch 146, avg loss per batch: 3.1683914681385854
[2025-02-13 10:37:20,109][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 147
[2025-02-13 10:37:40,882][rgc][INFO] - 	Updating weights of batch 147
[2025-02-13 10:37:40,943][rgc][INFO] - Batch 147, avg loss per batch: 1.6751609820465558
[2025-02-13 10:37:40,945][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 148
[2025-02-13 10:38:01,699][rgc][INFO] - 	Updating weights of batch 148
[2025-02-13 10:38:01,758][rgc][INFO] - Batch 148, avg loss per batch: 3.224692280726638
[2025-02-13 10:38:01,758][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 149
[2025-02-13 10:38:22,485][rgc][INFO] - 	Updating weights of batch 149
[2025-02-13 10:38:22,541][rgc][INFO] - Batch 149, avg loss per batch: 3.1065053163363556
[2025-02-13 10:38:22,542][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 150
[2025-02-13 10:38:43,241][rgc][INFO] - 	Updating weights of batch 150
[2025-02-13 10:38:43,300][rgc][INFO] - Batch 150, avg loss per batch: 1.5754082016540187
[2025-02-13 10:38:43,300][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 151
[2025-02-13 10:39:03,973][rgc][INFO] - 	Updating weights of batch 151
[2025-02-13 10:39:04,030][rgc][INFO] - Batch 151, avg loss per batch: 3.5163956927538984
[2025-02-13 10:39:04,031][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 152
[2025-02-13 10:39:24,760][rgc][INFO] - 	Updating weights of batch 152
[2025-02-13 10:39:24,817][rgc][INFO] - Batch 152, avg loss per batch: 5.08228977623253
[2025-02-13 10:39:24,817][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 153
[2025-02-13 10:39:45,568][rgc][INFO] - 	Updating weights of batch 153
[2025-02-13 10:39:45,627][rgc][INFO] - Batch 153, avg loss per batch: 2.4342631579108573
[2025-02-13 10:39:45,628][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 154
[2025-02-13 10:40:06,304][rgc][INFO] - 	Updating weights of batch 154
[2025-02-13 10:40:06,362][rgc][INFO] - Batch 154, avg loss per batch: 4.894029732974115
[2025-02-13 10:40:06,363][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 155
[2025-02-13 10:40:27,039][rgc][INFO] - 	Updating weights of batch 155
[2025-02-13 10:40:27,098][rgc][INFO] - Batch 155, avg loss per batch: 1.2352042794800155
[2025-02-13 10:40:27,099][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 156
[2025-02-13 10:40:47,852][rgc][INFO] - 	Updating weights of batch 156
[2025-02-13 10:40:47,911][rgc][INFO] - Batch 156, avg loss per batch: 4.526523568928594
[2025-02-13 10:40:47,912][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 157
[2025-02-13 10:41:08,640][rgc][INFO] - 	Updating weights of batch 157
[2025-02-13 10:41:08,698][rgc][INFO] - Batch 157, avg loss per batch: 3.5796457189978916
[2025-02-13 10:41:08,699][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 158
[2025-02-13 10:41:29,444][rgc][INFO] - 	Updating weights of batch 158
[2025-02-13 10:41:29,504][rgc][INFO] - Batch 158, avg loss per batch: 3.7852448498237643
[2025-02-13 10:41:29,504][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 159
[2025-02-13 10:41:50,180][rgc][INFO] - 	Updating weights of batch 159
[2025-02-13 10:41:50,240][rgc][INFO] - Batch 159, avg loss per batch: 1.8386005451380778
[2025-02-13 10:41:50,241][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 160
[2025-02-13 10:42:10,936][rgc][INFO] - 	Updating weights of batch 160
[2025-02-13 10:42:10,996][rgc][INFO] - Batch 160, avg loss per batch: 3.7946400034449264
[2025-02-13 10:42:10,996][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 161
[2025-02-13 10:42:31,752][rgc][INFO] - 	Updating weights of batch 161
[2025-02-13 10:42:31,811][rgc][INFO] - Batch 161, avg loss per batch: 2.89294835859217
[2025-02-13 10:42:31,812][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 162
[2025-02-13 10:42:52,546][rgc][INFO] - 	Updating weights of batch 162
[2025-02-13 10:42:52,606][rgc][INFO] - Batch 162, avg loss per batch: 3.6997045617707935
[2025-02-13 10:42:52,607][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 163
[2025-02-13 10:43:13,381][rgc][INFO] - 	Updating weights of batch 163
[2025-02-13 10:43:13,441][rgc][INFO] - Batch 163, avg loss per batch: 1.713118260924535
[2025-02-13 10:43:13,442][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 164
[2025-02-13 10:43:34,220][rgc][INFO] - 	Updating weights of batch 164
[2025-02-13 10:43:34,279][rgc][INFO] - Batch 164, avg loss per batch: 2.721327675174337
[2025-02-13 10:43:34,279][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 165
[2025-02-13 10:43:54,971][rgc][INFO] - 	Updating weights of batch 165
[2025-02-13 10:43:55,030][rgc][INFO] - Batch 165, avg loss per batch: 4.16883078237063
[2025-02-13 10:43:55,031][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 166
[2025-02-13 10:44:15,726][rgc][INFO] - 	Updating weights of batch 166
[2025-02-13 10:44:15,785][rgc][INFO] - Batch 166, avg loss per batch: 3.0125561237858465
[2025-02-13 10:44:15,786][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 167
[2025-02-13 10:44:36,518][rgc][INFO] - 	Updating weights of batch 167
[2025-02-13 10:44:36,577][rgc][INFO] - Batch 167, avg loss per batch: 1.362561071943727
[2025-02-13 10:44:36,577][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 168
[2025-02-13 10:44:57,251][rgc][INFO] - 	Updating weights of batch 168
[2025-02-13 10:44:57,310][rgc][INFO] - Batch 168, avg loss per batch: 3.3599989313441467
[2025-02-13 10:44:57,311][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 169
[2025-02-13 10:45:18,006][rgc][INFO] - 	Updating weights of batch 169
[2025-02-13 10:45:18,065][rgc][INFO] - Batch 169, avg loss per batch: 2.831293641178973
[2025-02-13 10:45:18,065][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 170
[2025-02-13 10:45:38,826][rgc][INFO] - 	Updating weights of batch 170
[2025-02-13 10:45:38,883][rgc][INFO] - Batch 170, avg loss per batch: 2.591054556530932
[2025-02-13 10:45:38,883][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 171
[2025-02-13 10:45:59,560][rgc][INFO] - 	Updating weights of batch 171
[2025-02-13 10:45:59,620][rgc][INFO] - Batch 171, avg loss per batch: 2.8319173068103396
[2025-02-13 10:45:59,622][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 172
[2025-02-13 10:46:20,311][rgc][INFO] - 	Updating weights of batch 172
[2025-02-13 10:46:20,370][rgc][INFO] - Batch 172, avg loss per batch: 4.802113587585701
[2025-02-13 10:46:20,371][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 173
[2025-02-13 10:46:41,101][rgc][INFO] - 	Updating weights of batch 173
[2025-02-13 10:46:41,159][rgc][INFO] - Batch 173, avg loss per batch: 2.1777652214401204
[2025-02-13 10:46:41,160][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 174
[2025-02-13 10:47:01,915][rgc][INFO] - 	Updating weights of batch 174
[2025-02-13 10:47:01,975][rgc][INFO] - Batch 174, avg loss per batch: 3.3095334721156107
[2025-02-13 10:47:01,975][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 175
[2025-02-13 10:47:22,747][rgc][INFO] - 	Updating weights of batch 175
[2025-02-13 10:47:22,806][rgc][INFO] - Batch 175, avg loss per batch: 1.7374113605982857
[2025-02-13 10:47:22,806][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 176
[2025-02-13 10:47:43,555][rgc][INFO] - 	Updating weights of batch 176
[2025-02-13 10:47:43,611][rgc][INFO] - Batch 176, avg loss per batch: 4.341725053835152
[2025-02-13 10:47:43,612][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 177
[2025-02-13 10:48:04,361][rgc][INFO] - 	Updating weights of batch 177
[2025-02-13 10:48:04,418][rgc][INFO] - Batch 177, avg loss per batch: 2.5309276938115657
[2025-02-13 10:48:04,419][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 178
[2025-02-13 10:48:25,145][rgc][INFO] - 	Updating weights of batch 178
[2025-02-13 10:48:25,204][rgc][INFO] - Batch 178, avg loss per batch: 2.8696779847009086
[2025-02-13 10:48:25,205][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 179
[2025-02-13 10:48:45,960][rgc][INFO] - 	Updating weights of batch 179
[2025-02-13 10:48:46,016][rgc][INFO] - Batch 179, avg loss per batch: 2.311392093993499
[2025-02-13 10:48:46,017][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 180
[2025-02-13 10:49:06,767][rgc][INFO] - 	Updating weights of batch 180
[2025-02-13 10:49:06,826][rgc][INFO] - Batch 180, avg loss per batch: 1.9036813952992089
[2025-02-13 10:49:06,827][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 181
[2025-02-13 10:49:27,515][rgc][INFO] - 	Updating weights of batch 181
[2025-02-13 10:49:27,572][rgc][INFO] - Batch 181, avg loss per batch: 2.69780046156039
[2025-02-13 10:49:27,573][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 182
[2025-02-13 10:49:48,271][rgc][INFO] - 	Updating weights of batch 182
[2025-02-13 10:49:48,330][rgc][INFO] - Batch 182, avg loss per batch: 3.3498550155903133
[2025-02-13 10:49:48,331][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 183
[2025-02-13 10:50:09,079][rgc][INFO] - 	Updating weights of batch 183
[2025-02-13 10:50:09,140][rgc][INFO] - Batch 183, avg loss per batch: 2.3797219963159733
[2025-02-13 10:50:09,141][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 184
[2025-02-13 10:50:29,824][rgc][INFO] - 	Updating weights of batch 184
[2025-02-13 10:50:29,881][rgc][INFO] - Batch 184, avg loss per batch: 2.653352696850683
[2025-02-13 10:50:29,881][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 185
[2025-02-13 10:50:50,569][rgc][INFO] - 	Updating weights of batch 185
[2025-02-13 10:50:50,626][rgc][INFO] - Batch 185, avg loss per batch: 2.751306746823807
[2025-02-13 10:50:50,627][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 186
[2025-02-13 10:51:11,403][rgc][INFO] - 	Updating weights of batch 186
[2025-02-13 10:51:11,463][rgc][INFO] - Batch 186, avg loss per batch: 1.1907246462714258
[2025-02-13 10:51:11,463][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 187
[2025-02-13 10:51:32,136][rgc][INFO] - 	Updating weights of batch 187
[2025-02-13 10:51:32,195][rgc][INFO] - Batch 187, avg loss per batch: 3.8539849513160727
[2025-02-13 10:51:32,196][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 188
[2025-02-13 10:51:52,949][rgc][INFO] - 	Updating weights of batch 188
[2025-02-13 10:51:53,007][rgc][INFO] - Batch 188, avg loss per batch: 1.8186242598637385
[2025-02-13 10:51:53,007][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 189
[2025-02-13 10:52:13,748][rgc][INFO] - 	Updating weights of batch 189
[2025-02-13 10:52:13,808][rgc][INFO] - Batch 189, avg loss per batch: 1.4706843447532123
[2025-02-13 10:52:13,809][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 190
[2025-02-13 10:52:34,562][rgc][INFO] - 	Updating weights of batch 190
[2025-02-13 10:52:34,619][rgc][INFO] - Batch 190, avg loss per batch: 1.4084387245041297
[2025-02-13 10:52:34,619][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 191
[2025-02-13 10:52:55,370][rgc][INFO] - 	Updating weights of batch 191
[2025-02-13 10:52:55,427][rgc][INFO] - Batch 191, avg loss per batch: 2.9425700541061905
[2025-02-13 10:52:55,427][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 192
[2025-02-13 10:53:16,157][rgc][INFO] - 	Updating weights of batch 192
[2025-02-13 10:53:16,216][rgc][INFO] - Batch 192, avg loss per batch: 3.7765040267238357
[2025-02-13 10:53:16,217][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 193
[2025-02-13 10:53:36,960][rgc][INFO] - 	Updating weights of batch 193
[2025-02-13 10:53:37,010][rgc][INFO] - Batch 193, avg loss per batch: 2.622852944682867
[2025-02-13 10:53:37,010][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 194
[2025-02-13 10:53:57,750][rgc][INFO] - 	Updating weights of batch 194
[2025-02-13 10:53:57,826][rgc][INFO] - Batch 194, avg loss per batch: 1.6138329464430636
[2025-02-13 10:53:57,827][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 195
[2025-02-13 10:54:18,569][rgc][INFO] - 	Updating weights of batch 195
[2025-02-13 10:54:18,618][rgc][INFO] - Batch 195, avg loss per batch: 4.160665418901187
[2025-02-13 10:54:18,619][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 196
[2025-02-13 10:54:39,353][rgc][INFO] - 	Updating weights of batch 196
[2025-02-13 10:54:39,405][rgc][INFO] - Batch 196, avg loss per batch: 3.5717226477008217
[2025-02-13 10:54:39,405][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 197
[2025-02-13 10:55:00,120][rgc][INFO] - 	Updating weights of batch 197
[2025-02-13 10:55:00,172][rgc][INFO] - Batch 197, avg loss per batch: 3.393482795276789
[2025-02-13 10:55:00,173][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 198
[2025-02-13 10:55:20,850][rgc][INFO] - 	Updating weights of batch 198
[2025-02-13 10:55:20,900][rgc][INFO] - Batch 198, avg loss per batch: 3.09726173272308
[2025-02-13 10:55:20,901][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 199
[2025-02-13 10:55:41,647][rgc][INFO] - 	Updating weights of batch 199
[2025-02-13 10:55:41,729][rgc][INFO] - Batch 199, avg loss per batch: 2.891790391261532
[2025-02-13 10:55:53,289][rgc][INFO] - AVG rho on val data: 0.0876729137836108
[2025-02-13 10:55:53,289][rgc][INFO] - AVG mae on val data: 0.6685055383681937
[2025-02-13 10:56:04,461][rgc][INFO] - AVG rho on test data: 0.23624474337378798
[2025-02-13 10:56:04,461][rgc][INFO] - AVG mae on test data: 0.6274236606514361
[2025-02-13 10:56:17,302][rgc][INFO] - AVG rho on train data: 0.0687133826625255
[2025-02-13 10:56:17,302][rgc][INFO] - AVG mae on train data: 0.6598155385537453
[2025-02-13 10:56:17,302][rgc][INFO] - Current best rhos: train 0.0687133826625255, val 0.0876729137836108, test 0.23624474337378798
[2025-02-13 10:56:17,304][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 200
[2025-02-13 10:56:38,028][rgc][INFO] - 	Updating weights of batch 200
[2025-02-13 10:56:38,081][rgc][INFO] - Batch 200, avg loss per batch: 3.6542011211923433
[2025-02-13 10:56:38,082][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 201
[2025-02-13 10:56:58,809][rgc][INFO] - 	Updating weights of batch 201
[2025-02-13 10:56:58,864][rgc][INFO] - Batch 201, avg loss per batch: 3.958949352284843
[2025-02-13 10:56:58,865][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 202
[2025-02-13 10:57:19,601][rgc][INFO] - 	Updating weights of batch 202
[2025-02-13 10:57:19,654][rgc][INFO] - Batch 202, avg loss per batch: 2.4729603988997626
[2025-02-13 10:57:19,654][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 203
[2025-02-13 10:57:40,327][rgc][INFO] - 	Updating weights of batch 203
[2025-02-13 10:57:40,399][rgc][INFO] - Batch 203, avg loss per batch: 4.640873704483228
[2025-02-13 10:57:40,400][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 204
[2025-02-13 10:58:01,065][rgc][INFO] - 	Updating weights of batch 204
[2025-02-13 10:58:01,118][rgc][INFO] - Batch 204, avg loss per batch: 3.9820965344562076
[2025-02-13 10:58:01,119][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 205
[2025-02-13 10:58:21,794][rgc][INFO] - 	Updating weights of batch 205
[2025-02-13 10:58:21,860][rgc][INFO] - Batch 205, avg loss per batch: 2.537218870856937
[2025-02-13 10:58:21,861][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 206
[2025-02-13 10:58:42,530][rgc][INFO] - 	Updating weights of batch 206
[2025-02-13 10:58:42,586][rgc][INFO] - Batch 206, avg loss per batch: 2.724425412979971
[2025-02-13 10:58:42,587][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 207
[2025-02-13 10:59:03,331][rgc][INFO] - 	Updating weights of batch 207
[2025-02-13 10:59:03,387][rgc][INFO] - Batch 207, avg loss per batch: 2.52181379906004
[2025-02-13 10:59:03,388][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 208
[2025-02-13 10:59:24,085][rgc][INFO] - 	Updating weights of batch 208
[2025-02-13 10:59:24,140][rgc][INFO] - Batch 208, avg loss per batch: 2.759651735468184
[2025-02-13 10:59:24,141][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 209
[2025-02-13 10:59:44,811][rgc][INFO] - 	Updating weights of batch 209
[2025-02-13 10:59:44,867][rgc][INFO] - Batch 209, avg loss per batch: 3.545617786052076
[2025-02-13 10:59:44,869][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 210
[2025-02-13 11:00:05,556][rgc][INFO] - 	Updating weights of batch 210
[2025-02-13 11:00:05,606][rgc][INFO] - Batch 210, avg loss per batch: 3.091040815760265
[2025-02-13 11:00:05,607][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 211
[2025-02-13 11:00:26,348][rgc][INFO] - 	Updating weights of batch 211
[2025-02-13 11:00:26,428][rgc][INFO] - Batch 211, avg loss per batch: 2.387246614741786
[2025-02-13 11:00:26,431][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 212
[2025-02-13 11:00:47,103][rgc][INFO] - 	Updating weights of batch 212
[2025-02-13 11:00:47,155][rgc][INFO] - Batch 212, avg loss per batch: 1.9726532741522984
[2025-02-13 11:00:47,156][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 213
[2025-02-13 11:01:07,878][rgc][INFO] - 	Updating weights of batch 213
[2025-02-13 11:01:07,934][rgc][INFO] - Batch 213, avg loss per batch: 2.725844390752594
[2025-02-13 11:01:07,935][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 214
[2025-02-13 11:01:28,684][rgc][INFO] - 	Updating weights of batch 214
[2025-02-13 11:01:28,739][rgc][INFO] - Batch 214, avg loss per batch: 1.1209330260763415
[2025-02-13 11:01:28,740][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 215
[2025-02-13 11:01:49,425][rgc][INFO] - 	Updating weights of batch 215
[2025-02-13 11:01:49,479][rgc][INFO] - Batch 215, avg loss per batch: 3.9182463203380307
[2025-02-13 11:01:49,480][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 216
[2025-02-13 11:02:10,154][rgc][INFO] - 	Updating weights of batch 216
[2025-02-13 11:02:10,224][rgc][INFO] - Batch 216, avg loss per batch: 4.618705204676235
[2025-02-13 11:02:10,225][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 217
[2025-02-13 11:02:30,979][rgc][INFO] - 	Updating weights of batch 217
[2025-02-13 11:02:31,032][rgc][INFO] - Batch 217, avg loss per batch: 3.846514879504917
[2025-02-13 11:02:31,032][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 218
[2025-02-13 11:02:51,722][rgc][INFO] - 	Updating weights of batch 218
[2025-02-13 11:02:51,786][rgc][INFO] - Batch 218, avg loss per batch: 2.725973525504176
[2025-02-13 11:02:51,787][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 219
[2025-02-13 11:03:12,527][rgc][INFO] - 	Updating weights of batch 219
[2025-02-13 11:03:12,591][rgc][INFO] - Batch 219, avg loss per batch: 4.130106719111658
[2025-02-13 11:03:12,592][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 220
[2025-02-13 11:03:33,335][rgc][INFO] - 	Updating weights of batch 220
[2025-02-13 11:03:33,389][rgc][INFO] - Batch 220, avg loss per batch: 4.0978991010614445
[2025-02-13 11:03:33,389][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 221
[2025-02-13 11:03:54,148][rgc][INFO] - 	Updating weights of batch 221
[2025-02-13 11:03:54,202][rgc][INFO] - Batch 221, avg loss per batch: 2.368453007973947
[2025-02-13 11:03:54,203][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 222
[2025-02-13 11:04:14,930][rgc][INFO] - 	Updating weights of batch 222
[2025-02-13 11:04:14,979][rgc][INFO] - Batch 222, avg loss per batch: 2.0598898459202464
[2025-02-13 11:04:14,980][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 223
[2025-02-13 11:04:35,735][rgc][INFO] - 	Updating weights of batch 223
[2025-02-13 11:04:35,786][rgc][INFO] - Batch 223, avg loss per batch: 2.314384382971247
[2025-02-13 11:04:35,787][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 224
[2025-02-13 11:04:56,557][rgc][INFO] - 	Updating weights of batch 224
[2025-02-13 11:04:56,608][rgc][INFO] - Batch 224, avg loss per batch: 2.411597346186519
[2025-02-13 11:04:56,609][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 225
[2025-02-13 11:05:17,365][rgc][INFO] - 	Updating weights of batch 225
[2025-02-13 11:05:17,415][rgc][INFO] - Batch 225, avg loss per batch: 5.623577188434298
[2025-02-13 11:05:17,416][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 226
[2025-02-13 11:05:38,149][rgc][INFO] - 	Updating weights of batch 226
[2025-02-13 11:05:38,199][rgc][INFO] - Batch 226, avg loss per batch: 4.264510431356118
[2025-02-13 11:05:38,200][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 227
[2025-02-13 11:05:58,943][rgc][INFO] - 	Updating weights of batch 227
[2025-02-13 11:05:58,993][rgc][INFO] - Batch 227, avg loss per batch: 4.95272883629676
[2025-02-13 11:05:58,994][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 228
[2025-02-13 11:06:19,728][rgc][INFO] - 	Updating weights of batch 228
[2025-02-13 11:06:19,780][rgc][INFO] - Batch 228, avg loss per batch: 7.7427257789900485
[2025-02-13 11:06:19,781][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 229
[2025-02-13 11:06:40,534][rgc][INFO] - 	Updating weights of batch 229
[2025-02-13 11:06:40,585][rgc][INFO] - Batch 229, avg loss per batch: 3.258136824530669
[2025-02-13 11:06:40,586][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 230
[2025-02-13 11:07:01,297][rgc][INFO] - 	Updating weights of batch 230
[2025-02-13 11:07:01,362][rgc][INFO] - Batch 230, avg loss per batch: 2.69132897365765
[2025-02-13 11:07:01,363][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 231
[2025-02-13 11:07:22,099][rgc][INFO] - 	Updating weights of batch 231
[2025-02-13 11:07:22,152][rgc][INFO] - Batch 231, avg loss per batch: 3.967735259517316
[2025-02-13 11:07:22,152][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 232
[2025-02-13 11:07:42,837][rgc][INFO] - 	Updating weights of batch 232
[2025-02-13 11:07:42,887][rgc][INFO] - Batch 232, avg loss per batch: 10.559669704591816
[2025-02-13 11:07:42,888][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 233
[2025-02-13 11:08:03,645][rgc][INFO] - 	Updating weights of batch 233
[2025-02-13 11:08:03,721][rgc][INFO] - Batch 233, avg loss per batch: 7.496413580332323
[2025-02-13 11:08:03,722][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 234
[2025-02-13 11:08:24,455][rgc][INFO] - 	Updating weights of batch 234
[2025-02-13 11:08:24,505][rgc][INFO] - Batch 234, avg loss per batch: 4.940500988864585
[2025-02-13 11:08:24,506][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 235
[2025-02-13 11:08:45,244][rgc][INFO] - 	Updating weights of batch 235
[2025-02-13 11:08:45,297][rgc][INFO] - Batch 235, avg loss per batch: 5.190871038670943
[2025-02-13 11:08:45,298][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 236
[2025-02-13 11:09:06,037][rgc][INFO] - 	Updating weights of batch 236
[2025-02-13 11:09:06,089][rgc][INFO] - Batch 236, avg loss per batch: 3.8164108585436374
[2025-02-13 11:09:06,090][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 237
[2025-02-13 11:09:26,845][rgc][INFO] - 	Updating weights of batch 237
[2025-02-13 11:09:26,897][rgc][INFO] - Batch 237, avg loss per batch: 3.9342828185452783
[2025-02-13 11:09:26,898][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 238
[2025-02-13 11:09:47,622][rgc][INFO] - 	Updating weights of batch 238
[2025-02-13 11:09:47,671][rgc][INFO] - Batch 238, avg loss per batch: 3.8980515584314586
[2025-02-13 11:09:47,672][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 239
[2025-02-13 11:10:08,400][rgc][INFO] - 	Updating weights of batch 239
[2025-02-13 11:10:08,457][rgc][INFO] - Batch 239, avg loss per batch: 3.1736990173914297
[2025-02-13 11:10:08,458][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 240
[2025-02-13 11:10:29,204][rgc][INFO] - 	Updating weights of batch 240
[2025-02-13 11:10:29,258][rgc][INFO] - Batch 240, avg loss per batch: 3.4090163863682794
[2025-02-13 11:10:29,259][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 241
[2025-02-13 11:10:49,984][rgc][INFO] - 	Updating weights of batch 241
[2025-02-13 11:10:50,035][rgc][INFO] - Batch 241, avg loss per batch: 4.508603303249453
[2025-02-13 11:10:50,036][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 242
[2025-02-13 11:11:10,787][rgc][INFO] - 	Updating weights of batch 242
[2025-02-13 11:11:10,841][rgc][INFO] - Batch 242, avg loss per batch: 3.752270519945657
[2025-02-13 11:11:10,842][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 243
[2025-02-13 11:11:31,594][rgc][INFO] - 	Updating weights of batch 243
[2025-02-13 11:11:31,650][rgc][INFO] - Batch 243, avg loss per batch: 6.325253257195479
[2025-02-13 11:11:31,650][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 244
[2025-02-13 11:11:52,325][rgc][INFO] - 	Updating weights of batch 244
[2025-02-13 11:11:52,403][rgc][INFO] - Batch 244, avg loss per batch: 3.3186092404770156
[2025-02-13 11:11:52,404][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 245
[2025-02-13 11:12:13,081][rgc][INFO] - 	Updating weights of batch 245
[2025-02-13 11:12:13,140][rgc][INFO] - Batch 245, avg loss per batch: 2.694484723828194
[2025-02-13 11:12:13,141][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 246
[2025-02-13 11:12:33,813][rgc][INFO] - 	Updating weights of batch 246
[2025-02-13 11:12:33,864][rgc][INFO] - Batch 246, avg loss per batch: 4.141574722925688
[2025-02-13 11:12:33,865][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 247
[2025-02-13 11:12:54,560][rgc][INFO] - 	Updating weights of batch 247
[2025-02-13 11:12:54,615][rgc][INFO] - Batch 247, avg loss per batch: 2.441536930537879
[2025-02-13 11:12:54,616][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 248
[2025-02-13 11:13:15,371][rgc][INFO] - 	Updating weights of batch 248
[2025-02-13 11:13:15,426][rgc][INFO] - Batch 248, avg loss per batch: 2.242248869555711
[2025-02-13 11:13:15,427][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 249
[2025-02-13 11:13:36,116][rgc][INFO] - 	Updating weights of batch 249
[2025-02-13 11:13:36,168][rgc][INFO] - Batch 249, avg loss per batch: 3.3142020993779364
[2025-02-13 11:13:36,169][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 250
[2025-02-13 11:13:56,910][rgc][INFO] - 	Updating weights of batch 250
[2025-02-13 11:13:56,962][rgc][INFO] - Batch 250, avg loss per batch: 3.8949044444883234
[2025-02-13 11:13:56,962][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 251
[2025-02-13 11:14:17,710][rgc][INFO] - 	Updating weights of batch 251
[2025-02-13 11:14:17,765][rgc][INFO] - Batch 251, avg loss per batch: 2.3510430509262497
[2025-02-13 11:14:17,766][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 252
[2025-02-13 11:14:38,493][rgc][INFO] - 	Updating weights of batch 252
[2025-02-13 11:14:38,569][rgc][INFO] - Batch 252, avg loss per batch: 2.678947715999007
[2025-02-13 11:14:38,570][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 253
[2025-02-13 11:14:59,314][rgc][INFO] - 	Updating weights of batch 253
[2025-02-13 11:14:59,374][rgc][INFO] - Batch 253, avg loss per batch: 2.190383698929375
[2025-02-13 11:14:59,375][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 254
[2025-02-13 11:15:20,101][rgc][INFO] - 	Updating weights of batch 254
[2025-02-13 11:15:20,155][rgc][INFO] - Batch 254, avg loss per batch: 3.520054801841266
[2025-02-13 11:15:20,156][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 255
[2025-02-13 11:15:40,821][rgc][INFO] - 	Updating weights of batch 255
[2025-02-13 11:15:40,874][rgc][INFO] - Batch 255, avg loss per batch: 3.9823167048640524
[2025-02-13 11:15:40,875][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 256
[2025-02-13 11:16:01,624][rgc][INFO] - 	Updating weights of batch 256
[2025-02-13 11:16:01,680][rgc][INFO] - Batch 256, avg loss per batch: 4.549171836775295
[2025-02-13 11:16:01,680][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 257
[2025-02-13 11:16:22,402][rgc][INFO] - 	Updating weights of batch 257
[2025-02-13 11:16:22,454][rgc][INFO] - Batch 257, avg loss per batch: 4.3095150214084565
[2025-02-13 11:16:22,455][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 258
[2025-02-13 11:16:43,144][rgc][INFO] - 	Updating weights of batch 258
[2025-02-13 11:16:43,196][rgc][INFO] - Batch 258, avg loss per batch: 1.8098518908515078
[2025-02-13 11:16:43,196][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 259
[2025-02-13 11:17:03,950][rgc][INFO] - 	Updating weights of batch 259
[2025-02-13 11:17:04,003][rgc][INFO] - Batch 259, avg loss per batch: 2.3979080359859286
[2025-02-13 11:17:04,004][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 260
[2025-02-13 11:17:24,769][rgc][INFO] - 	Updating weights of batch 260
[2025-02-13 11:17:24,822][rgc][INFO] - Batch 260, avg loss per batch: 2.585708381787705
[2025-02-13 11:17:24,822][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 261
[2025-02-13 11:17:45,564][rgc][INFO] - 	Updating weights of batch 261
[2025-02-13 11:17:45,615][rgc][INFO] - Batch 261, avg loss per batch: 4.73658162651908
[2025-02-13 11:17:45,616][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 262
[2025-02-13 11:18:06,383][rgc][INFO] - 	Updating weights of batch 262
[2025-02-13 11:18:06,435][rgc][INFO] - Batch 262, avg loss per batch: 4.107141293237644
[2025-02-13 11:18:06,436][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 263
[2025-02-13 11:18:27,191][rgc][INFO] - 	Updating weights of batch 263
[2025-02-13 11:18:27,247][rgc][INFO] - Batch 263, avg loss per batch: 2.097955186351057
[2025-02-13 11:18:27,248][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 264
[2025-02-13 11:18:47,990][rgc][INFO] - 	Updating weights of batch 264
[2025-02-13 11:18:48,040][rgc][INFO] - Batch 264, avg loss per batch: 2.422406780746711
[2025-02-13 11:18:48,041][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 265
[2025-02-13 11:19:08,719][rgc][INFO] - 	Updating weights of batch 265
[2025-02-13 11:19:08,772][rgc][INFO] - Batch 265, avg loss per batch: 3.9154044496459424
[2025-02-13 11:19:08,773][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 266
[2025-02-13 11:19:29,526][rgc][INFO] - 	Updating weights of batch 266
[2025-02-13 11:19:29,576][rgc][INFO] - Batch 266, avg loss per batch: 3.4272644280240225
[2025-02-13 11:19:29,577][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 267
[2025-02-13 11:19:50,319][rgc][INFO] - 	Updating weights of batch 267
[2025-02-13 11:19:50,370][rgc][INFO] - Batch 267, avg loss per batch: 3.970772922369752
[2025-02-13 11:19:50,371][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 268
[2025-02-13 11:20:11,074][rgc][INFO] - 	Updating weights of batch 268
[2025-02-13 11:20:11,125][rgc][INFO] - Batch 268, avg loss per batch: 3.99417977628282
[2025-02-13 11:20:11,126][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 269
[2025-02-13 11:20:31,881][rgc][INFO] - 	Updating weights of batch 269
[2025-02-13 11:20:31,933][rgc][INFO] - Batch 269, avg loss per batch: 1.8172695474017702
[2025-02-13 11:20:31,933][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 270
[2025-02-13 11:20:52,607][rgc][INFO] - 	Updating weights of batch 270
[2025-02-13 11:20:52,659][rgc][INFO] - Batch 270, avg loss per batch: 2.7314956056013844
[2025-02-13 11:20:52,660][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 271
[2025-02-13 11:21:13,352][rgc][INFO] - 	Updating weights of batch 271
[2025-02-13 11:21:13,434][rgc][INFO] - Batch 271, avg loss per batch: 4.121091559912188
[2025-02-13 11:21:13,435][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 272
[2025-02-13 11:21:34,166][rgc][INFO] - 	Updating weights of batch 272
[2025-02-13 11:21:34,221][rgc][INFO] - Batch 272, avg loss per batch: 1.9105608267865617
[2025-02-13 11:21:34,221][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 273
[2025-02-13 11:21:54,922][rgc][INFO] - 	Updating weights of batch 273
[2025-02-13 11:21:54,973][rgc][INFO] - Batch 273, avg loss per batch: 1.6721324934775303
[2025-02-13 11:21:54,974][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 274
[2025-02-13 11:22:15,737][rgc][INFO] - 	Updating weights of batch 274
[2025-02-13 11:22:15,801][rgc][INFO] - Batch 274, avg loss per batch: 2.8549324094973967
[2025-02-13 11:22:15,803][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 275
[2025-02-13 11:22:36,615][rgc][INFO] - 	Updating weights of batch 275
[2025-02-13 11:22:36,694][rgc][INFO] - Batch 275, avg loss per batch: 3.9582450641090885
[2025-02-13 11:22:36,695][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 276
[2025-02-13 11:22:57,383][rgc][INFO] - 	Updating weights of batch 276
[2025-02-13 11:22:57,451][rgc][INFO] - Batch 276, avg loss per batch: 1.7280077458833318
[2025-02-13 11:22:57,452][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 277
[2025-02-13 11:23:18,216][rgc][INFO] - 	Updating weights of batch 277
[2025-02-13 11:23:18,274][rgc][INFO] - Batch 277, avg loss per batch: 4.02105854072517
[2025-02-13 11:23:18,276][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 278
[2025-02-13 11:23:39,020][rgc][INFO] - 	Updating weights of batch 278
[2025-02-13 11:23:39,073][rgc][INFO] - Batch 278, avg loss per batch: 4.485712422637714
[2025-02-13 11:23:39,073][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 279
[2025-02-13 11:23:59,806][rgc][INFO] - 	Updating weights of batch 279
[2025-02-13 11:23:59,881][rgc][INFO] - Batch 279, avg loss per batch: 3.6234978082420053
[2025-02-13 11:23:59,882][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 280
[2025-02-13 11:24:20,616][rgc][INFO] - 	Updating weights of batch 280
[2025-02-13 11:24:20,671][rgc][INFO] - Batch 280, avg loss per batch: 1.735190600000248
[2025-02-13 11:24:20,672][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 281
[2025-02-13 11:24:41,338][rgc][INFO] - 	Updating weights of batch 281
[2025-02-13 11:24:41,415][rgc][INFO] - Batch 281, avg loss per batch: 4.67292408552408
[2025-02-13 11:24:41,416][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 282
[2025-02-13 11:25:02,110][rgc][INFO] - 	Updating weights of batch 282
[2025-02-13 11:25:02,162][rgc][INFO] - Batch 282, avg loss per batch: 6.150423413385602
[2025-02-13 11:25:02,162][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 283
[2025-02-13 11:25:22,835][rgc][INFO] - 	Updating weights of batch 283
[2025-02-13 11:25:22,897][rgc][INFO] - Batch 283, avg loss per batch: 2.9426664563368075
[2025-02-13 11:25:22,898][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 284
[2025-02-13 11:25:43,575][rgc][INFO] - 	Updating weights of batch 284
[2025-02-13 11:25:43,630][rgc][INFO] - Batch 284, avg loss per batch: 2.541668083265489
[2025-02-13 11:25:43,631][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 285
[2025-02-13 11:26:04,307][rgc][INFO] - 	Updating weights of batch 285
[2025-02-13 11:26:04,362][rgc][INFO] - Batch 285, avg loss per batch: 3.8585707849418416
[2025-02-13 11:26:04,363][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 286
[2025-02-13 11:26:25,040][rgc][INFO] - 	Updating weights of batch 286
[2025-02-13 11:26:25,096][rgc][INFO] - Batch 286, avg loss per batch: 3.456966909751189
[2025-02-13 11:26:25,096][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 287
[2025-02-13 11:26:45,783][rgc][INFO] - 	Updating weights of batch 287
[2025-02-13 11:26:45,851][rgc][INFO] - Batch 287, avg loss per batch: 2.444937723473762
[2025-02-13 11:26:45,852][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 288
[2025-02-13 11:27:06,600][rgc][INFO] - 	Updating weights of batch 288
[2025-02-13 11:27:06,653][rgc][INFO] - Batch 288, avg loss per batch: 2.884820378016532
[2025-02-13 11:27:06,654][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 289
[2025-02-13 11:27:27,336][rgc][INFO] - 	Updating weights of batch 289
[2025-02-13 11:27:27,388][rgc][INFO] - Batch 289, avg loss per batch: 2.770292905911811
[2025-02-13 11:27:27,390][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 290
[2025-02-13 11:27:48,120][rgc][INFO] - 	Updating weights of batch 290
[2025-02-13 11:27:48,172][rgc][INFO] - Batch 290, avg loss per batch: 7.703800616226253
[2025-02-13 11:27:48,172][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 291
[2025-02-13 11:28:08,928][rgc][INFO] - 	Updating weights of batch 291
[2025-02-13 11:28:08,986][rgc][INFO] - Batch 291, avg loss per batch: 2.775065336488969
[2025-02-13 11:28:08,987][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 292
[2025-02-13 11:28:29,724][rgc][INFO] - 	Updating weights of batch 292
[2025-02-13 11:28:29,776][rgc][INFO] - Batch 292, avg loss per batch: 2.703267238049908
[2025-02-13 11:28:29,777][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 293
[2025-02-13 11:28:50,460][rgc][INFO] - 	Updating weights of batch 293
[2025-02-13 11:28:50,515][rgc][INFO] - Batch 293, avg loss per batch: 3.513782808078969
[2025-02-13 11:28:50,516][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 294
[2025-02-13 11:29:11,274][rgc][INFO] - 	Updating weights of batch 294
[2025-02-13 11:29:11,327][rgc][INFO] - Batch 294, avg loss per batch: 1.8930305274318138
[2025-02-13 11:29:11,327][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 295
[2025-02-13 11:29:32,014][rgc][INFO] - 	Updating weights of batch 295
[2025-02-13 11:29:32,066][rgc][INFO] - Batch 295, avg loss per batch: 2.015803957248555
[2025-02-13 11:29:32,066][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 296
[2025-02-13 11:29:52,752][rgc][INFO] - 	Updating weights of batch 296
[2025-02-13 11:29:52,823][rgc][INFO] - Batch 296, avg loss per batch: 4.8874390294073615
[2025-02-13 11:29:52,824][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 297
[2025-02-13 11:30:13,578][rgc][INFO] - 	Updating weights of batch 297
[2025-02-13 11:30:13,628][rgc][INFO] - Batch 297, avg loss per batch: 2.2408786992485488
[2025-02-13 11:30:13,629][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 298
[2025-02-13 11:30:34,322][rgc][INFO] - 	Updating weights of batch 298
[2025-02-13 11:30:34,376][rgc][INFO] - Batch 298, avg loss per batch: 3.462540164562158
[2025-02-13 11:30:34,376][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 299
[2025-02-13 11:30:55,062][rgc][INFO] - 	Updating weights of batch 299
[2025-02-13 11:30:55,121][rgc][INFO] - Batch 299, avg loss per batch: 2.1823918515345735
[2025-02-13 11:31:06,672][rgc][INFO] - AVG rho on val data: 0.10692559615095458
[2025-02-13 11:31:06,672][rgc][INFO] - AVG mae on val data: 0.5776392150585978
[2025-02-13 11:31:17,843][rgc][INFO] - AVG rho on test data: 0.29086522713948704
[2025-02-13 11:31:17,843][rgc][INFO] - AVG mae on test data: 0.5845348869786003
[2025-02-13 11:31:30,683][rgc][INFO] - AVG rho on train data: 0.11168490257645117
[2025-02-13 11:31:30,683][rgc][INFO] - AVG mae on train data: 0.5858924570577871
[2025-02-13 11:31:30,684][rgc][INFO] - Current best rhos: train 0.11168490257645117, val 0.10692559615095458, test 0.29086522713948704
[2025-02-13 11:31:30,685][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 300
[2025-02-13 11:31:51,366][rgc][INFO] - 	Updating weights of batch 300
[2025-02-13 11:31:51,418][rgc][INFO] - Batch 300, avg loss per batch: 2.830009087368939
[2025-02-13 11:31:51,418][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 301
[2025-02-13 11:32:12,097][rgc][INFO] - 	Updating weights of batch 301
[2025-02-13 11:32:12,153][rgc][INFO] - Batch 301, avg loss per batch: 2.5558999027363276
[2025-02-13 11:32:12,154][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 302
[2025-02-13 11:32:32,832][rgc][INFO] - 	Updating weights of batch 302
[2025-02-13 11:32:32,888][rgc][INFO] - Batch 302, avg loss per batch: 2.3858755525483417
[2025-02-13 11:32:32,889][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 303
[2025-02-13 11:32:53,633][rgc][INFO] - 	Updating weights of batch 303
[2025-02-13 11:32:53,687][rgc][INFO] - Batch 303, avg loss per batch: 1.6557251983391317
[2025-02-13 11:32:53,688][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 304
[2025-02-13 11:33:14,360][rgc][INFO] - 	Updating weights of batch 304
[2025-02-13 11:33:14,412][rgc][INFO] - Batch 304, avg loss per batch: 1.4897122152775124
[2025-02-13 11:33:14,413][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 305
[2025-02-13 11:33:35,158][rgc][INFO] - 	Updating weights of batch 305
[2025-02-13 11:33:35,223][rgc][INFO] - Batch 305, avg loss per batch: 1.8113921118733485
[2025-02-13 11:33:35,224][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 306
[2025-02-13 11:33:55,962][rgc][INFO] - 	Updating weights of batch 306
[2025-02-13 11:33:56,018][rgc][INFO] - Batch 306, avg loss per batch: 2.270422090266792
[2025-02-13 11:33:56,019][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 307
[2025-02-13 11:34:16,675][rgc][INFO] - 	Updating weights of batch 307
[2025-02-13 11:34:16,746][rgc][INFO] - Batch 307, avg loss per batch: 3.499242747871474
[2025-02-13 11:34:16,747][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 308
[2025-02-13 11:34:37,477][rgc][INFO] - 	Updating weights of batch 308
[2025-02-13 11:34:37,526][rgc][INFO] - Batch 308, avg loss per batch: 2.298253002617885
[2025-02-13 11:34:37,526][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 309
[2025-02-13 11:34:58,256][rgc][INFO] - 	Updating weights of batch 309
[2025-02-13 11:34:58,308][rgc][INFO] - Batch 309, avg loss per batch: 2.6186011019521427
[2025-02-13 11:34:58,309][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 310
[2025-02-13 11:35:19,054][rgc][INFO] - 	Updating weights of batch 310
[2025-02-13 11:35:19,110][rgc][INFO] - Batch 310, avg loss per batch: 2.0644431138798063
[2025-02-13 11:35:19,111][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 311
[2025-02-13 11:35:39,850][rgc][INFO] - 	Updating weights of batch 311
[2025-02-13 11:35:39,901][rgc][INFO] - Batch 311, avg loss per batch: 3.8950540628744594
[2025-02-13 11:35:39,901][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 312
[2025-02-13 11:36:00,569][rgc][INFO] - 	Updating weights of batch 312
[2025-02-13 11:36:00,623][rgc][INFO] - Batch 312, avg loss per batch: 3.4954576616102258
[2025-02-13 11:36:00,624][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 313
[2025-02-13 11:36:21,291][rgc][INFO] - 	Updating weights of batch 313
[2025-02-13 11:36:21,343][rgc][INFO] - Batch 313, avg loss per batch: 2.5846668779279103
[2025-02-13 11:36:21,344][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 314
[2025-02-13 11:36:42,090][rgc][INFO] - 	Updating weights of batch 314
[2025-02-13 11:36:42,141][rgc][INFO] - Batch 314, avg loss per batch: 2.0330908084009653
[2025-02-13 11:36:42,142][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 315
[2025-02-13 11:37:02,896][rgc][INFO] - 	Updating weights of batch 315
[2025-02-13 11:37:02,947][rgc][INFO] - Batch 315, avg loss per batch: 3.7039143702892976
[2025-02-13 11:37:02,948][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 316
[2025-02-13 11:37:23,633][rgc][INFO] - 	Updating weights of batch 316
[2025-02-13 11:37:23,683][rgc][INFO] - Batch 316, avg loss per batch: 1.8915335163813514
[2025-02-13 11:37:23,683][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 317
[2025-02-13 11:37:44,412][rgc][INFO] - 	Updating weights of batch 317
[2025-02-13 11:37:44,476][rgc][INFO] - Batch 317, avg loss per batch: 3.5286307455589885
[2025-02-13 11:37:44,477][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 318
[2025-02-13 11:38:05,205][rgc][INFO] - 	Updating weights of batch 318
[2025-02-13 11:38:05,258][rgc][INFO] - Batch 318, avg loss per batch: 4.826410509104026
[2025-02-13 11:38:05,259][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 319
[2025-02-13 11:38:26,000][rgc][INFO] - 	Updating weights of batch 319
[2025-02-13 11:38:26,080][rgc][INFO] - Batch 319, avg loss per batch: 2.363245005019146
[2025-02-13 11:38:26,081][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 320
[2025-02-13 11:38:46,855][rgc][INFO] - 	Updating weights of batch 320
[2025-02-13 11:38:46,907][rgc][INFO] - Batch 320, avg loss per batch: 1.5655802061602557
[2025-02-13 11:38:46,907][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 321
[2025-02-13 11:39:07,644][rgc][INFO] - 	Updating weights of batch 321
[2025-02-13 11:39:07,697][rgc][INFO] - Batch 321, avg loss per batch: 2.1351688609393
[2025-02-13 11:39:07,698][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 322
[2025-02-13 11:39:28,399][rgc][INFO] - 	Updating weights of batch 322
[2025-02-13 11:39:28,449][rgc][INFO] - Batch 322, avg loss per batch: 3.8735996955850225
[2025-02-13 11:39:28,450][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 323
[2025-02-13 11:39:49,216][rgc][INFO] - 	Updating weights of batch 323
[2025-02-13 11:39:49,267][rgc][INFO] - Batch 323, avg loss per batch: 2.817800434098662
[2025-02-13 11:39:49,269][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 324
[2025-02-13 11:40:10,082][rgc][INFO] - 	Updating weights of batch 324
[2025-02-13 11:40:10,174][rgc][INFO] - Batch 324, avg loss per batch: 4.615542246987048
[2025-02-13 11:40:10,176][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 325
[2025-02-13 11:40:30,969][rgc][INFO] - 	Updating weights of batch 325
[2025-02-13 11:40:31,033][rgc][INFO] - Batch 325, avg loss per batch: 1.2378998399082157
[2025-02-13 11:40:31,034][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 326
[2025-02-13 11:40:51,769][rgc][INFO] - 	Updating weights of batch 326
[2025-02-13 11:40:51,844][rgc][INFO] - Batch 326, avg loss per batch: 4.695927465835402
[2025-02-13 11:40:51,847][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 327
[2025-02-13 11:41:12,668][rgc][INFO] - 	Updating weights of batch 327
[2025-02-13 11:41:12,725][rgc][INFO] - Batch 327, avg loss per batch: 5.227034783059795
[2025-02-13 11:41:12,726][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 328
[2025-02-13 11:41:33,414][rgc][INFO] - 	Updating weights of batch 328
[2025-02-13 11:41:33,473][rgc][INFO] - Batch 328, avg loss per batch: 2.524768189282282
[2025-02-13 11:41:33,474][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 329
[2025-02-13 11:41:54,248][rgc][INFO] - 	Updating weights of batch 329
[2025-02-13 11:41:54,305][rgc][INFO] - Batch 329, avg loss per batch: 2.0339206657355566
[2025-02-13 11:41:54,306][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 330
[2025-02-13 11:42:15,027][rgc][INFO] - 	Updating weights of batch 330
[2025-02-13 11:42:15,084][rgc][INFO] - Batch 330, avg loss per batch: 5.263879242048086
[2025-02-13 11:42:15,085][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 331
[2025-02-13 11:42:35,802][rgc][INFO] - 	Updating weights of batch 331
[2025-02-13 11:42:35,858][rgc][INFO] - Batch 331, avg loss per batch: 6.780456478519228
[2025-02-13 11:42:35,859][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 332
[2025-02-13 11:42:56,553][rgc][INFO] - 	Updating weights of batch 332
[2025-02-13 11:42:56,611][rgc][INFO] - Batch 332, avg loss per batch: 3.60740343896704
[2025-02-13 11:42:56,612][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 333
[2025-02-13 11:43:17,356][rgc][INFO] - 	Updating weights of batch 333
[2025-02-13 11:43:17,424][rgc][INFO] - Batch 333, avg loss per batch: 1.543264509428421
[2025-02-13 11:43:17,425][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 334
[2025-02-13 11:43:38,147][rgc][INFO] - 	Updating weights of batch 334
[2025-02-13 11:43:38,197][rgc][INFO] - Batch 334, avg loss per batch: 3.791426835537379
[2025-02-13 11:43:38,198][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 335
[2025-02-13 11:43:58,867][rgc][INFO] - 	Updating weights of batch 335
[2025-02-13 11:43:58,920][rgc][INFO] - Batch 335, avg loss per batch: 2.7988119886484637
[2025-02-13 11:43:58,921][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 336
[2025-02-13 11:44:19,688][rgc][INFO] - 	Updating weights of batch 336
[2025-02-13 11:44:19,769][rgc][INFO] - Batch 336, avg loss per batch: 2.7109880322066555
[2025-02-13 11:44:19,770][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 337
[2025-02-13 11:44:40,463][rgc][INFO] - 	Updating weights of batch 337
[2025-02-13 11:44:40,513][rgc][INFO] - Batch 337, avg loss per batch: 4.505527350579117
[2025-02-13 11:44:40,513][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 338
[2025-02-13 11:45:01,246][rgc][INFO] - 	Updating weights of batch 338
[2025-02-13 11:45:01,296][rgc][INFO] - Batch 338, avg loss per batch: 2.8851794137864673
[2025-02-13 11:45:01,298][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 339
[2025-02-13 11:45:22,038][rgc][INFO] - 	Updating weights of batch 339
[2025-02-13 11:45:22,089][rgc][INFO] - Batch 339, avg loss per batch: 2.540925861478714
[2025-02-13 11:45:22,089][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 340
[2025-02-13 11:45:42,851][rgc][INFO] - 	Updating weights of batch 340
[2025-02-13 11:45:42,933][rgc][INFO] - Batch 340, avg loss per batch: 4.281782809544089
[2025-02-13 11:45:42,934][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 341
[2025-02-13 11:46:03,685][rgc][INFO] - 	Updating weights of batch 341
[2025-02-13 11:46:03,736][rgc][INFO] - Batch 341, avg loss per batch: 1.140551659707374
[2025-02-13 11:46:03,737][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 342
[2025-02-13 11:46:24,462][rgc][INFO] - 	Updating weights of batch 342
[2025-02-13 11:46:24,517][rgc][INFO] - Batch 342, avg loss per batch: 4.215499141644706
[2025-02-13 11:46:24,518][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 343
[2025-02-13 11:46:45,270][rgc][INFO] - 	Updating weights of batch 343
[2025-02-13 11:46:45,325][rgc][INFO] - Batch 343, avg loss per batch: 8.632360014680419
[2025-02-13 11:46:45,325][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 344
[2025-02-13 11:47:06,062][rgc][INFO] - 	Updating weights of batch 344
[2025-02-13 11:47:06,130][rgc][INFO] - Batch 344, avg loss per batch: 9.051549710338897
[2025-02-13 11:47:06,131][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 345
[2025-02-13 11:47:26,830][rgc][INFO] - 	Updating weights of batch 345
[2025-02-13 11:47:26,888][rgc][INFO] - Batch 345, avg loss per batch: 3.5621341067481422
[2025-02-13 11:47:26,889][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 346
[2025-02-13 11:47:47,623][rgc][INFO] - 	Updating weights of batch 346
[2025-02-13 11:47:47,679][rgc][INFO] - Batch 346, avg loss per batch: 1.6344271316292402
[2025-02-13 11:47:47,680][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 347
[2025-02-13 11:48:08,440][rgc][INFO] - 	Updating weights of batch 347
[2025-02-13 11:48:08,496][rgc][INFO] - Batch 347, avg loss per batch: 3.877729857166546
[2025-02-13 11:48:08,497][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 348
[2025-02-13 11:48:29,267][rgc][INFO] - 	Updating weights of batch 348
[2025-02-13 11:48:29,326][rgc][INFO] - Batch 348, avg loss per batch: 5.332396115722647
[2025-02-13 11:48:29,327][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 349
[2025-02-13 11:48:50,060][rgc][INFO] - 	Updating weights of batch 349
[2025-02-13 11:48:50,120][rgc][INFO] - Batch 349, avg loss per batch: 2.730055033395158
[2025-02-13 11:48:50,121][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 350
[2025-02-13 11:49:10,862][rgc][INFO] - 	Updating weights of batch 350
[2025-02-13 11:49:10,920][rgc][INFO] - Batch 350, avg loss per batch: 3.120161740413856
[2025-02-13 11:49:10,921][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 351
[2025-02-13 11:49:31,618][rgc][INFO] - 	Updating weights of batch 351
[2025-02-13 11:49:31,698][rgc][INFO] - Batch 351, avg loss per batch: 2.618048306096408
[2025-02-13 11:49:31,699][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 352
[2025-02-13 11:49:52,396][rgc][INFO] - 	Updating weights of batch 352
[2025-02-13 11:49:52,452][rgc][INFO] - Batch 352, avg loss per batch: 2.5754907145657553
[2025-02-13 11:49:52,453][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 353
[2025-02-13 11:50:13,201][rgc][INFO] - 	Updating weights of batch 353
[2025-02-13 11:50:13,264][rgc][INFO] - Batch 353, avg loss per batch: 1.5317243312521627
[2025-02-13 11:50:13,266][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 354
[2025-02-13 11:50:34,035][rgc][INFO] - 	Updating weights of batch 354
[2025-02-13 11:50:34,094][rgc][INFO] - Batch 354, avg loss per batch: 3.0519663285837684
[2025-02-13 11:50:34,096][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 355
[2025-02-13 11:50:54,808][rgc][INFO] - 	Updating weights of batch 355
[2025-02-13 11:50:54,859][rgc][INFO] - Batch 355, avg loss per batch: 4.561495895254815
[2025-02-13 11:50:54,860][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 356
[2025-02-13 11:51:15,588][rgc][INFO] - 	Updating weights of batch 356
[2025-02-13 11:51:15,643][rgc][INFO] - Batch 356, avg loss per batch: 2.87579235328446
[2025-02-13 11:51:15,643][rgc][INFO] - 	Applying batch grad function of epoch 0 and batch 357
[2025-02-13 11:51:36,318][rgc][INFO] - 	Updating weights of batch 357
[2025-02-13 11:51:36,371][rgc][INFO] - Batch 357, avg loss per batch: 1.7364197220625095
[2025-02-13 11:51:36,379][rgc][INFO] - ================= Epoch 0, loss: 1201.7015433246188 ===============
[2025-02-13 11:51:36,379][rgc][INFO] - Visualizing histograms
[2025-02-13 11:51:59,227][rgc][INFO] - AVG rho on val data: 0.1601211778176932
[2025-02-13 11:51:59,228][rgc][INFO] - AVG Mean Absolute Error on val data: 0.552304914685532
[2025-02-13 11:52:10,411][rgc][INFO] - AVG rho on test data: 0.3035928279575264
[2025-02-13 11:52:10,411][rgc][INFO] - AVG Mean Absolute Error on test data: 0.580590007686214
[2025-02-13 11:52:23,258][rgc][INFO] - AVG rho on train data: 0.16025246863261014
[2025-02-13 11:52:23,258][rgc][INFO] - AVG Mean Absolute Error on train data: 0.5689990810694676
[2025-02-13 11:52:23,259][rgc][INFO] - Current best rhos: train 0.16025246863261014, val 0.1601211778176932, test 0.3035928279575264
[2025-02-13 11:52:23,269][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 0
[2025-02-13 11:52:44,008][rgc][INFO] - 	Updating weights of batch 0
[2025-02-13 11:52:44,061][rgc][INFO] - Batch 0, avg loss per batch: 3.7227276088404646
[2025-02-13 11:52:44,062][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 1
[2025-02-13 11:53:04,794][rgc][INFO] - 	Updating weights of batch 1
[2025-02-13 11:53:04,845][rgc][INFO] - Batch 1, avg loss per batch: 2.8054466450558033
[2025-02-13 11:53:04,845][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 2
[2025-02-13 11:53:25,616][rgc][INFO] - 	Updating weights of batch 2
[2025-02-13 11:53:25,670][rgc][INFO] - Batch 2, avg loss per batch: 2.9244908454351166
[2025-02-13 11:53:25,671][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 3
[2025-02-13 11:53:46,441][rgc][INFO] - 	Updating weights of batch 3
[2025-02-13 11:53:46,495][rgc][INFO] - Batch 3, avg loss per batch: 5.343585874509158
[2025-02-13 11:53:46,495][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 4
[2025-02-13 11:54:07,215][rgc][INFO] - 	Updating weights of batch 4
[2025-02-13 11:54:07,264][rgc][INFO] - Batch 4, avg loss per batch: 2.7885669090707235
[2025-02-13 11:54:07,265][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 5
[2025-02-13 11:54:28,014][rgc][INFO] - 	Updating weights of batch 5
[2025-02-13 11:54:28,069][rgc][INFO] - Batch 5, avg loss per batch: 3.097069310923928
[2025-02-13 11:54:28,070][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 6
[2025-02-13 11:54:48,815][rgc][INFO] - 	Updating weights of batch 6
[2025-02-13 11:54:48,872][rgc][INFO] - Batch 6, avg loss per batch: 4.083121670285069
[2025-02-13 11:54:48,873][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 7
[2025-02-13 11:55:09,623][rgc][INFO] - 	Updating weights of batch 7
[2025-02-13 11:55:09,677][rgc][INFO] - Batch 7, avg loss per batch: 1.4454764450602429
[2025-02-13 11:55:09,677][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 8
[2025-02-13 11:55:30,344][rgc][INFO] - 	Updating weights of batch 8
[2025-02-13 11:55:30,397][rgc][INFO] - Batch 8, avg loss per batch: 1.8756673268767843
[2025-02-13 11:55:30,398][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 9
[2025-02-13 11:55:51,123][rgc][INFO] - 	Updating weights of batch 9
[2025-02-13 11:55:51,176][rgc][INFO] - Batch 9, avg loss per batch: 3.436579597846884
[2025-02-13 11:55:51,177][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 10
[2025-02-13 11:56:11,851][rgc][INFO] - 	Updating weights of batch 10
[2025-02-13 11:56:11,898][rgc][INFO] - Batch 10, avg loss per batch: 1.3183563143275157
[2025-02-13 11:56:11,898][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 11
[2025-02-13 11:56:32,636][rgc][INFO] - 	Updating weights of batch 11
[2025-02-13 11:56:32,711][rgc][INFO] - Batch 11, avg loss per batch: 2.3884417318329803
[2025-02-13 11:56:32,712][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 12
[2025-02-13 11:56:53,401][rgc][INFO] - 	Updating weights of batch 12
[2025-02-13 11:56:53,477][rgc][INFO] - Batch 12, avg loss per batch: 2.9628866508651037
[2025-02-13 11:56:53,478][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 13
[2025-02-13 11:57:14,218][rgc][INFO] - 	Updating weights of batch 13
[2025-02-13 11:57:14,277][rgc][INFO] - Batch 13, avg loss per batch: 2.95578212230137
[2025-02-13 11:57:14,277][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 14
[2025-02-13 11:57:34,942][rgc][INFO] - 	Updating weights of batch 14
[2025-02-13 11:57:34,999][rgc][INFO] - Batch 14, avg loss per batch: 2.0567717703065536
[2025-02-13 11:57:35,000][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 15
[2025-02-13 11:57:55,746][rgc][INFO] - 	Updating weights of batch 15
[2025-02-13 11:57:55,801][rgc][INFO] - Batch 15, avg loss per batch: 2.8890208225768417
[2025-02-13 11:57:55,802][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 16
[2025-02-13 11:58:16,472][rgc][INFO] - 	Updating weights of batch 16
[2025-02-13 11:58:16,526][rgc][INFO] - Batch 16, avg loss per batch: 2.6279152432404604
[2025-02-13 11:58:16,527][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 17
[2025-02-13 11:58:37,247][rgc][INFO] - 	Updating weights of batch 17
[2025-02-13 11:58:37,301][rgc][INFO] - Batch 17, avg loss per batch: 3.321022393673615
[2025-02-13 11:58:37,302][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 18
[2025-02-13 11:58:58,065][rgc][INFO] - 	Updating weights of batch 18
[2025-02-13 11:58:58,115][rgc][INFO] - Batch 18, avg loss per batch: 4.044519927675854
[2025-02-13 11:58:58,116][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 19
[2025-02-13 11:59:18,807][rgc][INFO] - 	Updating weights of batch 19
[2025-02-13 11:59:18,862][rgc][INFO] - Batch 19, avg loss per batch: 3.3684324658415328
[2025-02-13 11:59:18,863][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 20
[2025-02-13 11:59:39,633][rgc][INFO] - 	Updating weights of batch 20
[2025-02-13 11:59:39,712][rgc][INFO] - Batch 20, avg loss per batch: 3.277315646785265
[2025-02-13 11:59:39,713][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 21
[2025-02-13 12:00:00,467][rgc][INFO] - 	Updating weights of batch 21
[2025-02-13 12:00:00,523][rgc][INFO] - Batch 21, avg loss per batch: 2.744568721706422
[2025-02-13 12:00:00,524][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 22
[2025-02-13 12:00:21,283][rgc][INFO] - 	Updating weights of batch 22
[2025-02-13 12:00:21,342][rgc][INFO] - Batch 22, avg loss per batch: 3.614192287929117
[2025-02-13 12:00:21,343][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 23
[2025-02-13 12:00:42,097][rgc][INFO] - 	Updating weights of batch 23
[2025-02-13 12:00:42,179][rgc][INFO] - Batch 23, avg loss per batch: 1.5517265407196263
[2025-02-13 12:00:42,180][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 24
[2025-02-13 12:01:02,866][rgc][INFO] - 	Updating weights of batch 24
[2025-02-13 12:01:02,928][rgc][INFO] - Batch 24, avg loss per batch: 2.587800325299229
[2025-02-13 12:01:02,929][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 25
[2025-02-13 12:01:23,658][rgc][INFO] - 	Updating weights of batch 25
[2025-02-13 12:01:23,720][rgc][INFO] - Batch 25, avg loss per batch: 2.06912520811923
[2025-02-13 12:01:23,721][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 26
[2025-02-13 12:01:44,462][rgc][INFO] - 	Updating weights of batch 26
[2025-02-13 12:01:44,517][rgc][INFO] - Batch 26, avg loss per batch: 4.45114834545103
[2025-02-13 12:01:44,517][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 27
[2025-02-13 12:02:05,258][rgc][INFO] - 	Updating weights of batch 27
[2025-02-13 12:02:05,314][rgc][INFO] - Batch 27, avg loss per batch: 2.3806867503470994
[2025-02-13 12:02:05,316][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 28
[2025-02-13 12:02:26,004][rgc][INFO] - 	Updating weights of batch 28
[2025-02-13 12:02:26,059][rgc][INFO] - Batch 28, avg loss per batch: 2.1694613980782695
[2025-02-13 12:02:26,059][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 29
[2025-02-13 12:02:46,781][rgc][INFO] - 	Updating weights of batch 29
[2025-02-13 12:02:46,833][rgc][INFO] - Batch 29, avg loss per batch: 1.4453295694725972
[2025-02-13 12:02:46,834][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 30
[2025-02-13 12:03:07,589][rgc][INFO] - 	Updating weights of batch 30
[2025-02-13 12:03:07,641][rgc][INFO] - Batch 30, avg loss per batch: 2.18142815704895
[2025-02-13 12:03:07,642][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 31
[2025-02-13 12:03:28,365][rgc][INFO] - 	Updating weights of batch 31
[2025-02-13 12:03:28,415][rgc][INFO] - Batch 31, avg loss per batch: 0.9749686797376604
[2025-02-13 12:03:28,416][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 32
[2025-02-13 12:03:49,100][rgc][INFO] - 	Updating weights of batch 32
[2025-02-13 12:03:49,153][rgc][INFO] - Batch 32, avg loss per batch: 3.101011472230713
[2025-02-13 12:03:49,153][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 33
[2025-02-13 12:04:09,816][rgc][INFO] - 	Updating weights of batch 33
[2025-02-13 12:04:09,869][rgc][INFO] - Batch 33, avg loss per batch: 1.7017975193358659
[2025-02-13 12:04:09,870][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 34
[2025-02-13 12:04:30,619][rgc][INFO] - 	Updating weights of batch 34
[2025-02-13 12:04:30,671][rgc][INFO] - Batch 34, avg loss per batch: 2.482129477753258
[2025-02-13 12:04:30,671][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 35
[2025-02-13 12:04:51,371][rgc][INFO] - 	Updating weights of batch 35
[2025-02-13 12:04:51,426][rgc][INFO] - Batch 35, avg loss per batch: 2.1607534868304707
[2025-02-13 12:04:51,427][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 36
[2025-02-13 12:05:12,165][rgc][INFO] - 	Updating weights of batch 36
[2025-02-13 12:05:12,214][rgc][INFO] - Batch 36, avg loss per batch: 2.300243136268419
[2025-02-13 12:05:12,215][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 37
[2025-02-13 12:05:32,986][rgc][INFO] - 	Updating weights of batch 37
[2025-02-13 12:05:33,042][rgc][INFO] - Batch 37, avg loss per batch: 1.491670395800252
[2025-02-13 12:05:33,043][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 38
[2025-02-13 12:05:53,806][rgc][INFO] - 	Updating weights of batch 38
[2025-02-13 12:05:53,857][rgc][INFO] - Batch 38, avg loss per batch: 2.4904052544956983
[2025-02-13 12:05:53,857][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 39
[2025-02-13 12:06:14,527][rgc][INFO] - 	Updating weights of batch 39
[2025-02-13 12:06:14,581][rgc][INFO] - Batch 39, avg loss per batch: 4.006796527537051
[2025-02-13 12:06:14,582][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 40
[2025-02-13 12:06:35,249][rgc][INFO] - 	Updating weights of batch 40
[2025-02-13 12:06:35,301][rgc][INFO] - Batch 40, avg loss per batch: 1.8978731061731915
[2025-02-13 12:06:35,302][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 41
[2025-02-13 12:06:56,025][rgc][INFO] - 	Updating weights of batch 41
[2025-02-13 12:06:56,077][rgc][INFO] - Batch 41, avg loss per batch: 1.9844766448006834
[2025-02-13 12:06:56,078][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 42
[2025-02-13 12:07:16,820][rgc][INFO] - 	Updating weights of batch 42
[2025-02-13 12:07:16,882][rgc][INFO] - Batch 42, avg loss per batch: 3.306891476374296
[2025-02-13 12:07:16,883][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 43
[2025-02-13 12:07:37,606][rgc][INFO] - 	Updating weights of batch 43
[2025-02-13 12:07:37,665][rgc][INFO] - Batch 43, avg loss per batch: 3.025377290887171
[2025-02-13 12:07:37,666][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 44
[2025-02-13 12:07:58,406][rgc][INFO] - 	Updating weights of batch 44
[2025-02-13 12:07:58,458][rgc][INFO] - Batch 44, avg loss per batch: 5.399764427766318
[2025-02-13 12:07:58,459][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 45
[2025-02-13 12:08:19,147][rgc][INFO] - 	Updating weights of batch 45
[2025-02-13 12:08:19,199][rgc][INFO] - Batch 45, avg loss per batch: 3.2622538856233967
[2025-02-13 12:08:19,199][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 46
[2025-02-13 12:08:39,873][rgc][INFO] - 	Updating weights of batch 46
[2025-02-13 12:08:39,936][rgc][INFO] - Batch 46, avg loss per batch: 5.130633580398199
[2025-02-13 12:08:39,936][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 47
[2025-02-13 12:09:00,675][rgc][INFO] - 	Updating weights of batch 47
[2025-02-13 12:09:00,729][rgc][INFO] - Batch 47, avg loss per batch: 2.0426045455601973
[2025-02-13 12:09:00,731][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 48
[2025-02-13 12:09:21,396][rgc][INFO] - 	Updating weights of batch 48
[2025-02-13 12:09:21,455][rgc][INFO] - Batch 48, avg loss per batch: 2.1177142424853748
[2025-02-13 12:09:21,457][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 49
[2025-02-13 12:09:42,139][rgc][INFO] - 	Updating weights of batch 49
[2025-02-13 12:09:42,195][rgc][INFO] - Batch 49, avg loss per batch: 2.445434720479134
[2025-02-13 12:09:42,196][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 50
[2025-02-13 12:10:02,964][rgc][INFO] - 	Updating weights of batch 50
[2025-02-13 12:10:03,018][rgc][INFO] - Batch 50, avg loss per batch: 3.4554525467628663
[2025-02-13 12:10:03,019][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 51
[2025-02-13 12:10:23,716][rgc][INFO] - 	Updating weights of batch 51
[2025-02-13 12:10:23,765][rgc][INFO] - Batch 51, avg loss per batch: 1.3002406546804461
[2025-02-13 12:10:23,766][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 52
[2025-02-13 12:10:44,527][rgc][INFO] - 	Updating weights of batch 52
[2025-02-13 12:10:44,585][rgc][INFO] - Batch 52, avg loss per batch: 3.50716826019855
[2025-02-13 12:10:44,586][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 53
[2025-02-13 12:11:05,264][rgc][INFO] - 	Updating weights of batch 53
[2025-02-13 12:11:05,317][rgc][INFO] - Batch 53, avg loss per batch: 2.2921432748914894
[2025-02-13 12:11:05,318][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 54
[2025-02-13 12:11:26,070][rgc][INFO] - 	Updating weights of batch 54
[2025-02-13 12:11:26,138][rgc][INFO] - Batch 54, avg loss per batch: 2.0813705369099678
[2025-02-13 12:11:26,140][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 55
[2025-02-13 12:11:46,868][rgc][INFO] - 	Updating weights of batch 55
[2025-02-13 12:11:46,923][rgc][INFO] - Batch 55, avg loss per batch: 3.7493017713656904
[2025-02-13 12:11:46,923][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 56
[2025-02-13 12:12:07,660][rgc][INFO] - 	Updating weights of batch 56
[2025-02-13 12:12:07,716][rgc][INFO] - Batch 56, avg loss per batch: 2.023068947964796
[2025-02-13 12:12:07,717][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 57
[2025-02-13 12:12:28,449][rgc][INFO] - 	Updating weights of batch 57
[2025-02-13 12:12:28,518][rgc][INFO] - Batch 57, avg loss per batch: 2.4455843772584878
[2025-02-13 12:12:28,519][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 58
[2025-02-13 12:12:49,215][rgc][INFO] - 	Updating weights of batch 58
[2025-02-13 12:12:49,268][rgc][INFO] - Batch 58, avg loss per batch: 1.8856842553649948
[2025-02-13 12:12:49,269][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 59
[2025-02-13 12:13:09,949][rgc][INFO] - 	Updating weights of batch 59
[2025-02-13 12:13:10,000][rgc][INFO] - Batch 59, avg loss per batch: 2.428794811976922
[2025-02-13 12:13:10,001][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 60
[2025-02-13 12:13:30,673][rgc][INFO] - 	Updating weights of batch 60
[2025-02-13 12:13:30,727][rgc][INFO] - Batch 60, avg loss per batch: 4.549092166902435
[2025-02-13 12:13:30,728][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 61
[2025-02-13 12:13:51,405][rgc][INFO] - 	Updating weights of batch 61
[2025-02-13 12:13:51,466][rgc][INFO] - Batch 61, avg loss per batch: 1.6212550783693855
[2025-02-13 12:13:51,467][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 62
[2025-02-13 12:14:12,231][rgc][INFO] - 	Updating weights of batch 62
[2025-02-13 12:14:12,282][rgc][INFO] - Batch 62, avg loss per batch: 1.6299034200686116
[2025-02-13 12:14:12,283][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 63
[2025-02-13 12:14:32,947][rgc][INFO] - 	Updating weights of batch 63
[2025-02-13 12:14:32,997][rgc][INFO] - Batch 63, avg loss per batch: 3.1798765066998778
[2025-02-13 12:14:32,998][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 64
[2025-02-13 12:14:53,668][rgc][INFO] - 	Updating weights of batch 64
[2025-02-13 12:14:53,718][rgc][INFO] - Batch 64, avg loss per batch: 3.897479457584084
[2025-02-13 12:14:53,719][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 65
[2025-02-13 12:15:14,449][rgc][INFO] - 	Updating weights of batch 65
[2025-02-13 12:15:14,501][rgc][INFO] - Batch 65, avg loss per batch: 1.7884275403336634
[2025-02-13 12:15:14,502][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 66
[2025-02-13 12:15:35,171][rgc][INFO] - 	Updating weights of batch 66
[2025-02-13 12:15:35,226][rgc][INFO] - Batch 66, avg loss per batch: 2.46400083564348
[2025-02-13 12:15:35,226][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 67
[2025-02-13 12:15:55,912][rgc][INFO] - 	Updating weights of batch 67
[2025-02-13 12:15:55,968][rgc][INFO] - Batch 67, avg loss per batch: 3.0215130709910305
[2025-02-13 12:15:55,969][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 68
[2025-02-13 12:16:16,694][rgc][INFO] - 	Updating weights of batch 68
[2025-02-13 12:16:16,746][rgc][INFO] - Batch 68, avg loss per batch: 2.097200863070538
[2025-02-13 12:16:16,747][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 69
[2025-02-13 12:16:37,508][rgc][INFO] - 	Updating weights of batch 69
[2025-02-13 12:16:37,564][rgc][INFO] - Batch 69, avg loss per batch: 2.512988280680823
[2025-02-13 12:16:37,565][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 70
[2025-02-13 12:16:58,311][rgc][INFO] - 	Updating weights of batch 70
[2025-02-13 12:16:58,366][rgc][INFO] - Batch 70, avg loss per batch: 2.8100553278654394
[2025-02-13 12:16:58,367][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 71
[2025-02-13 12:17:19,042][rgc][INFO] - 	Updating weights of batch 71
[2025-02-13 12:17:19,098][rgc][INFO] - Batch 71, avg loss per batch: 2.554971405914575
[2025-02-13 12:17:19,099][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 72
[2025-02-13 12:17:39,768][rgc][INFO] - 	Updating weights of batch 72
[2025-02-13 12:17:39,818][rgc][INFO] - Batch 72, avg loss per batch: 3.4602750404014655
[2025-02-13 12:17:39,819][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 73
[2025-02-13 12:18:00,500][rgc][INFO] - 	Updating weights of batch 73
[2025-02-13 12:18:00,551][rgc][INFO] - Batch 73, avg loss per batch: 3.057242750420242
[2025-02-13 12:18:00,552][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 74
[2025-02-13 12:18:21,236][rgc][INFO] - 	Updating weights of batch 74
[2025-02-13 12:18:21,288][rgc][INFO] - Batch 74, avg loss per batch: 1.5782991949508691
[2025-02-13 12:18:21,288][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 75
[2025-02-13 12:18:41,946][rgc][INFO] - 	Updating weights of batch 75
[2025-02-13 12:18:41,999][rgc][INFO] - Batch 75, avg loss per batch: 2.241719660483505
[2025-02-13 12:18:42,000][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 76
[2025-02-13 12:19:02,690][rgc][INFO] - 	Updating weights of batch 76
[2025-02-13 12:19:02,750][rgc][INFO] - Batch 76, avg loss per batch: 1.9188299046970136
[2025-02-13 12:19:02,751][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 77
[2025-02-13 12:19:23,431][rgc][INFO] - 	Updating weights of batch 77
[2025-02-13 12:19:23,485][rgc][INFO] - Batch 77, avg loss per batch: 2.881273648654767
[2025-02-13 12:19:23,486][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 78
[2025-02-13 12:19:44,169][rgc][INFO] - 	Updating weights of batch 78
[2025-02-13 12:19:44,222][rgc][INFO] - Batch 78, avg loss per batch: 1.8552714005328452
[2025-02-13 12:19:44,223][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 79
[2025-02-13 12:20:04,951][rgc][INFO] - 	Updating weights of batch 79
[2025-02-13 12:20:05,019][rgc][INFO] - Batch 79, avg loss per batch: 1.2160141434449083
[2025-02-13 12:20:05,020][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 80
[2025-02-13 12:20:25,782][rgc][INFO] - 	Updating weights of batch 80
[2025-02-13 12:20:25,834][rgc][INFO] - Batch 80, avg loss per batch: 3.2279682689650313
[2025-02-13 12:20:25,835][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 81
[2025-02-13 12:20:46,586][rgc][INFO] - 	Updating weights of batch 81
[2025-02-13 12:20:46,641][rgc][INFO] - Batch 81, avg loss per batch: 3.0718111980423717
[2025-02-13 12:20:46,642][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 82
[2025-02-13 12:21:07,306][rgc][INFO] - 	Updating weights of batch 82
[2025-02-13 12:21:07,362][rgc][INFO] - Batch 82, avg loss per batch: 2.6038145382725464
[2025-02-13 12:21:07,363][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 83
[2025-02-13 12:21:28,112][rgc][INFO] - 	Updating weights of batch 83
[2025-02-13 12:21:28,166][rgc][INFO] - Batch 83, avg loss per batch: 2.430982082445393
[2025-02-13 12:21:28,167][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 84
[2025-02-13 12:21:48,894][rgc][INFO] - 	Updating weights of batch 84
[2025-02-13 12:21:48,958][rgc][INFO] - Batch 84, avg loss per batch: 3.113526883120703
[2025-02-13 12:21:48,959][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 85
[2025-02-13 12:22:09,701][rgc][INFO] - 	Updating weights of batch 85
[2025-02-13 12:22:09,753][rgc][INFO] - Batch 85, avg loss per batch: 3.483254408172611
[2025-02-13 12:22:09,754][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 86
[2025-02-13 12:22:30,498][rgc][INFO] - 	Updating weights of batch 86
[2025-02-13 12:22:30,561][rgc][INFO] - Batch 86, avg loss per batch: 1.7186090047133666
[2025-02-13 12:22:30,562][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 87
[2025-02-13 12:22:51,227][rgc][INFO] - 	Updating weights of batch 87
[2025-02-13 12:22:51,280][rgc][INFO] - Batch 87, avg loss per batch: 2.47029887321131
[2025-02-13 12:22:51,281][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 88
[2025-02-13 12:23:11,961][rgc][INFO] - 	Updating weights of batch 88
[2025-02-13 12:23:12,014][rgc][INFO] - Batch 88, avg loss per batch: 3.641637098096312
[2025-02-13 12:23:12,015][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 89
[2025-02-13 12:23:32,759][rgc][INFO] - 	Updating weights of batch 89
[2025-02-13 12:23:32,814][rgc][INFO] - Batch 89, avg loss per batch: 1.7072353099742952
[2025-02-13 12:23:32,815][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 90
[2025-02-13 12:23:53,499][rgc][INFO] - 	Updating weights of batch 90
[2025-02-13 12:23:53,551][rgc][INFO] - Batch 90, avg loss per batch: 2.5201246380110653
[2025-02-13 12:23:53,552][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 91
[2025-02-13 12:24:14,217][rgc][INFO] - 	Updating weights of batch 91
[2025-02-13 12:24:14,271][rgc][INFO] - Batch 91, avg loss per batch: 3.8075857126124335
[2025-02-13 12:24:14,272][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 92
[2025-02-13 12:24:34,950][rgc][INFO] - 	Updating weights of batch 92
[2025-02-13 12:24:35,006][rgc][INFO] - Batch 92, avg loss per batch: 2.241511233009588
[2025-02-13 12:24:35,007][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 93
[2025-02-13 12:24:55,676][rgc][INFO] - 	Updating weights of batch 93
[2025-02-13 12:24:55,735][rgc][INFO] - Batch 93, avg loss per batch: 3.834309590391416
[2025-02-13 12:24:55,736][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 94
[2025-02-13 12:25:16,412][rgc][INFO] - 	Updating weights of batch 94
[2025-02-13 12:25:16,476][rgc][INFO] - Batch 94, avg loss per batch: 5.375208050203347
[2025-02-13 12:25:16,477][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 95
[2025-02-13 12:25:37,152][rgc][INFO] - 	Updating weights of batch 95
[2025-02-13 12:25:37,205][rgc][INFO] - Batch 95, avg loss per batch: 2.547578989718852
[2025-02-13 12:25:37,205][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 96
[2025-02-13 12:25:57,894][rgc][INFO] - 	Updating weights of batch 96
[2025-02-13 12:25:57,945][rgc][INFO] - Batch 96, avg loss per batch: 1.912650251253302
[2025-02-13 12:25:57,946][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 97
[2025-02-13 12:26:18,684][rgc][INFO] - 	Updating weights of batch 97
[2025-02-13 12:26:18,747][rgc][INFO] - Batch 97, avg loss per batch: 2.275222079010026
[2025-02-13 12:26:18,748][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 98
[2025-02-13 12:26:39,430][rgc][INFO] - 	Updating weights of batch 98
[2025-02-13 12:26:39,486][rgc][INFO] - Batch 98, avg loss per batch: 4.404631919378439
[2025-02-13 12:26:39,487][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 99
[2025-02-13 12:27:00,211][rgc][INFO] - 	Updating weights of batch 99
[2025-02-13 12:27:00,266][rgc][INFO] - Batch 99, avg loss per batch: 2.2906363438782273
[2025-02-13 12:27:11,782][rgc][INFO] - AVG rho on val data: 0.25065243033438855
[2025-02-13 12:27:11,782][rgc][INFO] - AVG mae on val data: 0.54237401556063
[2025-02-13 12:27:22,951][rgc][INFO] - AVG rho on test data: 0.2938951854929506
[2025-02-13 12:27:22,951][rgc][INFO] - AVG mae on test data: 0.577700763507422
[2025-02-13 12:27:35,789][rgc][INFO] - AVG rho on train data: 0.2063941698696264
[2025-02-13 12:27:35,790][rgc][INFO] - AVG mae on train data: 0.5651175702597983
[2025-02-13 12:27:35,791][rgc][INFO] - Current best rhos: train 0.2063941698696264, val 0.25065243033438855, test 0.2938951854929506
[2025-02-13 12:27:35,792][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 100
[2025-02-13 12:27:56,472][rgc][INFO] - 	Updating weights of batch 100
[2025-02-13 12:27:56,526][rgc][INFO] - Batch 100, avg loss per batch: 2.0352777852431907
[2025-02-13 12:27:56,527][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 101
[2025-02-13 12:28:17,206][rgc][INFO] - 	Updating weights of batch 101
[2025-02-13 12:28:17,257][rgc][INFO] - Batch 101, avg loss per batch: 3.8800060370677443
[2025-02-13 12:28:17,259][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 102
[2025-02-13 12:28:37,933][rgc][INFO] - 	Updating weights of batch 102
[2025-02-13 12:28:37,985][rgc][INFO] - Batch 102, avg loss per batch: 2.1229843593916042
[2025-02-13 12:28:37,986][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 103
[2025-02-13 12:28:58,652][rgc][INFO] - 	Updating weights of batch 103
[2025-02-13 12:28:58,703][rgc][INFO] - Batch 103, avg loss per batch: 3.9652523876676833
[2025-02-13 12:28:58,704][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 104
[2025-02-13 12:29:19,385][rgc][INFO] - 	Updating weights of batch 104
[2025-02-13 12:29:19,441][rgc][INFO] - Batch 104, avg loss per batch: 2.999827195200568
[2025-02-13 12:29:19,442][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 105
[2025-02-13 12:29:40,211][rgc][INFO] - 	Updating weights of batch 105
[2025-02-13 12:29:40,266][rgc][INFO] - Batch 105, avg loss per batch: 2.0098958866211616
[2025-02-13 12:29:40,267][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 106
[2025-02-13 12:30:01,035][rgc][INFO] - 	Updating weights of batch 106
[2025-02-13 12:30:01,103][rgc][INFO] - Batch 106, avg loss per batch: 1.9484662081695232
[2025-02-13 12:30:01,104][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 107
[2025-02-13 12:30:21,854][rgc][INFO] - 	Updating weights of batch 107
[2025-02-13 12:30:21,909][rgc][INFO] - Batch 107, avg loss per batch: 4.363422598287291
[2025-02-13 12:30:21,909][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 108
[2025-02-13 12:30:42,575][rgc][INFO] - 	Updating weights of batch 108
[2025-02-13 12:30:42,626][rgc][INFO] - Batch 108, avg loss per batch: 2.284866839040612
[2025-02-13 12:30:42,627][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 109
[2025-02-13 12:31:03,377][rgc][INFO] - 	Updating weights of batch 109
[2025-02-13 12:31:03,425][rgc][INFO] - Batch 109, avg loss per batch: 2.6340167335703826
[2025-02-13 12:31:03,426][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 110
[2025-02-13 12:31:24,150][rgc][INFO] - 	Updating weights of batch 110
[2025-02-13 12:31:24,200][rgc][INFO] - Batch 110, avg loss per batch: 2.458386289760157
[2025-02-13 12:31:24,201][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 111
[2025-02-13 12:31:44,874][rgc][INFO] - 	Updating weights of batch 111
[2025-02-13 12:31:44,928][rgc][INFO] - Batch 111, avg loss per batch: 2.5401196304679163
[2025-02-13 12:31:44,929][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 112
[2025-02-13 12:32:05,608][rgc][INFO] - 	Updating weights of batch 112
[2025-02-13 12:32:05,664][rgc][INFO] - Batch 112, avg loss per batch: 2.4125221927017826
[2025-02-13 12:32:05,665][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 113
[2025-02-13 12:32:26,340][rgc][INFO] - 	Updating weights of batch 113
[2025-02-13 12:32:26,393][rgc][INFO] - Batch 113, avg loss per batch: 1.5087579069406625
[2025-02-13 12:32:26,394][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 114
[2025-02-13 12:32:47,071][rgc][INFO] - 	Updating weights of batch 114
[2025-02-13 12:32:47,123][rgc][INFO] - Batch 114, avg loss per batch: 1.9278102636335332
[2025-02-13 12:32:47,124][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 115
[2025-02-13 12:33:07,807][rgc][INFO] - 	Updating weights of batch 115
[2025-02-13 12:33:07,860][rgc][INFO] - Batch 115, avg loss per batch: 1.2597326508906481
[2025-02-13 12:33:07,861][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 116
[2025-02-13 12:33:28,602][rgc][INFO] - 	Updating weights of batch 116
[2025-02-13 12:33:28,653][rgc][INFO] - Batch 116, avg loss per batch: 2.175094314981502
[2025-02-13 12:33:28,654][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 117
[2025-02-13 12:33:49,338][rgc][INFO] - 	Updating weights of batch 117
[2025-02-13 12:33:49,388][rgc][INFO] - Batch 117, avg loss per batch: 1.5829894694429019
[2025-02-13 12:33:49,389][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 118
[2025-02-13 12:34:10,063][rgc][INFO] - 	Updating weights of batch 118
[2025-02-13 12:34:10,116][rgc][INFO] - Batch 118, avg loss per batch: 3.2111546893371252
[2025-02-13 12:34:10,117][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 119
[2025-02-13 12:34:30,883][rgc][INFO] - 	Updating weights of batch 119
[2025-02-13 12:34:30,936][rgc][INFO] - Batch 119, avg loss per batch: 4.24463387134714
[2025-02-13 12:34:30,937][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 120
[2025-02-13 12:34:51,612][rgc][INFO] - 	Updating weights of batch 120
[2025-02-13 12:34:51,663][rgc][INFO] - Batch 120, avg loss per batch: 1.7781764895057814
[2025-02-13 12:34:51,664][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 121
[2025-02-13 12:35:12,421][rgc][INFO] - 	Updating weights of batch 121
[2025-02-13 12:35:12,473][rgc][INFO] - Batch 121, avg loss per batch: 1.046950984193764
[2025-02-13 12:35:12,474][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 122
[2025-02-13 12:35:33,232][rgc][INFO] - 	Updating weights of batch 122
[2025-02-13 12:35:33,285][rgc][INFO] - Batch 122, avg loss per batch: 2.9153817899810472
[2025-02-13 12:35:33,285][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 123
[2025-02-13 12:35:53,957][rgc][INFO] - 	Updating weights of batch 123
[2025-02-13 12:35:54,009][rgc][INFO] - Batch 123, avg loss per batch: 4.662742662880934
[2025-02-13 12:35:54,010][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 124
[2025-02-13 12:36:14,733][rgc][INFO] - 	Updating weights of batch 124
[2025-02-13 12:36:14,787][rgc][INFO] - Batch 124, avg loss per batch: 3.6958833114358565
[2025-02-13 12:36:14,788][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 125
[2025-02-13 12:36:35,531][rgc][INFO] - 	Updating weights of batch 125
[2025-02-13 12:36:35,604][rgc][INFO] - Batch 125, avg loss per batch: 2.9667735513608533
[2025-02-13 12:36:35,605][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 126
[2025-02-13 12:36:56,273][rgc][INFO] - 	Updating weights of batch 126
[2025-02-13 12:36:56,354][rgc][INFO] - Batch 126, avg loss per batch: 2.380920664892579
[2025-02-13 12:36:56,355][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 127
[2025-02-13 12:37:17,074][rgc][INFO] - 	Updating weights of batch 127
[2025-02-13 12:37:17,125][rgc][INFO] - Batch 127, avg loss per batch: 3.033979029839208
[2025-02-13 12:37:17,125][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 128
[2025-02-13 12:37:37,807][rgc][INFO] - 	Updating weights of batch 128
[2025-02-13 12:37:37,857][rgc][INFO] - Batch 128, avg loss per batch: 3.2310938335854944
[2025-02-13 12:37:37,857][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 129
[2025-02-13 12:37:58,546][rgc][INFO] - 	Updating weights of batch 129
[2025-02-13 12:37:58,601][rgc][INFO] - Batch 129, avg loss per batch: 1.6191466021495113
[2025-02-13 12:37:58,602][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 130
[2025-02-13 12:38:19,273][rgc][INFO] - 	Updating weights of batch 130
[2025-02-13 12:38:19,329][rgc][INFO] - Batch 130, avg loss per batch: 1.5050330848793454
[2025-02-13 12:38:19,330][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 131
[2025-02-13 12:38:40,011][rgc][INFO] - 	Updating weights of batch 131
[2025-02-13 12:38:40,064][rgc][INFO] - Batch 131, avg loss per batch: 3.3639751427458937
[2025-02-13 12:38:40,065][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 132
[2025-02-13 12:39:00,795][rgc][INFO] - 	Updating weights of batch 132
[2025-02-13 12:39:00,858][rgc][INFO] - Batch 132, avg loss per batch: 2.289251369193339
[2025-02-13 12:39:00,859][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 133
[2025-02-13 12:39:21,606][rgc][INFO] - 	Updating weights of batch 133
[2025-02-13 12:39:21,658][rgc][INFO] - Batch 133, avg loss per batch: 2.6733219207207863
[2025-02-13 12:39:21,659][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 134
[2025-02-13 12:39:42,329][rgc][INFO] - 	Updating weights of batch 134
[2025-02-13 12:39:42,386][rgc][INFO] - Batch 134, avg loss per batch: 3.827557344851301
[2025-02-13 12:39:42,387][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 135
[2025-02-13 12:40:03,078][rgc][INFO] - 	Updating weights of batch 135
[2025-02-13 12:40:03,146][rgc][INFO] - Batch 135, avg loss per batch: 1.4130366731313853
[2025-02-13 12:40:03,147][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 136
[2025-02-13 12:40:23,807][rgc][INFO] - 	Updating weights of batch 136
[2025-02-13 12:40:23,863][rgc][INFO] - Batch 136, avg loss per batch: 1.6702652301119785
[2025-02-13 12:40:23,864][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 137
[2025-02-13 12:40:44,588][rgc][INFO] - 	Updating weights of batch 137
[2025-02-13 12:40:44,641][rgc][INFO] - Batch 137, avg loss per batch: 5.412846611143811
[2025-02-13 12:40:44,642][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 138
[2025-02-13 12:41:05,322][rgc][INFO] - 	Updating weights of batch 138
[2025-02-13 12:41:05,374][rgc][INFO] - Batch 138, avg loss per batch: 2.262832230450347
[2025-02-13 12:41:05,375][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 139
[2025-02-13 12:41:26,102][rgc][INFO] - 	Updating weights of batch 139
[2025-02-13 12:41:26,154][rgc][INFO] - Batch 139, avg loss per batch: 1.6926157689093484
[2025-02-13 12:41:26,155][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 140
[2025-02-13 12:41:46,923][rgc][INFO] - 	Updating weights of batch 140
[2025-02-13 12:41:46,976][rgc][INFO] - Batch 140, avg loss per batch: 2.3553900401742216
[2025-02-13 12:41:46,977][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 141
[2025-02-13 12:42:07,650][rgc][INFO] - 	Updating weights of batch 141
[2025-02-13 12:42:07,702][rgc][INFO] - Batch 141, avg loss per batch: 5.301778844482748
[2025-02-13 12:42:07,702][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 142
[2025-02-13 12:42:28,386][rgc][INFO] - 	Updating weights of batch 142
[2025-02-13 12:42:28,448][rgc][INFO] - Batch 142, avg loss per batch: 2.4131223622017064
[2025-02-13 12:42:28,449][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 143
[2025-02-13 12:42:49,164][rgc][INFO] - 	Updating weights of batch 143
[2025-02-13 12:42:49,214][rgc][INFO] - Batch 143, avg loss per batch: 3.7299094791220657
[2025-02-13 12:42:49,214][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 144
[2025-02-13 12:43:09,901][rgc][INFO] - 	Updating weights of batch 144
[2025-02-13 12:43:09,973][rgc][INFO] - Batch 144, avg loss per batch: 3.68711430393458
[2025-02-13 12:43:09,974][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 145
[2025-02-13 12:43:30,628][rgc][INFO] - 	Updating weights of batch 145
[2025-02-13 12:43:30,678][rgc][INFO] - Batch 145, avg loss per batch: 3.60923933034868
[2025-02-13 12:43:30,679][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 146
[2025-02-13 12:43:51,408][rgc][INFO] - 	Updating weights of batch 146
[2025-02-13 12:43:51,457][rgc][INFO] - Batch 146, avg loss per batch: 3.2981205979219195
[2025-02-13 12:43:51,457][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 147
[2025-02-13 12:44:12,196][rgc][INFO] - 	Updating weights of batch 147
[2025-02-13 12:44:12,249][rgc][INFO] - Batch 147, avg loss per batch: 5.02649695352225
[2025-02-13 12:44:12,250][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 148
[2025-02-13 12:44:33,020][rgc][INFO] - 	Updating weights of batch 148
[2025-02-13 12:44:33,086][rgc][INFO] - Batch 148, avg loss per batch: 2.3223654761196375
[2025-02-13 12:44:33,087][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 149
[2025-02-13 12:44:53,752][rgc][INFO] - 	Updating weights of batch 149
[2025-02-13 12:44:53,803][rgc][INFO] - Batch 149, avg loss per batch: 3.8164151133345143
[2025-02-13 12:44:53,804][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 150
[2025-02-13 12:45:14,576][rgc][INFO] - 	Updating weights of batch 150
[2025-02-13 12:45:14,636][rgc][INFO] - Batch 150, avg loss per batch: 3.225415997938948
[2025-02-13 12:45:14,636][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 151
[2025-02-13 12:45:35,354][rgc][INFO] - 	Updating weights of batch 151
[2025-02-13 12:45:35,404][rgc][INFO] - Batch 151, avg loss per batch: 4.211363401852337
[2025-02-13 12:45:35,404][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 152
[2025-02-13 12:45:56,068][rgc][INFO] - 	Updating weights of batch 152
[2025-02-13 12:45:56,120][rgc][INFO] - Batch 152, avg loss per batch: 2.1096871831714976
[2025-02-13 12:45:56,121][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 153
[2025-02-13 12:46:16,791][rgc][INFO] - 	Updating weights of batch 153
[2025-02-13 12:46:16,873][rgc][INFO] - Batch 153, avg loss per batch: 3.2979098824923407
[2025-02-13 12:46:16,874][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 154
[2025-02-13 12:46:37,528][rgc][INFO] - 	Updating weights of batch 154
[2025-02-13 12:46:37,581][rgc][INFO] - Batch 154, avg loss per batch: 3.1327700904308236
[2025-02-13 12:46:37,581][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 155
[2025-02-13 12:46:58,254][rgc][INFO] - 	Updating weights of batch 155
[2025-02-13 12:46:58,325][rgc][INFO] - Batch 155, avg loss per batch: 1.7390572350525737
[2025-02-13 12:46:58,325][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 156
[2025-02-13 12:47:19,013][rgc][INFO] - 	Updating weights of batch 156
[2025-02-13 12:47:19,064][rgc][INFO] - Batch 156, avg loss per batch: 5.216987033858718
[2025-02-13 12:47:19,064][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 157
[2025-02-13 12:47:39,760][rgc][INFO] - 	Updating weights of batch 157
[2025-02-13 12:47:39,816][rgc][INFO] - Batch 157, avg loss per batch: 3.009792632185383
[2025-02-13 12:47:39,817][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 158
[2025-02-13 12:48:00,511][rgc][INFO] - 	Updating weights of batch 158
[2025-02-13 12:48:00,561][rgc][INFO] - Batch 158, avg loss per batch: 3.7257725910919697
[2025-02-13 12:48:00,562][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 159
[2025-02-13 12:48:21,238][rgc][INFO] - 	Updating weights of batch 159
[2025-02-13 12:48:21,289][rgc][INFO] - Batch 159, avg loss per batch: 3.446254079844688
[2025-02-13 12:48:21,290][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 160
[2025-02-13 12:48:41,977][rgc][INFO] - 	Updating weights of batch 160
[2025-02-13 12:48:42,038][rgc][INFO] - Batch 160, avg loss per batch: 2.2213021452369017
[2025-02-13 12:48:42,039][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 161
[2025-02-13 12:49:02,777][rgc][INFO] - 	Updating weights of batch 161
[2025-02-13 12:49:02,831][rgc][INFO] - Batch 161, avg loss per batch: 2.3516358859211586
[2025-02-13 12:49:02,831][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 162
[2025-02-13 12:49:23,568][rgc][INFO] - 	Updating weights of batch 162
[2025-02-13 12:49:23,626][rgc][INFO] - Batch 162, avg loss per batch: 4.411471177140349
[2025-02-13 12:49:23,627][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 163
[2025-02-13 12:49:44,400][rgc][INFO] - 	Updating weights of batch 163
[2025-02-13 12:49:44,454][rgc][INFO] - Batch 163, avg loss per batch: 4.28215896728785
[2025-02-13 12:49:44,455][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 164
[2025-02-13 12:50:05,214][rgc][INFO] - 	Updating weights of batch 164
[2025-02-13 12:50:05,276][rgc][INFO] - Batch 164, avg loss per batch: 2.6210405028236896
[2025-02-13 12:50:05,277][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 165
[2025-02-13 12:50:25,980][rgc][INFO] - 	Updating weights of batch 165
[2025-02-13 12:50:26,056][rgc][INFO] - Batch 165, avg loss per batch: 0.89109501681457
[2025-02-13 12:50:26,057][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 166
[2025-02-13 12:50:46,742][rgc][INFO] - 	Updating weights of batch 166
[2025-02-13 12:50:46,791][rgc][INFO] - Batch 166, avg loss per batch: 2.780725457664193
[2025-02-13 12:50:46,792][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 167
[2025-02-13 12:51:07,467][rgc][INFO] - 	Updating weights of batch 167
[2025-02-13 12:51:07,519][rgc][INFO] - Batch 167, avg loss per batch: 2.2284901952718816
[2025-02-13 12:51:07,520][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 168
[2025-02-13 12:51:28,254][rgc][INFO] - 	Updating weights of batch 168
[2025-02-13 12:51:28,305][rgc][INFO] - Batch 168, avg loss per batch: 3.944739035077852
[2025-02-13 12:51:28,306][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 169
[2025-02-13 12:51:48,997][rgc][INFO] - 	Updating weights of batch 169
[2025-02-13 12:51:49,049][rgc][INFO] - Batch 169, avg loss per batch: 2.6683439377928693
[2025-02-13 12:51:49,050][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 170
[2025-02-13 12:52:09,728][rgc][INFO] - 	Updating weights of batch 170
[2025-02-13 12:52:09,780][rgc][INFO] - Batch 170, avg loss per batch: 2.0219757303444656
[2025-02-13 12:52:09,781][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 171
[2025-02-13 12:52:30,463][rgc][INFO] - 	Updating weights of batch 171
[2025-02-13 12:52:30,516][rgc][INFO] - Batch 171, avg loss per batch: 2.8956803423975463
[2025-02-13 12:52:30,517][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 172
[2025-02-13 12:52:51,210][rgc][INFO] - 	Updating weights of batch 172
[2025-02-13 12:52:51,261][rgc][INFO] - Batch 172, avg loss per batch: 3.4824603377841807
[2025-02-13 12:52:51,261][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 173
[2025-02-13 12:53:11,956][rgc][INFO] - 	Updating weights of batch 173
[2025-02-13 12:53:12,010][rgc][INFO] - Batch 173, avg loss per batch: 2.4877182574969394
[2025-02-13 12:53:12,011][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 174
[2025-02-13 12:53:32,732][rgc][INFO] - 	Updating weights of batch 174
[2025-02-13 12:53:32,781][rgc][INFO] - Batch 174, avg loss per batch: 2.1384761661830978
[2025-02-13 12:53:32,782][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 175
[2025-02-13 12:53:53,453][rgc][INFO] - 	Updating weights of batch 175
[2025-02-13 12:53:53,518][rgc][INFO] - Batch 175, avg loss per batch: 2.7607385010193317
[2025-02-13 12:53:53,519][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 176
[2025-02-13 12:54:14,208][rgc][INFO] - 	Updating weights of batch 176
[2025-02-13 12:54:14,284][rgc][INFO] - Batch 176, avg loss per batch: 4.121595306350774
[2025-02-13 12:54:14,285][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 177
[2025-02-13 12:54:34,979][rgc][INFO] - 	Updating weights of batch 177
[2025-02-13 12:54:35,035][rgc][INFO] - Batch 177, avg loss per batch: 2.549193276073885
[2025-02-13 12:54:35,037][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 178
[2025-02-13 12:54:55,784][rgc][INFO] - 	Updating weights of batch 178
[2025-02-13 12:54:55,839][rgc][INFO] - Batch 178, avg loss per batch: 3.0204664140576245
[2025-02-13 12:54:55,840][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 179
[2025-02-13 12:55:16,615][rgc][INFO] - 	Updating weights of batch 179
[2025-02-13 12:55:16,669][rgc][INFO] - Batch 179, avg loss per batch: 2.8861514187177555
[2025-02-13 12:55:16,670][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 180
[2025-02-13 12:55:37,351][rgc][INFO] - 	Updating weights of batch 180
[2025-02-13 12:55:37,404][rgc][INFO] - Batch 180, avg loss per batch: 3.326315609719637
[2025-02-13 12:55:37,404][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 181
[2025-02-13 12:55:58,091][rgc][INFO] - 	Updating weights of batch 181
[2025-02-13 12:55:58,150][rgc][INFO] - Batch 181, avg loss per batch: 3.1100153494497254
[2025-02-13 12:55:58,151][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 182
[2025-02-13 12:56:18,824][rgc][INFO] - 	Updating weights of batch 182
[2025-02-13 12:56:18,878][rgc][INFO] - Batch 182, avg loss per batch: 2.595717641419703
[2025-02-13 12:56:18,879][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 183
[2025-02-13 12:56:39,621][rgc][INFO] - 	Updating weights of batch 183
[2025-02-13 12:56:39,674][rgc][INFO] - Batch 183, avg loss per batch: 3.0662387998191045
[2025-02-13 12:56:39,676][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 184
[2025-02-13 12:57:00,353][rgc][INFO] - 	Updating weights of batch 184
[2025-02-13 12:57:00,402][rgc][INFO] - Batch 184, avg loss per batch: 1.7488960286505086
[2025-02-13 12:57:00,403][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 185
[2025-02-13 12:57:21,168][rgc][INFO] - 	Updating weights of batch 185
[2025-02-13 12:57:21,218][rgc][INFO] - Batch 185, avg loss per batch: 1.079352114174294
[2025-02-13 12:57:21,219][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 186
[2025-02-13 12:57:41,953][rgc][INFO] - 	Updating weights of batch 186
[2025-02-13 12:57:42,010][rgc][INFO] - Batch 186, avg loss per batch: 2.170391920556227
[2025-02-13 12:57:42,011][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 187
[2025-02-13 12:58:02,747][rgc][INFO] - 	Updating weights of batch 187
[2025-02-13 12:58:02,796][rgc][INFO] - Batch 187, avg loss per batch: 4.770269734277824
[2025-02-13 12:58:02,797][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 188
[2025-02-13 12:58:23,483][rgc][INFO] - 	Updating weights of batch 188
[2025-02-13 12:58:23,535][rgc][INFO] - Batch 188, avg loss per batch: 2.539734260834428
[2025-02-13 12:58:23,537][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 189
[2025-02-13 12:58:44,205][rgc][INFO] - 	Updating weights of batch 189
[2025-02-13 12:58:44,259][rgc][INFO] - Batch 189, avg loss per batch: 3.456735336718155
[2025-02-13 12:58:44,259][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 190
[2025-02-13 12:59:04,936][rgc][INFO] - 	Updating weights of batch 190
[2025-02-13 12:59:05,003][rgc][INFO] - Batch 190, avg loss per batch: 4.15620636996705
[2025-02-13 12:59:05,004][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 191
[2025-02-13 12:59:25,730][rgc][INFO] - 	Updating weights of batch 191
[2025-02-13 12:59:25,804][rgc][INFO] - Batch 191, avg loss per batch: 5.014297135929885
[2025-02-13 12:59:25,806][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 192
[2025-02-13 12:59:46,490][rgc][INFO] - 	Updating weights of batch 192
[2025-02-13 12:59:46,548][rgc][INFO] - Batch 192, avg loss per batch: 3.247798789241312
[2025-02-13 12:59:46,549][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 193
[2025-02-13 13:00:07,227][rgc][INFO] - 	Updating weights of batch 193
[2025-02-13 13:00:07,278][rgc][INFO] - Batch 193, avg loss per batch: 3.019617009064893
[2025-02-13 13:00:07,278][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 194
[2025-02-13 13:00:28,047][rgc][INFO] - 	Updating weights of batch 194
[2025-02-13 13:00:28,103][rgc][INFO] - Batch 194, avg loss per batch: 2.0690242803555368
[2025-02-13 13:00:28,104][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 195
[2025-02-13 13:00:48,786][rgc][INFO] - 	Updating weights of batch 195
[2025-02-13 13:00:48,852][rgc][INFO] - Batch 195, avg loss per batch: 3.2111907623203426
[2025-02-13 13:00:48,854][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 196
[2025-02-13 13:01:09,541][rgc][INFO] - 	Updating weights of batch 196
[2025-02-13 13:01:09,607][rgc][INFO] - Batch 196, avg loss per batch: 2.9181675068495796
[2025-02-13 13:01:09,608][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 197
[2025-02-13 13:01:30,277][rgc][INFO] - 	Updating weights of batch 197
[2025-02-13 13:01:30,329][rgc][INFO] - Batch 197, avg loss per batch: 4.489745155512367
[2025-02-13 13:01:30,330][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 198
[2025-02-13 13:01:50,999][rgc][INFO] - 	Updating weights of batch 198
[2025-02-13 13:01:51,065][rgc][INFO] - Batch 198, avg loss per batch: 1.1592337884979824
[2025-02-13 13:01:51,065][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 199
[2025-02-13 13:02:11,736][rgc][INFO] - 	Updating weights of batch 199
[2025-02-13 13:02:11,799][rgc][INFO] - Batch 199, avg loss per batch: 1.2566527947828559
[2025-02-13 13:02:23,344][rgc][INFO] - AVG rho on val data: 0.2051881026499533
[2025-02-13 13:02:23,345][rgc][INFO] - AVG mae on val data: 0.5524605827805946
[2025-02-13 13:02:34,529][rgc][INFO] - AVG rho on test data: 0.24499728398028792
[2025-02-13 13:02:34,529][rgc][INFO] - AVG mae on test data: 0.5708286509410625
[2025-02-13 13:02:47,369][rgc][INFO] - AVG rho on train data: 0.1007429441934921
[2025-02-13 13:02:47,369][rgc][INFO] - AVG mae on train data: 0.5842397475515793
[2025-02-13 13:02:47,369][rgc][INFO] - Current best rhos: train 0.2063941698696264, val 0.25065243033438855, test 0.2938951854929506
[2025-02-13 13:02:47,371][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 200
[2025-02-13 13:03:08,047][rgc][INFO] - 	Updating weights of batch 200
[2025-02-13 13:03:08,103][rgc][INFO] - Batch 200, avg loss per batch: 2.35804656697599
[2025-02-13 13:03:08,103][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 201
[2025-02-13 13:03:28,857][rgc][INFO] - 	Updating weights of batch 201
[2025-02-13 13:03:28,907][rgc][INFO] - Batch 201, avg loss per batch: 3.2089196212645104
[2025-02-13 13:03:28,907][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 202
[2025-02-13 13:03:49,573][rgc][INFO] - 	Updating weights of batch 202
[2025-02-13 13:03:49,623][rgc][INFO] - Batch 202, avg loss per batch: 3.4164115157131234
[2025-02-13 13:03:49,623][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 203
[2025-02-13 13:04:10,363][rgc][INFO] - 	Updating weights of batch 203
[2025-02-13 13:04:10,417][rgc][INFO] - Batch 203, avg loss per batch: 2.8114467936057395
[2025-02-13 13:04:10,418][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 204
[2025-02-13 13:04:31,109][rgc][INFO] - 	Updating weights of batch 204
[2025-02-13 13:04:31,180][rgc][INFO] - Batch 204, avg loss per batch: 2.390432803299068
[2025-02-13 13:04:31,181][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 205
[2025-02-13 13:04:51,869][rgc][INFO] - 	Updating weights of batch 205
[2025-02-13 13:04:51,924][rgc][INFO] - Batch 205, avg loss per batch: 3.054029289467247
[2025-02-13 13:04:51,925][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 206
[2025-02-13 13:05:12,662][rgc][INFO] - 	Updating weights of batch 206
[2025-02-13 13:05:12,717][rgc][INFO] - Batch 206, avg loss per batch: 3.289228614044179
[2025-02-13 13:05:12,718][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 207
[2025-02-13 13:05:33,445][rgc][INFO] - 	Updating weights of batch 207
[2025-02-13 13:05:33,498][rgc][INFO] - Batch 207, avg loss per batch: 2.6882644381783796
[2025-02-13 13:05:33,498][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 208
[2025-02-13 13:05:54,177][rgc][INFO] - 	Updating weights of batch 208
[2025-02-13 13:05:54,228][rgc][INFO] - Batch 208, avg loss per batch: 1.7761158128874563
[2025-02-13 13:05:54,229][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 209
[2025-02-13 13:06:14,945][rgc][INFO] - 	Updating weights of batch 209
[2025-02-13 13:06:14,997][rgc][INFO] - Batch 209, avg loss per batch: 3.1770422970987275
[2025-02-13 13:06:14,998][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 210
[2025-02-13 13:06:35,666][rgc][INFO] - 	Updating weights of batch 210
[2025-02-13 13:06:35,718][rgc][INFO] - Batch 210, avg loss per batch: 3.932681091018278
[2025-02-13 13:06:35,719][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 211
[2025-02-13 13:06:56,406][rgc][INFO] - 	Updating weights of batch 211
[2025-02-13 13:06:56,456][rgc][INFO] - Batch 211, avg loss per batch: 3.2682484403023846
[2025-02-13 13:06:56,456][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 212
[2025-02-13 13:07:17,176][rgc][INFO] - 	Updating weights of batch 212
[2025-02-13 13:07:17,227][rgc][INFO] - Batch 212, avg loss per batch: 2.9191496861634225
[2025-02-13 13:07:17,228][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 213
[2025-02-13 13:07:37,901][rgc][INFO] - 	Updating weights of batch 213
[2025-02-13 13:07:37,957][rgc][INFO] - Batch 213, avg loss per batch: 2.3825876368454604
[2025-02-13 13:07:37,958][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 214
[2025-02-13 13:07:58,713][rgc][INFO] - 	Updating weights of batch 214
[2025-02-13 13:07:58,767][rgc][INFO] - Batch 214, avg loss per batch: 1.8248810271949787
[2025-02-13 13:07:58,767][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 215
[2025-02-13 13:08:19,509][rgc][INFO] - 	Updating weights of batch 215
[2025-02-13 13:08:19,561][rgc][INFO] - Batch 215, avg loss per batch: 3.8357544599928017
[2025-02-13 13:08:19,561][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 216
[2025-02-13 13:08:40,322][rgc][INFO] - 	Updating weights of batch 216
[2025-02-13 13:08:40,371][rgc][INFO] - Batch 216, avg loss per batch: 0.8562118310610876
[2025-02-13 13:08:40,372][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 217
[2025-02-13 13:09:01,050][rgc][INFO] - 	Updating weights of batch 217
[2025-02-13 13:09:01,105][rgc][INFO] - Batch 217, avg loss per batch: 3.7674911053157523
[2025-02-13 13:09:01,105][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 218
[2025-02-13 13:09:21,796][rgc][INFO] - 	Updating weights of batch 218
[2025-02-13 13:09:21,844][rgc][INFO] - Batch 218, avg loss per batch: 2.8989178154377404
[2025-02-13 13:09:21,845][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 219
[2025-02-13 13:09:42,524][rgc][INFO] - 	Updating weights of batch 219
[2025-02-13 13:09:42,580][rgc][INFO] - Batch 219, avg loss per batch: 2.4809211263393935
[2025-02-13 13:09:42,581][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 220
[2025-02-13 13:10:03,247][rgc][INFO] - 	Updating weights of batch 220
[2025-02-13 13:10:03,298][rgc][INFO] - Batch 220, avg loss per batch: 2.5631205390830867
[2025-02-13 13:10:03,299][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 221
[2025-02-13 13:10:24,066][rgc][INFO] - 	Updating weights of batch 221
[2025-02-13 13:10:24,116][rgc][INFO] - Batch 221, avg loss per batch: 1.0498115349106332
[2025-02-13 13:10:24,116][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 222
[2025-02-13 13:10:44,792][rgc][INFO] - 	Updating weights of batch 222
[2025-02-13 13:10:44,846][rgc][INFO] - Batch 222, avg loss per batch: 2.7030011716056777
[2025-02-13 13:10:44,847][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 223
[2025-02-13 13:11:05,523][rgc][INFO] - 	Updating weights of batch 223
[2025-02-13 13:11:05,573][rgc][INFO] - Batch 223, avg loss per batch: 2.3373163984680616
[2025-02-13 13:11:05,574][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 224
[2025-02-13 13:11:26,323][rgc][INFO] - 	Updating weights of batch 224
[2025-02-13 13:11:26,377][rgc][INFO] - Batch 224, avg loss per batch: 1.2461752131904942
[2025-02-13 13:11:26,379][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 225
[2025-02-13 13:11:47,058][rgc][INFO] - 	Updating weights of batch 225
[2025-02-13 13:11:47,110][rgc][INFO] - Batch 225, avg loss per batch: 3.357368500360709
[2025-02-13 13:11:47,111][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 226
[2025-02-13 13:12:07,881][rgc][INFO] - 	Updating weights of batch 226
[2025-02-13 13:12:07,933][rgc][INFO] - Batch 226, avg loss per batch: 2.2103278185151614
[2025-02-13 13:12:07,934][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 227
[2025-02-13 13:12:28,656][rgc][INFO] - 	Updating weights of batch 227
[2025-02-13 13:12:28,709][rgc][INFO] - Batch 227, avg loss per batch: 4.152019078965177
[2025-02-13 13:12:28,709][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 228
[2025-02-13 13:12:49,377][rgc][INFO] - 	Updating weights of batch 228
[2025-02-13 13:12:49,435][rgc][INFO] - Batch 228, avg loss per batch: 1.553000177150123
[2025-02-13 13:12:49,435][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 229
[2025-02-13 13:13:10,119][rgc][INFO] - 	Updating weights of batch 229
[2025-02-13 13:13:10,169][rgc][INFO] - Batch 229, avg loss per batch: 3.767909483154271
[2025-02-13 13:13:10,171][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 230
[2025-02-13 13:13:30,856][rgc][INFO] - 	Updating weights of batch 230
[2025-02-13 13:13:30,908][rgc][INFO] - Batch 230, avg loss per batch: 2.3205626551055243
[2025-02-13 13:13:30,909][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 231
[2025-02-13 13:13:51,572][rgc][INFO] - 	Updating weights of batch 231
[2025-02-13 13:13:51,621][rgc][INFO] - Batch 231, avg loss per batch: 2.2680237440414848
[2025-02-13 13:13:51,621][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 232
[2025-02-13 13:14:12,289][rgc][INFO] - 	Updating weights of batch 232
[2025-02-13 13:14:12,339][rgc][INFO] - Batch 232, avg loss per batch: 4.043091604749927
[2025-02-13 13:14:12,340][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 233
[2025-02-13 13:14:33,058][rgc][INFO] - 	Updating weights of batch 233
[2025-02-13 13:14:33,123][rgc][INFO] - Batch 233, avg loss per batch: 2.6755634938833497
[2025-02-13 13:14:33,124][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 234
[2025-02-13 13:14:53,877][rgc][INFO] - 	Updating weights of batch 234
[2025-02-13 13:14:53,926][rgc][INFO] - Batch 234, avg loss per batch: 2.342157644856683
[2025-02-13 13:14:53,927][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 235
[2025-02-13 13:15:14,683][rgc][INFO] - 	Updating weights of batch 235
[2025-02-13 13:15:14,738][rgc][INFO] - Batch 235, avg loss per batch: 2.288178230252692
[2025-02-13 13:15:14,739][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 236
[2025-02-13 13:15:35,482][rgc][INFO] - 	Updating weights of batch 236
[2025-02-13 13:15:35,534][rgc][INFO] - Batch 236, avg loss per batch: 3.0822987254780214
[2025-02-13 13:15:35,535][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 237
[2025-02-13 13:15:56,268][rgc][INFO] - 	Updating weights of batch 237
[2025-02-13 13:15:56,323][rgc][INFO] - Batch 237, avg loss per batch: 2.389176195202219
[2025-02-13 13:15:56,323][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 238
[2025-02-13 13:16:17,062][rgc][INFO] - 	Updating weights of batch 238
[2025-02-13 13:16:17,120][rgc][INFO] - Batch 238, avg loss per batch: 2.4074818818574055
[2025-02-13 13:16:17,121][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 239
[2025-02-13 13:16:37,796][rgc][INFO] - 	Updating weights of batch 239
[2025-02-13 13:16:37,850][rgc][INFO] - Batch 239, avg loss per batch: 1.766769044033068
[2025-02-13 13:16:37,851][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 240
[2025-02-13 13:16:58,526][rgc][INFO] - 	Updating weights of batch 240
[2025-02-13 13:16:58,579][rgc][INFO] - Batch 240, avg loss per batch: 5.16012465341454
[2025-02-13 13:16:58,580][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 241
[2025-02-13 13:17:19,245][rgc][INFO] - 	Updating weights of batch 241
[2025-02-13 13:17:19,294][rgc][INFO] - Batch 241, avg loss per batch: 2.777969205931801
[2025-02-13 13:17:19,295][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 242
[2025-02-13 13:17:39,971][rgc][INFO] - 	Updating weights of batch 242
[2025-02-13 13:17:40,022][rgc][INFO] - Batch 242, avg loss per batch: 4.549692804463785
[2025-02-13 13:17:40,022][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 243
[2025-02-13 13:18:00,758][rgc][INFO] - 	Updating weights of batch 243
[2025-02-13 13:18:00,807][rgc][INFO] - Batch 243, avg loss per batch: 4.097524518996919
[2025-02-13 13:18:00,807][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 244
[2025-02-13 13:18:21,533][rgc][INFO] - 	Updating weights of batch 244
[2025-02-13 13:18:21,609][rgc][INFO] - Batch 244, avg loss per batch: 1.6648385392531504
[2025-02-13 13:18:21,610][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 245
[2025-02-13 13:18:42,296][rgc][INFO] - 	Updating weights of batch 245
[2025-02-13 13:18:42,346][rgc][INFO] - Batch 245, avg loss per batch: 2.7441979835488373
[2025-02-13 13:18:42,347][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 246
[2025-02-13 13:19:03,070][rgc][INFO] - 	Updating weights of batch 246
[2025-02-13 13:19:03,118][rgc][INFO] - Batch 246, avg loss per batch: 2.263781470278389
[2025-02-13 13:19:03,119][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 247
[2025-02-13 13:19:23,806][rgc][INFO] - 	Updating weights of batch 247
[2025-02-13 13:19:23,856][rgc][INFO] - Batch 247, avg loss per batch: 3.6620063413295725
[2025-02-13 13:19:23,857][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 248
[2025-02-13 13:19:44,527][rgc][INFO] - 	Updating weights of batch 248
[2025-02-13 13:19:44,579][rgc][INFO] - Batch 248, avg loss per batch: 1.7089149356907676
[2025-02-13 13:19:44,580][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 249
[2025-02-13 13:20:05,251][rgc][INFO] - 	Updating weights of batch 249
[2025-02-13 13:20:05,303][rgc][INFO] - Batch 249, avg loss per batch: 1.6958943982455408
[2025-02-13 13:20:05,304][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 250
[2025-02-13 13:20:25,971][rgc][INFO] - 	Updating weights of batch 250
[2025-02-13 13:20:26,025][rgc][INFO] - Batch 250, avg loss per batch: 1.9713143569982812
[2025-02-13 13:20:26,026][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 251
[2025-02-13 13:20:46,694][rgc][INFO] - 	Updating weights of batch 251
[2025-02-13 13:20:46,752][rgc][INFO] - Batch 251, avg loss per batch: 2.1172024601794717
[2025-02-13 13:20:46,753][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 252
[2025-02-13 13:21:07,437][rgc][INFO] - 	Updating weights of batch 252
[2025-02-13 13:21:07,493][rgc][INFO] - Batch 252, avg loss per batch: 2.791284399423578
[2025-02-13 13:21:07,494][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 253
[2025-02-13 13:21:28,230][rgc][INFO] - 	Updating weights of batch 253
[2025-02-13 13:21:28,295][rgc][INFO] - Batch 253, avg loss per batch: 2.2045112524656307
[2025-02-13 13:21:28,295][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 254
[2025-02-13 13:21:48,974][rgc][INFO] - 	Updating weights of batch 254
[2025-02-13 13:21:49,024][rgc][INFO] - Batch 254, avg loss per batch: 2.1594242509588613
[2025-02-13 13:21:49,025][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 255
[2025-02-13 13:22:09,796][rgc][INFO] - 	Updating weights of batch 255
[2025-02-13 13:22:09,849][rgc][INFO] - Batch 255, avg loss per batch: 2.454702053286164
[2025-02-13 13:22:09,850][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 256
[2025-02-13 13:22:30,539][rgc][INFO] - 	Updating weights of batch 256
[2025-02-13 13:22:30,592][rgc][INFO] - Batch 256, avg loss per batch: 1.9270588515223146
[2025-02-13 13:22:30,594][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 257
[2025-02-13 13:22:51,346][rgc][INFO] - 	Updating weights of batch 257
[2025-02-13 13:22:51,399][rgc][INFO] - Batch 257, avg loss per batch: 1.9965181526157363
[2025-02-13 13:22:51,400][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 258
[2025-02-13 13:23:12,167][rgc][INFO] - 	Updating weights of batch 258
[2025-02-13 13:23:12,223][rgc][INFO] - Batch 258, avg loss per batch: 2.6479212294004135
[2025-02-13 13:23:12,223][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 259
[2025-02-13 13:23:32,915][rgc][INFO] - 	Updating weights of batch 259
[2025-02-13 13:23:32,969][rgc][INFO] - Batch 259, avg loss per batch: 3.2025685958839465
[2025-02-13 13:23:32,970][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 260
[2025-02-13 13:23:53,650][rgc][INFO] - 	Updating weights of batch 260
[2025-02-13 13:23:53,701][rgc][INFO] - Batch 260, avg loss per batch: 2.2927500968307326
[2025-02-13 13:23:53,701][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 261
[2025-02-13 13:24:14,389][rgc][INFO] - 	Updating weights of batch 261
[2025-02-13 13:24:14,443][rgc][INFO] - Batch 261, avg loss per batch: 3.6845232573480127
[2025-02-13 13:24:14,444][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 262
[2025-02-13 13:24:35,111][rgc][INFO] - 	Updating weights of batch 262
[2025-02-13 13:24:35,174][rgc][INFO] - Batch 262, avg loss per batch: 2.7997815790675364
[2025-02-13 13:24:35,176][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 263
[2025-02-13 13:24:55,946][rgc][INFO] - 	Updating weights of batch 263
[2025-02-13 13:24:55,996][rgc][INFO] - Batch 263, avg loss per batch: 2.4738559076658686
[2025-02-13 13:24:55,997][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 264
[2025-02-13 13:25:16,672][rgc][INFO] - 	Updating weights of batch 264
[2025-02-13 13:25:16,725][rgc][INFO] - Batch 264, avg loss per batch: 1.7010013971921574
[2025-02-13 13:25:16,726][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 265
[2025-02-13 13:25:37,411][rgc][INFO] - 	Updating weights of batch 265
[2025-02-13 13:25:37,471][rgc][INFO] - Batch 265, avg loss per batch: 4.991014555859245
[2025-02-13 13:25:37,472][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 266
[2025-02-13 13:25:58,163][rgc][INFO] - 	Updating weights of batch 266
[2025-02-13 13:25:58,213][rgc][INFO] - Batch 266, avg loss per batch: 2.961147952475536
[2025-02-13 13:25:58,214][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 267
[2025-02-13 13:26:18,884][rgc][INFO] - 	Updating weights of batch 267
[2025-02-13 13:26:18,939][rgc][INFO] - Batch 267, avg loss per batch: 2.2466154699474474
[2025-02-13 13:26:18,940][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 268
[2025-02-13 13:26:39,704][rgc][INFO] - 	Updating weights of batch 268
[2025-02-13 13:26:39,755][rgc][INFO] - Batch 268, avg loss per batch: 2.1069856672804432
[2025-02-13 13:26:39,756][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 269
[2025-02-13 13:27:00,486][rgc][INFO] - 	Updating weights of batch 269
[2025-02-13 13:27:00,540][rgc][INFO] - Batch 269, avg loss per batch: 2.4064441827325496
[2025-02-13 13:27:00,541][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 270
[2025-02-13 13:27:21,255][rgc][INFO] - 	Updating weights of batch 270
[2025-02-13 13:27:21,311][rgc][INFO] - Batch 270, avg loss per batch: 2.8138322820558073
[2025-02-13 13:27:21,311][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 271
[2025-02-13 13:27:41,981][rgc][INFO] - 	Updating weights of batch 271
[2025-02-13 13:27:42,056][rgc][INFO] - Batch 271, avg loss per batch: 4.390654319113937
[2025-02-13 13:27:42,057][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 272
[2025-02-13 13:28:02,747][rgc][INFO] - 	Updating weights of batch 272
[2025-02-13 13:28:02,799][rgc][INFO] - Batch 272, avg loss per batch: 2.98041546165638
[2025-02-13 13:28:02,799][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 273
[2025-02-13 13:28:23,480][rgc][INFO] - 	Updating weights of batch 273
[2025-02-13 13:28:23,537][rgc][INFO] - Batch 273, avg loss per batch: 4.712726512013893
[2025-02-13 13:28:23,538][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 274
[2025-02-13 13:28:44,226][rgc][INFO] - 	Updating weights of batch 274
[2025-02-13 13:28:44,302][rgc][INFO] - Batch 274, avg loss per batch: 2.300552898618616
[2025-02-13 13:28:44,303][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 275
[2025-02-13 13:29:04,992][rgc][INFO] - 	Updating weights of batch 275
[2025-02-13 13:29:05,058][rgc][INFO] - Batch 275, avg loss per batch: 2.182312950171465
[2025-02-13 13:29:05,059][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 276
[2025-02-13 13:29:25,792][rgc][INFO] - 	Updating weights of batch 276
[2025-02-13 13:29:25,845][rgc][INFO] - Batch 276, avg loss per batch: 3.16155634125597
[2025-02-13 13:29:25,845][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 277
[2025-02-13 13:29:46,566][rgc][INFO] - 	Updating weights of batch 277
[2025-02-13 13:29:46,619][rgc][INFO] - Batch 277, avg loss per batch: 3.2110951414340563
[2025-02-13 13:29:46,620][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 278
[2025-02-13 13:30:07,287][rgc][INFO] - 	Updating weights of batch 278
[2025-02-13 13:30:07,364][rgc][INFO] - Batch 278, avg loss per batch: 4.078043979629509
[2025-02-13 13:30:07,365][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 279
[2025-02-13 13:30:28,074][rgc][INFO] - 	Updating weights of batch 279
[2025-02-13 13:30:28,127][rgc][INFO] - Batch 279, avg loss per batch: 2.4247401954589787
[2025-02-13 13:30:28,128][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 280
[2025-02-13 13:30:48,802][rgc][INFO] - 	Updating weights of batch 280
[2025-02-13 13:30:48,860][rgc][INFO] - Batch 280, avg loss per batch: 1.609713972698748
[2025-02-13 13:30:48,861][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 281
[2025-02-13 13:31:09,537][rgc][INFO] - 	Updating weights of batch 281
[2025-02-13 13:31:09,587][rgc][INFO] - Batch 281, avg loss per batch: 1.8773427139187056
[2025-02-13 13:31:09,588][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 282
[2025-02-13 13:31:30,354][rgc][INFO] - 	Updating weights of batch 282
[2025-02-13 13:31:30,404][rgc][INFO] - Batch 282, avg loss per batch: 3.46959150344023
[2025-02-13 13:31:30,405][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 283
[2025-02-13 13:31:51,070][rgc][INFO] - 	Updating weights of batch 283
[2025-02-13 13:31:51,122][rgc][INFO] - Batch 283, avg loss per batch: 2.3740708740416854
[2025-02-13 13:31:51,123][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 284
[2025-02-13 13:32:11,875][rgc][INFO] - 	Updating weights of batch 284
[2025-02-13 13:32:11,955][rgc][INFO] - Batch 284, avg loss per batch: 4.901166591676476
[2025-02-13 13:32:11,956][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 285
[2025-02-13 13:32:32,698][rgc][INFO] - 	Updating weights of batch 285
[2025-02-13 13:32:32,755][rgc][INFO] - Batch 285, avg loss per batch: 1.2630070582213044
[2025-02-13 13:32:32,757][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 286
[2025-02-13 13:32:53,446][rgc][INFO] - 	Updating weights of batch 286
[2025-02-13 13:32:53,500][rgc][INFO] - Batch 286, avg loss per batch: 1.3468909815348358
[2025-02-13 13:32:53,502][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 287
[2025-02-13 13:33:14,192][rgc][INFO] - 	Updating weights of batch 287
[2025-02-13 13:33:14,243][rgc][INFO] - Batch 287, avg loss per batch: 3.4637063444219676
[2025-02-13 13:33:14,245][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 288
[2025-02-13 13:33:34,983][rgc][INFO] - 	Updating weights of batch 288
[2025-02-13 13:33:35,034][rgc][INFO] - Batch 288, avg loss per batch: 2.391356561770549
[2025-02-13 13:33:35,035][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 289
[2025-02-13 13:33:55,776][rgc][INFO] - 	Updating weights of batch 289
[2025-02-13 13:33:55,826][rgc][INFO] - Batch 289, avg loss per batch: 1.5134474864551724
[2025-02-13 13:33:55,827][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 290
[2025-02-13 13:34:16,560][rgc][INFO] - 	Updating weights of batch 290
[2025-02-13 13:34:16,617][rgc][INFO] - Batch 290, avg loss per batch: 2.7328909310358744
[2025-02-13 13:34:16,618][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 291
[2025-02-13 13:34:37,305][rgc][INFO] - 	Updating weights of batch 291
[2025-02-13 13:34:37,359][rgc][INFO] - Batch 291, avg loss per batch: 3.344420018455999
[2025-02-13 13:34:37,360][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 292
[2025-02-13 13:34:58,054][rgc][INFO] - 	Updating weights of batch 292
[2025-02-13 13:34:58,112][rgc][INFO] - Batch 292, avg loss per batch: 1.851079885757159
[2025-02-13 13:34:58,113][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 293
[2025-02-13 13:35:18,851][rgc][INFO] - 	Updating weights of batch 293
[2025-02-13 13:35:18,906][rgc][INFO] - Batch 293, avg loss per batch: 3.2429159192196226
[2025-02-13 13:35:18,907][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 294
[2025-02-13 13:35:39,643][rgc][INFO] - 	Updating weights of batch 294
[2025-02-13 13:35:39,695][rgc][INFO] - Batch 294, avg loss per batch: 1.8236459471575366
[2025-02-13 13:35:39,696][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 295
[2025-02-13 13:36:00,366][rgc][INFO] - 	Updating weights of batch 295
[2025-02-13 13:36:00,416][rgc][INFO] - Batch 295, avg loss per batch: 3.249992487950124
[2025-02-13 13:36:00,417][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 296
[2025-02-13 13:36:21,155][rgc][INFO] - 	Updating weights of batch 296
[2025-02-13 13:36:21,211][rgc][INFO] - Batch 296, avg loss per batch: 3.0436599478940343
[2025-02-13 13:36:21,212][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 297
[2025-02-13 13:36:41,880][rgc][INFO] - 	Updating weights of batch 297
[2025-02-13 13:36:41,938][rgc][INFO] - Batch 297, avg loss per batch: 3.8691202729642256
[2025-02-13 13:36:41,940][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 298
[2025-02-13 13:37:02,656][rgc][INFO] - 	Updating weights of batch 298
[2025-02-13 13:37:02,708][rgc][INFO] - Batch 298, avg loss per batch: 2.3209559255757535
[2025-02-13 13:37:02,709][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 299
[2025-02-13 13:37:23,469][rgc][INFO] - 	Updating weights of batch 299
[2025-02-13 13:37:23,522][rgc][INFO] - Batch 299, avg loss per batch: 4.49802299975582
[2025-02-13 13:37:35,075][rgc][INFO] - AVG rho on val data: 0.28403201292146585
[2025-02-13 13:37:35,075][rgc][INFO] - AVG mae on val data: 0.5498851251766101
[2025-02-13 13:37:46,244][rgc][INFO] - AVG rho on test data: 0.3118059305914899
[2025-02-13 13:37:46,244][rgc][INFO] - AVG mae on test data: 0.6031166327900068
[2025-02-13 13:37:59,083][rgc][INFO] - AVG rho on train data: 0.22776812926799744
[2025-02-13 13:37:59,083][rgc][INFO] - AVG mae on train data: 0.5653599872268573
[2025-02-13 13:37:59,085][rgc][INFO] - Current best rhos: train 0.22776812926799744, val 0.28403201292146585, test 0.3118059305914899
[2025-02-13 13:37:59,091][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 300
[2025-02-13 13:38:19,831][rgc][INFO] - 	Updating weights of batch 300
[2025-02-13 13:38:19,885][rgc][INFO] - Batch 300, avg loss per batch: 3.6263090022733033
[2025-02-13 13:38:19,886][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 301
[2025-02-13 13:38:40,645][rgc][INFO] - 	Updating weights of batch 301
[2025-02-13 13:38:40,696][rgc][INFO] - Batch 301, avg loss per batch: 2.5550824632618014
[2025-02-13 13:38:40,696][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 302
[2025-02-13 13:39:01,381][rgc][INFO] - 	Updating weights of batch 302
[2025-02-13 13:39:01,432][rgc][INFO] - Batch 302, avg loss per batch: 0.9705498392488014
[2025-02-13 13:39:01,433][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 303
[2025-02-13 13:39:22,115][rgc][INFO] - 	Updating weights of batch 303
[2025-02-13 13:39:22,168][rgc][INFO] - Batch 303, avg loss per batch: 1.7598120856030142
[2025-02-13 13:39:22,169][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 304
[2025-02-13 13:39:42,899][rgc][INFO] - 	Updating weights of batch 304
[2025-02-13 13:39:42,957][rgc][INFO] - Batch 304, avg loss per batch: 2.0450021956353677
[2025-02-13 13:39:42,958][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 305
[2025-02-13 13:40:03,721][rgc][INFO] - 	Updating weights of batch 305
[2025-02-13 13:40:03,774][rgc][INFO] - Batch 305, avg loss per batch: 4.303846587631481
[2025-02-13 13:40:03,774][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 306
[2025-02-13 13:40:24,459][rgc][INFO] - 	Updating weights of batch 306
[2025-02-13 13:40:24,509][rgc][INFO] - Batch 306, avg loss per batch: 2.3542344684075918
[2025-02-13 13:40:24,510][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 307
[2025-02-13 13:40:45,223][rgc][INFO] - 	Updating weights of batch 307
[2025-02-13 13:40:45,285][rgc][INFO] - Batch 307, avg loss per batch: 1.9105987462269416
[2025-02-13 13:40:45,285][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 308
[2025-02-13 13:41:05,968][rgc][INFO] - 	Updating weights of batch 308
[2025-02-13 13:41:06,019][rgc][INFO] - Batch 308, avg loss per batch: 2.854103121947768
[2025-02-13 13:41:06,020][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 309
[2025-02-13 13:41:26,677][rgc][INFO] - 	Updating weights of batch 309
[2025-02-13 13:41:26,733][rgc][INFO] - Batch 309, avg loss per batch: 3.732775777384819
[2025-02-13 13:41:26,735][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 310
[2025-02-13 13:41:47,462][rgc][INFO] - 	Updating weights of batch 310
[2025-02-13 13:41:47,517][rgc][INFO] - Batch 310, avg loss per batch: 2.741382301976988
[2025-02-13 13:41:47,517][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 311
[2025-02-13 13:42:08,246][rgc][INFO] - 	Updating weights of batch 311
[2025-02-13 13:42:08,296][rgc][INFO] - Batch 311, avg loss per batch: 3.4641510806741214
[2025-02-13 13:42:08,297][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 312
[2025-02-13 13:42:29,059][rgc][INFO] - 	Updating weights of batch 312
[2025-02-13 13:42:29,114][rgc][INFO] - Batch 312, avg loss per batch: 1.0603733899854042
[2025-02-13 13:42:29,115][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 313
[2025-02-13 13:42:49,800][rgc][INFO] - 	Updating weights of batch 313
[2025-02-13 13:42:49,862][rgc][INFO] - Batch 313, avg loss per batch: 3.6048123252294753
[2025-02-13 13:42:49,864][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 314
[2025-02-13 13:43:10,532][rgc][INFO] - 	Updating weights of batch 314
[2025-02-13 13:43:10,583][rgc][INFO] - Batch 314, avg loss per batch: 2.4394047253144686
[2025-02-13 13:43:10,583][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 315
[2025-02-13 13:43:31,301][rgc][INFO] - 	Updating weights of batch 315
[2025-02-13 13:43:31,357][rgc][INFO] - Batch 315, avg loss per batch: 2.7198512417958383
[2025-02-13 13:43:31,358][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 316
[2025-02-13 13:43:52,097][rgc][INFO] - 	Updating weights of batch 316
[2025-02-13 13:43:52,156][rgc][INFO] - Batch 316, avg loss per batch: 2.349730827173836
[2025-02-13 13:43:52,157][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 317
[2025-02-13 13:44:12,917][rgc][INFO] - 	Updating weights of batch 317
[2025-02-13 13:44:12,972][rgc][INFO] - Batch 317, avg loss per batch: 2.2536746350680206
[2025-02-13 13:44:12,973][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 318
[2025-02-13 13:44:33,693][rgc][INFO] - 	Updating weights of batch 318
[2025-02-13 13:44:33,746][rgc][INFO] - Batch 318, avg loss per batch: 1.5751482155681233
[2025-02-13 13:44:33,747][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 319
[2025-02-13 13:44:54,515][rgc][INFO] - 	Updating weights of batch 319
[2025-02-13 13:44:54,564][rgc][INFO] - Batch 319, avg loss per batch: 2.0893839555331177
[2025-02-13 13:44:54,565][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 320
[2025-02-13 13:45:15,243][rgc][INFO] - 	Updating weights of batch 320
[2025-02-13 13:45:15,295][rgc][INFO] - Batch 320, avg loss per batch: 2.9473928359432873
[2025-02-13 13:45:15,296][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 321
[2025-02-13 13:45:35,976][rgc][INFO] - 	Updating weights of batch 321
[2025-02-13 13:45:36,035][rgc][INFO] - Batch 321, avg loss per batch: 3.079784107370985
[2025-02-13 13:45:36,036][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 322
[2025-02-13 13:45:56,777][rgc][INFO] - 	Updating weights of batch 322
[2025-02-13 13:45:56,829][rgc][INFO] - Batch 322, avg loss per batch: 2.479324017166063
[2025-02-13 13:45:56,830][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 323
[2025-02-13 13:46:17,571][rgc][INFO] - 	Updating weights of batch 323
[2025-02-13 13:46:17,626][rgc][INFO] - Batch 323, avg loss per batch: 2.4198046011674874
[2025-02-13 13:46:17,627][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 324
[2025-02-13 13:46:38,298][rgc][INFO] - 	Updating weights of batch 324
[2025-02-13 13:46:38,352][rgc][INFO] - Batch 324, avg loss per batch: 3.2955848866443214
[2025-02-13 13:46:38,353][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 325
[2025-02-13 13:46:59,029][rgc][INFO] - 	Updating weights of batch 325
[2025-02-13 13:46:59,080][rgc][INFO] - Batch 325, avg loss per batch: 2.2887116510739536
[2025-02-13 13:46:59,081][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 326
[2025-02-13 13:47:19,760][rgc][INFO] - 	Updating weights of batch 326
[2025-02-13 13:47:19,842][rgc][INFO] - Batch 326, avg loss per batch: 1.7528896237893044
[2025-02-13 13:47:19,843][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 327
[2025-02-13 13:47:40,553][rgc][INFO] - 	Updating weights of batch 327
[2025-02-13 13:47:40,606][rgc][INFO] - Batch 327, avg loss per batch: 2.785939793157129
[2025-02-13 13:47:40,606][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 328
[2025-02-13 13:48:01,278][rgc][INFO] - 	Updating weights of batch 328
[2025-02-13 13:48:01,330][rgc][INFO] - Batch 328, avg loss per batch: 2.774563057032191
[2025-02-13 13:48:01,332][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 329
[2025-02-13 13:48:22,098][rgc][INFO] - 	Updating weights of batch 329
[2025-02-13 13:48:22,151][rgc][INFO] - Batch 329, avg loss per batch: 4.042059205699256
[2025-02-13 13:48:22,152][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 330
[2025-02-13 13:48:42,822][rgc][INFO] - 	Updating weights of batch 330
[2025-02-13 13:48:42,878][rgc][INFO] - Batch 330, avg loss per batch: 2.866736012606316
[2025-02-13 13:48:42,879][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 331
[2025-02-13 13:49:03,538][rgc][INFO] - 	Updating weights of batch 331
[2025-02-13 13:49:03,594][rgc][INFO] - Batch 331, avg loss per batch: 3.6612298705736848
[2025-02-13 13:49:03,595][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 332
[2025-02-13 13:49:24,362][rgc][INFO] - 	Updating weights of batch 332
[2025-02-13 13:49:24,414][rgc][INFO] - Batch 332, avg loss per batch: 3.026964436666614
[2025-02-13 13:49:24,415][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 333
[2025-02-13 13:49:45,089][rgc][INFO] - 	Updating weights of batch 333
[2025-02-13 13:49:45,143][rgc][INFO] - Batch 333, avg loss per batch: 4.34743994942398
[2025-02-13 13:49:45,144][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 334
[2025-02-13 13:50:05,809][rgc][INFO] - 	Updating weights of batch 334
[2025-02-13 13:50:05,864][rgc][INFO] - Batch 334, avg loss per batch: 5.129554021686143
[2025-02-13 13:50:05,864][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 335
[2025-02-13 13:50:26,530][rgc][INFO] - 	Updating weights of batch 335
[2025-02-13 13:50:26,582][rgc][INFO] - Batch 335, avg loss per batch: 3.6481309429118296
[2025-02-13 13:50:26,583][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 336
[2025-02-13 13:50:47,302][rgc][INFO] - 	Updating weights of batch 336
[2025-02-13 13:50:47,357][rgc][INFO] - Batch 336, avg loss per batch: 7.021622432442846
[2025-02-13 13:50:47,358][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 337
[2025-02-13 13:51:08,015][rgc][INFO] - 	Updating weights of batch 337
[2025-02-13 13:51:08,062][rgc][INFO] - Batch 337, avg loss per batch: 1.907770221692041
[2025-02-13 13:51:08,063][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 338
[2025-02-13 13:51:28,789][rgc][INFO] - 	Updating weights of batch 338
[2025-02-13 13:51:28,839][rgc][INFO] - Batch 338, avg loss per batch: 2.1400765044055348
[2025-02-13 13:51:28,840][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 339
[2025-02-13 13:51:49,510][rgc][INFO] - 	Updating weights of batch 339
[2025-02-13 13:51:49,561][rgc][INFO] - Batch 339, avg loss per batch: 2.48624082277814
[2025-02-13 13:51:49,562][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 340
[2025-02-13 13:52:10,301][rgc][INFO] - 	Updating weights of batch 340
[2025-02-13 13:52:10,366][rgc][INFO] - Batch 340, avg loss per batch: 3.054266402438807
[2025-02-13 13:52:10,366][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 341
[2025-02-13 13:52:31,036][rgc][INFO] - 	Updating weights of batch 341
[2025-02-13 13:52:31,088][rgc][INFO] - Batch 341, avg loss per batch: 3.368333990107594
[2025-02-13 13:52:31,089][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 342
[2025-02-13 13:52:51,827][rgc][INFO] - 	Updating weights of batch 342
[2025-02-13 13:52:51,883][rgc][INFO] - Batch 342, avg loss per batch: 1.3723903558280848
[2025-02-13 13:52:51,884][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 343
[2025-02-13 13:53:12,570][rgc][INFO] - 	Updating weights of batch 343
[2025-02-13 13:53:12,629][rgc][INFO] - Batch 343, avg loss per batch: 3.8554846481101226
[2025-02-13 13:53:12,631][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 344
[2025-02-13 13:53:33,303][rgc][INFO] - 	Updating weights of batch 344
[2025-02-13 13:53:33,358][rgc][INFO] - Batch 344, avg loss per batch: 3.5109120277610564
[2025-02-13 13:53:33,360][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 345
[2025-02-13 13:53:54,126][rgc][INFO] - 	Updating weights of batch 345
[2025-02-13 13:53:54,180][rgc][INFO] - Batch 345, avg loss per batch: 4.443786633695904
[2025-02-13 13:53:54,182][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 346
[2025-02-13 13:54:14,871][rgc][INFO] - 	Updating weights of batch 346
[2025-02-13 13:54:14,918][rgc][INFO] - Batch 346, avg loss per batch: 2.6218329132712537
[2025-02-13 13:54:14,919][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 347
[2025-02-13 13:54:35,691][rgc][INFO] - 	Updating weights of batch 347
[2025-02-13 13:54:35,743][rgc][INFO] - Batch 347, avg loss per batch: 2.4055052373608583
[2025-02-13 13:54:35,744][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 348
[2025-02-13 13:54:56,479][rgc][INFO] - 	Updating weights of batch 348
[2025-02-13 13:54:56,528][rgc][INFO] - Batch 348, avg loss per batch: 2.369747499251952
[2025-02-13 13:54:56,528][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 349
[2025-02-13 13:55:17,202][rgc][INFO] - 	Updating weights of batch 349
[2025-02-13 13:55:17,255][rgc][INFO] - Batch 349, avg loss per batch: 1.8444299844933376
[2025-02-13 13:55:17,256][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 350
[2025-02-13 13:55:38,019][rgc][INFO] - 	Updating weights of batch 350
[2025-02-13 13:55:38,074][rgc][INFO] - Batch 350, avg loss per batch: 3.4649230451040443
[2025-02-13 13:55:38,075][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 351
[2025-02-13 13:55:58,752][rgc][INFO] - 	Updating weights of batch 351
[2025-02-13 13:55:58,809][rgc][INFO] - Batch 351, avg loss per batch: 3.54281653364985
[2025-02-13 13:55:58,810][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 352
[2025-02-13 13:56:19,475][rgc][INFO] - 	Updating weights of batch 352
[2025-02-13 13:56:19,527][rgc][INFO] - Batch 352, avg loss per batch: 2.1242747765690972
[2025-02-13 13:56:19,528][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 353
[2025-02-13 13:56:40,203][rgc][INFO] - 	Updating weights of batch 353
[2025-02-13 13:56:40,254][rgc][INFO] - Batch 353, avg loss per batch: 2.061521616389621
[2025-02-13 13:56:40,255][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 354
[2025-02-13 13:57:00,994][rgc][INFO] - 	Updating weights of batch 354
[2025-02-13 13:57:01,046][rgc][INFO] - Batch 354, avg loss per batch: 2.79250353831865
[2025-02-13 13:57:01,047][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 355
[2025-02-13 13:57:21,731][rgc][INFO] - 	Updating weights of batch 355
[2025-02-13 13:57:21,781][rgc][INFO] - Batch 355, avg loss per batch: 3.918949213366014
[2025-02-13 13:57:21,782][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 356
[2025-02-13 13:57:42,445][rgc][INFO] - 	Updating weights of batch 356
[2025-02-13 13:57:42,497][rgc][INFO] - Batch 356, avg loss per batch: 3.292858293440389
[2025-02-13 13:57:42,498][rgc][INFO] - 	Applying batch grad function of epoch 1 and batch 357
[2025-02-13 13:58:03,247][rgc][INFO] - 	Updating weights of batch 357
[2025-02-13 13:58:03,300][rgc][INFO] - Batch 357, avg loss per batch: 2.3340253762290617
[2025-02-13 13:58:03,309][rgc][INFO] - ================= Epoch 1, loss: 1003.9428249982789 ===============
[2025-02-13 13:58:03,310][rgc][INFO] - Visualizing histograms
[2025-02-13 13:58:26,167][rgc][INFO] - AVG rho on val data: 0.24348048934526823
[2025-02-13 13:58:26,167][rgc][INFO] - AVG Mean Absolute Error on val data: 0.53948400825579
[2025-02-13 13:58:37,345][rgc][INFO] - AVG rho on test data: 0.30560671888042823
[2025-02-13 13:58:37,345][rgc][INFO] - AVG Mean Absolute Error on test data: 0.573860430055739
[2025-02-13 13:58:50,181][rgc][INFO] - AVG rho on train data: 0.2186751326195528
[2025-02-13 13:58:50,182][rgc][INFO] - AVG Mean Absolute Error on train data: 0.5553182736784452
[2025-02-13 13:58:50,182][rgc][INFO] - Current best rhos: train 0.22776812926799744, val 0.28403201292146585, test 0.3118059305914899
[2025-02-13 13:58:50,197][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 0
[2025-02-13 13:59:10,888][rgc][INFO] - 	Updating weights of batch 0
[2025-02-13 13:59:10,940][rgc][INFO] - Batch 0, avg loss per batch: 3.0848565349152297
[2025-02-13 13:59:10,941][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 1
[2025-02-13 13:59:31,612][rgc][INFO] - 	Updating weights of batch 1
[2025-02-13 13:59:31,667][rgc][INFO] - Batch 1, avg loss per batch: 1.4902823806699366
[2025-02-13 13:59:31,668][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 2
[2025-02-13 13:59:52,350][rgc][INFO] - 	Updating weights of batch 2
[2025-02-13 13:59:52,399][rgc][INFO] - Batch 2, avg loss per batch: 4.472350212834272
[2025-02-13 13:59:52,400][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 3
[2025-02-13 14:00:13,084][rgc][INFO] - 	Updating weights of batch 3
[2025-02-13 14:00:13,137][rgc][INFO] - Batch 3, avg loss per batch: 4.887292311209661
[2025-02-13 14:00:13,138][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 4
[2025-02-13 14:00:33,836][rgc][INFO] - 	Updating weights of batch 4
[2025-02-13 14:00:33,886][rgc][INFO] - Batch 4, avg loss per batch: 3.4293907387233022
[2025-02-13 14:00:33,887][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 5
[2025-02-13 14:00:54,627][rgc][INFO] - 	Updating weights of batch 5
[2025-02-13 14:00:54,690][rgc][INFO] - Batch 5, avg loss per batch: 1.8203788731315227
[2025-02-13 14:00:54,691][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 6
[2025-02-13 14:01:15,439][rgc][INFO] - 	Updating weights of batch 6
[2025-02-13 14:01:15,489][rgc][INFO] - Batch 6, avg loss per batch: 3.439564141252094
[2025-02-13 14:01:15,490][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 7
[2025-02-13 14:01:36,270][rgc][INFO] - 	Updating weights of batch 7
[2025-02-13 14:01:36,327][rgc][INFO] - Batch 7, avg loss per batch: 3.424236193541015
[2025-02-13 14:01:36,328][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 8
[2025-02-13 14:01:57,022][rgc][INFO] - 	Updating weights of batch 8
[2025-02-13 14:01:57,076][rgc][INFO] - Batch 8, avg loss per batch: 2.955262034080292
[2025-02-13 14:01:57,076][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 9
[2025-02-13 14:02:17,837][rgc][INFO] - 	Updating weights of batch 9
[2025-02-13 14:02:17,892][rgc][INFO] - Batch 9, avg loss per batch: 1.7165962333204179
[2025-02-13 14:02:17,893][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 10
[2025-02-13 14:02:38,580][rgc][INFO] - 	Updating weights of batch 10
[2025-02-13 14:02:38,634][rgc][INFO] - Batch 10, avg loss per batch: 3.7404807325693747
[2025-02-13 14:02:38,634][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 11
[2025-02-13 14:02:59,375][rgc][INFO] - 	Updating weights of batch 11
[2025-02-13 14:02:59,425][rgc][INFO] - Batch 11, avg loss per batch: 2.5093881388813437
[2025-02-13 14:02:59,425][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 12
[2025-02-13 14:03:20,114][rgc][INFO] - 	Updating weights of batch 12
[2025-02-13 14:03:20,166][rgc][INFO] - Batch 12, avg loss per batch: 1.5250503133407332
[2025-02-13 14:03:20,167][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 13
[2025-02-13 14:03:40,946][rgc][INFO] - 	Updating weights of batch 13
[2025-02-13 14:03:41,001][rgc][INFO] - Batch 13, avg loss per batch: 1.381504552097366
[2025-02-13 14:03:41,002][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 14
[2025-02-13 14:04:01,744][rgc][INFO] - 	Updating weights of batch 14
[2025-02-13 14:04:01,798][rgc][INFO] - Batch 14, avg loss per batch: 4.412147872837295
[2025-02-13 14:04:01,798][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 15
[2025-02-13 14:04:22,465][rgc][INFO] - 	Updating weights of batch 15
[2025-02-13 14:04:22,527][rgc][INFO] - Batch 15, avg loss per batch: 1.6414188116400985
[2025-02-13 14:04:22,528][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 16
[2025-02-13 14:04:43,276][rgc][INFO] - 	Updating weights of batch 16
[2025-02-13 14:04:43,339][rgc][INFO] - Batch 16, avg loss per batch: 3.734599793236858
[2025-02-13 14:04:43,342][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 17
[2025-02-13 14:05:04,031][rgc][INFO] - 	Updating weights of batch 17
[2025-02-13 14:05:04,088][rgc][INFO] - Batch 17, avg loss per batch: 4.3906521068864865
[2025-02-13 14:05:04,088][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 18
[2025-02-13 14:05:24,756][rgc][INFO] - 	Updating weights of batch 18
[2025-02-13 14:05:24,810][rgc][INFO] - Batch 18, avg loss per batch: 3.322415190636109
[2025-02-13 14:05:24,811][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 19
[2025-02-13 14:05:45,571][rgc][INFO] - 	Updating weights of batch 19
[2025-02-13 14:05:45,628][rgc][INFO] - Batch 19, avg loss per batch: 4.616754903979251
[2025-02-13 14:05:45,629][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 20
[2025-02-13 14:06:06,316][rgc][INFO] - 	Updating weights of batch 20
[2025-02-13 14:06:06,369][rgc][INFO] - Batch 20, avg loss per batch: 1.7825973277598928
[2025-02-13 14:06:06,370][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 21
[2025-02-13 14:06:27,122][rgc][INFO] - 	Updating weights of batch 21
[2025-02-13 14:06:27,174][rgc][INFO] - Batch 21, avg loss per batch: 2.639591963392587
[2025-02-13 14:06:27,175][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 22
[2025-02-13 14:06:47,853][rgc][INFO] - 	Updating weights of batch 22
[2025-02-13 14:06:47,904][rgc][INFO] - Batch 22, avg loss per batch: 1.9056561695553154
[2025-02-13 14:06:47,905][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 23
[2025-02-13 14:07:08,598][rgc][INFO] - 	Updating weights of batch 23
[2025-02-13 14:07:08,654][rgc][INFO] - Batch 23, avg loss per batch: 2.0215360703648457
[2025-02-13 14:07:08,655][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 24
[2025-02-13 14:07:29,349][rgc][INFO] - 	Updating weights of batch 24
[2025-02-13 14:07:29,402][rgc][INFO] - Batch 24, avg loss per batch: 2.8224520173706713
[2025-02-13 14:07:29,403][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 25
[2025-02-13 14:07:50,092][rgc][INFO] - 	Updating weights of batch 25
[2025-02-13 14:07:50,149][rgc][INFO] - Batch 25, avg loss per batch: 2.7204451058392993
[2025-02-13 14:07:50,150][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 26
[2025-02-13 14:08:10,885][rgc][INFO] - 	Updating weights of batch 26
[2025-02-13 14:08:10,936][rgc][INFO] - Batch 26, avg loss per batch: 3.719613276566836
[2025-02-13 14:08:10,936][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 27
[2025-02-13 14:08:31,628][rgc][INFO] - 	Updating weights of batch 27
[2025-02-13 14:08:31,675][rgc][INFO] - Batch 27, avg loss per batch: 1.9224594029426576
[2025-02-13 14:08:31,676][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 28
[2025-02-13 14:08:52,364][rgc][INFO] - 	Updating weights of batch 28
[2025-02-13 14:08:52,425][rgc][INFO] - Batch 28, avg loss per batch: 3.6612893013651977
[2025-02-13 14:08:52,426][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 29
[2025-02-13 14:09:13,088][rgc][INFO] - 	Updating weights of batch 29
[2025-02-13 14:09:13,141][rgc][INFO] - Batch 29, avg loss per batch: 2.239450254811162
[2025-02-13 14:09:13,142][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 30
[2025-02-13 14:09:33,841][rgc][INFO] - 	Updating weights of batch 30
[2025-02-13 14:09:33,893][rgc][INFO] - Batch 30, avg loss per batch: 3.7339391808349163
[2025-02-13 14:09:33,894][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 31
[2025-02-13 14:09:54,571][rgc][INFO] - 	Updating weights of batch 31
[2025-02-13 14:09:54,625][rgc][INFO] - Batch 31, avg loss per batch: 4.421457991862039
[2025-02-13 14:09:54,626][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 32
[2025-02-13 14:10:15,376][rgc][INFO] - 	Updating weights of batch 32
[2025-02-13 14:10:15,434][rgc][INFO] - Batch 32, avg loss per batch: 3.110879842580342
[2025-02-13 14:10:15,434][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 33
[2025-02-13 14:10:36,128][rgc][INFO] - 	Updating weights of batch 33
[2025-02-13 14:10:36,179][rgc][INFO] - Batch 33, avg loss per batch: 5.0930532806606506
[2025-02-13 14:10:36,181][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 34
[2025-02-13 14:10:56,949][rgc][INFO] - 	Updating weights of batch 34
[2025-02-13 14:10:57,025][rgc][INFO] - Batch 34, avg loss per batch: 3.517818350306329
[2025-02-13 14:10:57,026][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 35
[2025-02-13 14:11:17,706][rgc][INFO] - 	Updating weights of batch 35
[2025-02-13 14:11:17,756][rgc][INFO] - Batch 35, avg loss per batch: 3.092262187960174
[2025-02-13 14:11:17,757][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 36
[2025-02-13 14:11:38,460][rgc][INFO] - 	Updating weights of batch 36
[2025-02-13 14:11:38,527][rgc][INFO] - Batch 36, avg loss per batch: 5.293436885076746
[2025-02-13 14:11:38,528][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 37
[2025-02-13 14:11:59,265][rgc][INFO] - 	Updating weights of batch 37
[2025-02-13 14:11:59,319][rgc][INFO] - Batch 37, avg loss per batch: 1.978970617745293
[2025-02-13 14:11:59,320][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 38
[2025-02-13 14:12:20,041][rgc][INFO] - 	Updating weights of batch 38
[2025-02-13 14:12:20,104][rgc][INFO] - Batch 38, avg loss per batch: 2.4202254659334574
[2025-02-13 14:12:20,106][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 39
[2025-02-13 14:12:40,786][rgc][INFO] - 	Updating weights of batch 39
[2025-02-13 14:12:40,840][rgc][INFO] - Batch 39, avg loss per batch: 1.196185784441325
[2025-02-13 14:12:40,841][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 40
[2025-02-13 14:13:01,602][rgc][INFO] - 	Updating weights of batch 40
[2025-02-13 14:13:01,656][rgc][INFO] - Batch 40, avg loss per batch: 3.7549192409123444
[2025-02-13 14:13:01,657][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 41
[2025-02-13 14:13:22,343][rgc][INFO] - 	Updating weights of batch 41
[2025-02-13 14:13:22,394][rgc][INFO] - Batch 41, avg loss per batch: 1.5482996264983804
[2025-02-13 14:13:22,396][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 42
[2025-02-13 14:13:43,079][rgc][INFO] - 	Updating weights of batch 42
[2025-02-13 14:13:43,133][rgc][INFO] - Batch 42, avg loss per batch: 2.855286293852701
[2025-02-13 14:13:43,134][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 43
[2025-02-13 14:14:03,806][rgc][INFO] - 	Updating weights of batch 43
[2025-02-13 14:14:03,861][rgc][INFO] - Batch 43, avg loss per batch: 3.325486814371475
[2025-02-13 14:14:03,862][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 44
[2025-02-13 14:14:24,546][rgc][INFO] - 	Updating weights of batch 44
[2025-02-13 14:14:24,602][rgc][INFO] - Batch 44, avg loss per batch: 3.9291813609607473
[2025-02-13 14:14:24,603][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 45
[2025-02-13 14:14:45,326][rgc][INFO] - 	Updating weights of batch 45
[2025-02-13 14:14:45,380][rgc][INFO] - Batch 45, avg loss per batch: 2.3189742521709102
[2025-02-13 14:14:45,382][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 46
[2025-02-13 14:15:06,146][rgc][INFO] - 	Updating weights of batch 46
[2025-02-13 14:15:06,200][rgc][INFO] - Batch 46, avg loss per batch: 1.0016744005150373
[2025-02-13 14:15:06,200][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 47
[2025-02-13 14:15:26,902][rgc][INFO] - 	Updating weights of batch 47
[2025-02-13 14:15:26,956][rgc][INFO] - Batch 47, avg loss per batch: 3.7028994268294952
[2025-02-13 14:15:26,957][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 48
[2025-02-13 14:15:47,647][rgc][INFO] - 	Updating weights of batch 48
[2025-02-13 14:15:47,704][rgc][INFO] - Batch 48, avg loss per batch: 4.216810364036541
[2025-02-13 14:15:47,705][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 49
[2025-02-13 14:16:08,389][rgc][INFO] - 	Updating weights of batch 49
[2025-02-13 14:16:08,474][rgc][INFO] - Batch 49, avg loss per batch: 3.30984667497576
[2025-02-13 14:16:08,474][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 50
[2025-02-13 14:16:29,153][rgc][INFO] - 	Updating weights of batch 50
[2025-02-13 14:16:29,235][rgc][INFO] - Batch 50, avg loss per batch: 3.348465826705471
[2025-02-13 14:16:29,237][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 51
[2025-02-13 14:16:49,960][rgc][INFO] - 	Updating weights of batch 51
[2025-02-13 14:16:50,009][rgc][INFO] - Batch 51, avg loss per batch: 2.7898157396582945
[2025-02-13 14:16:50,010][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 52
[2025-02-13 14:17:10,747][rgc][INFO] - 	Updating weights of batch 52
[2025-02-13 14:17:10,802][rgc][INFO] - Batch 52, avg loss per batch: 1.125150923696808
[2025-02-13 14:17:10,803][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 53
[2025-02-13 14:17:31,492][rgc][INFO] - 	Updating weights of batch 53
[2025-02-13 14:17:31,542][rgc][INFO] - Batch 53, avg loss per batch: 2.496379041306598
[2025-02-13 14:17:31,543][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 54
[2025-02-13 14:17:52,213][rgc][INFO] - 	Updating weights of batch 54
[2025-02-13 14:17:52,263][rgc][INFO] - Batch 54, avg loss per batch: 3.9521518312165544
[2025-02-13 14:17:52,264][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 55
[2025-02-13 14:18:12,944][rgc][INFO] - 	Updating weights of batch 55
[2025-02-13 14:18:13,004][rgc][INFO] - Batch 55, avg loss per batch: 1.033720957809928
[2025-02-13 14:18:13,006][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 56
[2025-02-13 14:18:33,732][rgc][INFO] - 	Updating weights of batch 56
[2025-02-13 14:18:33,812][rgc][INFO] - Batch 56, avg loss per batch: 3.056626816498301
[2025-02-13 14:18:33,814][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 57
[2025-02-13 14:18:54,496][rgc][INFO] - 	Updating weights of batch 57
[2025-02-13 14:18:54,548][rgc][INFO] - Batch 57, avg loss per batch: 2.2836696292438985
[2025-02-13 14:18:54,549][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 58
[2025-02-13 14:19:15,221][rgc][INFO] - 	Updating weights of batch 58
[2025-02-13 14:19:15,277][rgc][INFO] - Batch 58, avg loss per batch: 1.7656798988727107
[2025-02-13 14:19:15,278][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 59
[2025-02-13 14:19:36,028][rgc][INFO] - 	Updating weights of batch 59
[2025-02-13 14:19:36,082][rgc][INFO] - Batch 59, avg loss per batch: 4.130249362611829
[2025-02-13 14:19:36,083][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 60
[2025-02-13 14:19:56,757][rgc][INFO] - 	Updating weights of batch 60
[2025-02-13 14:19:56,808][rgc][INFO] - Batch 60, avg loss per batch: 3.3723603360168855
[2025-02-13 14:19:56,809][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 61
[2025-02-13 14:20:17,553][rgc][INFO] - 	Updating weights of batch 61
[2025-02-13 14:20:17,630][rgc][INFO] - Batch 61, avg loss per batch: 4.34641534885208
[2025-02-13 14:20:17,631][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 62
[2025-02-13 14:20:38,375][rgc][INFO] - 	Updating weights of batch 62
[2025-02-13 14:20:38,436][rgc][INFO] - Batch 62, avg loss per batch: 1.8877619581765837
[2025-02-13 14:20:38,437][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 63
[2025-02-13 14:20:59,121][rgc][INFO] - 	Updating weights of batch 63
[2025-02-13 14:20:59,182][rgc][INFO] - Batch 63, avg loss per batch: 3.1501230225013837
[2025-02-13 14:20:59,182][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 64
[2025-02-13 14:21:19,861][rgc][INFO] - 	Updating weights of batch 64
[2025-02-13 14:21:19,916][rgc][INFO] - Batch 64, avg loss per batch: 1.4593674090613096
[2025-02-13 14:21:19,917][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 65
[2025-02-13 14:21:40,593][rgc][INFO] - 	Updating weights of batch 65
[2025-02-13 14:21:40,643][rgc][INFO] - Batch 65, avg loss per batch: 2.1586311569079566
[2025-02-13 14:21:40,644][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 66
[2025-02-13 14:22:01,382][rgc][INFO] - 	Updating weights of batch 66
[2025-02-13 14:22:01,435][rgc][INFO] - Batch 66, avg loss per batch: 2.906558587943582
[2025-02-13 14:22:01,436][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 67
[2025-02-13 14:22:22,113][rgc][INFO] - 	Updating weights of batch 67
[2025-02-13 14:22:22,165][rgc][INFO] - Batch 67, avg loss per batch: 1.6654336744486025
[2025-02-13 14:22:22,166][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 68
[2025-02-13 14:22:42,903][rgc][INFO] - 	Updating weights of batch 68
[2025-02-13 14:22:42,960][rgc][INFO] - Batch 68, avg loss per batch: 2.039751366997992
[2025-02-13 14:22:42,961][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 69
[2025-02-13 14:23:03,645][rgc][INFO] - 	Updating weights of batch 69
[2025-02-13 14:23:03,696][rgc][INFO] - Batch 69, avg loss per batch: 2.7739493216132773
[2025-02-13 14:23:03,697][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 70
[2025-02-13 14:23:24,382][rgc][INFO] - 	Updating weights of batch 70
[2025-02-13 14:23:24,458][rgc][INFO] - Batch 70, avg loss per batch: 4.318995975943536
[2025-02-13 14:23:24,459][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 71
[2025-02-13 14:23:45,199][rgc][INFO] - 	Updating weights of batch 71
[2025-02-13 14:23:45,254][rgc][INFO] - Batch 71, avg loss per batch: 5.348538978956301
[2025-02-13 14:23:45,255][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 72
[2025-02-13 14:24:05,999][rgc][INFO] - 	Updating weights of batch 72
[2025-02-13 14:24:06,053][rgc][INFO] - Batch 72, avg loss per batch: 2.37950312984781
[2025-02-13 14:24:06,055][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 73
[2025-02-13 14:24:26,798][rgc][INFO] - 	Updating weights of batch 73
[2025-02-13 14:24:26,861][rgc][INFO] - Batch 73, avg loss per batch: 5.615222984802971
[2025-02-13 14:24:26,862][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 74
[2025-02-13 14:24:47,589][rgc][INFO] - 	Updating weights of batch 74
[2025-02-13 14:24:47,637][rgc][INFO] - Batch 74, avg loss per batch: 3.248473079025196
[2025-02-13 14:24:47,638][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 75
[2025-02-13 14:25:08,321][rgc][INFO] - 	Updating weights of batch 75
[2025-02-13 14:25:08,385][rgc][INFO] - Batch 75, avg loss per batch: 2.8604844539609253
[2025-02-13 14:25:08,387][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 76
[2025-02-13 14:25:29,110][rgc][INFO] - 	Updating weights of batch 76
[2025-02-13 14:25:29,165][rgc][INFO] - Batch 76, avg loss per batch: 1.9707495247748543
[2025-02-13 14:25:29,166][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 77
[2025-02-13 14:25:49,855][rgc][INFO] - 	Updating weights of batch 77
[2025-02-13 14:25:49,927][rgc][INFO] - Batch 77, avg loss per batch: 2.210830708963575
[2025-02-13 14:25:49,928][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 78
[2025-02-13 14:26:10,582][rgc][INFO] - 	Updating weights of batch 78
[2025-02-13 14:26:10,631][rgc][INFO] - Batch 78, avg loss per batch: 1.9282540721287011
[2025-02-13 14:26:10,632][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 79
[2025-02-13 14:26:31,294][rgc][INFO] - 	Updating weights of batch 79
[2025-02-13 14:26:31,345][rgc][INFO] - Batch 79, avg loss per batch: 3.4477784337976294
[2025-02-13 14:26:31,345][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 80
[2025-02-13 14:26:52,099][rgc][INFO] - 	Updating weights of batch 80
[2025-02-13 14:26:52,152][rgc][INFO] - Batch 80, avg loss per batch: 2.482447550909575
[2025-02-13 14:26:52,153][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 81
[2025-02-13 14:27:12,885][rgc][INFO] - 	Updating weights of batch 81
[2025-02-13 14:27:12,940][rgc][INFO] - Batch 81, avg loss per batch: 2.481297291004383
[2025-02-13 14:27:12,941][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 82
[2025-02-13 14:27:33,635][rgc][INFO] - 	Updating weights of batch 82
[2025-02-13 14:27:33,691][rgc][INFO] - Batch 82, avg loss per batch: 1.9772632326080368
[2025-02-13 14:27:33,692][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 83
[2025-02-13 14:27:54,386][rgc][INFO] - 	Updating weights of batch 83
[2025-02-13 14:27:54,436][rgc][INFO] - Batch 83, avg loss per batch: 3.5724999203172265
[2025-02-13 14:27:54,437][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 84
[2025-02-13 14:28:15,138][rgc][INFO] - 	Updating weights of batch 84
[2025-02-13 14:28:15,202][rgc][INFO] - Batch 84, avg loss per batch: 1.8497082010673633
[2025-02-13 14:28:15,203][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 85
[2025-02-13 14:28:35,892][rgc][INFO] - 	Updating weights of batch 85
[2025-02-13 14:28:35,946][rgc][INFO] - Batch 85, avg loss per batch: 0.8932300143176249
[2025-02-13 14:28:35,947][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 86
[2025-02-13 14:28:56,694][rgc][INFO] - 	Updating weights of batch 86
[2025-02-13 14:28:56,749][rgc][INFO] - Batch 86, avg loss per batch: 3.4758434207626134
[2025-02-13 14:28:56,749][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 87
[2025-02-13 14:29:17,475][rgc][INFO] - 	Updating weights of batch 87
[2025-02-13 14:29:17,532][rgc][INFO] - Batch 87, avg loss per batch: 5.17397322292206
[2025-02-13 14:29:17,533][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 88
[2025-02-13 14:29:38,226][rgc][INFO] - 	Updating weights of batch 88
[2025-02-13 14:29:38,291][rgc][INFO] - Batch 88, avg loss per batch: 2.294142794951151
[2025-02-13 14:29:38,292][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 89
[2025-02-13 14:29:59,055][rgc][INFO] - 	Updating weights of batch 89
[2025-02-13 14:29:59,130][rgc][INFO] - Batch 89, avg loss per batch: 4.942683404364509
[2025-02-13 14:29:59,130][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 90
[2025-02-13 14:30:19,877][rgc][INFO] - 	Updating weights of batch 90
[2025-02-13 14:30:19,928][rgc][INFO] - Batch 90, avg loss per batch: 5.702222998929039
[2025-02-13 14:30:19,929][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 91
[2025-02-13 14:30:40,680][rgc][INFO] - 	Updating weights of batch 91
[2025-02-13 14:30:40,742][rgc][INFO] - Batch 91, avg loss per batch: 2.35748850083958
[2025-02-13 14:30:40,743][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 92
[2025-02-13 14:31:01,487][rgc][INFO] - 	Updating weights of batch 92
[2025-02-13 14:31:01,537][rgc][INFO] - Batch 92, avg loss per batch: 2.2934931900950044
[2025-02-13 14:31:01,537][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 93
[2025-02-13 14:31:22,273][rgc][INFO] - 	Updating weights of batch 93
[2025-02-13 14:31:22,336][rgc][INFO] - Batch 93, avg loss per batch: 4.291245264682405
[2025-02-13 14:31:22,337][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 94
[2025-02-13 14:31:43,093][rgc][INFO] - 	Updating weights of batch 94
[2025-02-13 14:31:43,146][rgc][INFO] - Batch 94, avg loss per batch: 2.871935606254601
[2025-02-13 14:31:43,147][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 95
[2025-02-13 14:32:03,809][rgc][INFO] - 	Updating weights of batch 95
[2025-02-13 14:32:03,859][rgc][INFO] - Batch 95, avg loss per batch: 2.9470438012644875
[2025-02-13 14:32:03,860][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 96
[2025-02-13 14:32:24,566][rgc][INFO] - 	Updating weights of batch 96
[2025-02-13 14:32:24,623][rgc][INFO] - Batch 96, avg loss per batch: 3.1203471173980386
[2025-02-13 14:32:24,624][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 97
[2025-02-13 14:32:45,385][rgc][INFO] - 	Updating weights of batch 97
[2025-02-13 14:32:45,453][rgc][INFO] - Batch 97, avg loss per batch: 3.1729776966121137
[2025-02-13 14:32:45,454][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 98
[2025-02-13 14:33:06,195][rgc][INFO] - 	Updating weights of batch 98
[2025-02-13 14:33:06,247][rgc][INFO] - Batch 98, avg loss per batch: 3.3357911740669697
[2025-02-13 14:33:06,247][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 99
[2025-02-13 14:33:27,015][rgc][INFO] - 	Updating weights of batch 99
[2025-02-13 14:33:27,069][rgc][INFO] - Batch 99, avg loss per batch: 1.890791482962371
[2025-02-13 14:33:38,614][rgc][INFO] - AVG rho on val data: 0.29143604268683954
[2025-02-13 14:33:38,615][rgc][INFO] - AVG mae on val data: 0.5324797858299913
[2025-02-13 14:33:49,792][rgc][INFO] - AVG rho on test data: 0.3107578635750144
[2025-02-13 14:33:49,793][rgc][INFO] - AVG mae on test data: 0.5806867669384876
[2025-02-13 14:34:02,637][rgc][INFO] - AVG rho on train data: 0.23085750998819768
[2025-02-13 14:34:02,638][rgc][INFO] - AVG mae on train data: 0.5525308117029927
[2025-02-13 14:34:02,638][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 14:34:02,640][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 100
[2025-02-13 14:34:23,415][rgc][INFO] - 	Updating weights of batch 100
[2025-02-13 14:34:23,472][rgc][INFO] - Batch 100, avg loss per batch: 2.6726575443176106
[2025-02-13 14:34:23,473][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 101
[2025-02-13 14:34:44,197][rgc][INFO] - 	Updating weights of batch 101
[2025-02-13 14:34:44,248][rgc][INFO] - Batch 101, avg loss per batch: 1.943045741208536
[2025-02-13 14:34:44,249][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 102
[2025-02-13 14:35:04,944][rgc][INFO] - 	Updating weights of batch 102
[2025-02-13 14:35:04,997][rgc][INFO] - Batch 102, avg loss per batch: 5.648878954609123
[2025-02-13 14:35:04,999][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 103
[2025-02-13 14:35:25,759][rgc][INFO] - 	Updating weights of batch 103
[2025-02-13 14:35:25,812][rgc][INFO] - Batch 103, avg loss per batch: 2.4650772249904795
[2025-02-13 14:35:25,812][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 104
[2025-02-13 14:35:46,581][rgc][INFO] - 	Updating weights of batch 104
[2025-02-13 14:35:46,633][rgc][INFO] - Batch 104, avg loss per batch: 1.9018083306604308
[2025-02-13 14:35:46,634][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 105
[2025-02-13 14:36:07,405][rgc][INFO] - 	Updating weights of batch 105
[2025-02-13 14:36:07,459][rgc][INFO] - Batch 105, avg loss per batch: 2.782628448717622
[2025-02-13 14:36:07,460][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 106
[2025-02-13 14:36:28,161][rgc][INFO] - 	Updating weights of batch 106
[2025-02-13 14:36:28,217][rgc][INFO] - Batch 106, avg loss per batch: 3.0306741499590766
[2025-02-13 14:36:28,218][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 107
[2025-02-13 14:36:48,975][rgc][INFO] - 	Updating weights of batch 107
[2025-02-13 14:36:49,040][rgc][INFO] - Batch 107, avg loss per batch: 2.143345808495264
[2025-02-13 14:36:49,041][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 108
[2025-02-13 14:37:09,731][rgc][INFO] - 	Updating weights of batch 108
[2025-02-13 14:37:09,784][rgc][INFO] - Batch 108, avg loss per batch: 3.5623005053827614
[2025-02-13 14:37:09,785][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 109
[2025-02-13 14:37:30,535][rgc][INFO] - 	Updating weights of batch 109
[2025-02-13 14:37:30,586][rgc][INFO] - Batch 109, avg loss per batch: 2.991443725233864
[2025-02-13 14:37:30,587][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 110
[2025-02-13 14:37:51,260][rgc][INFO] - 	Updating weights of batch 110
[2025-02-13 14:37:51,311][rgc][INFO] - Batch 110, avg loss per batch: 2.3207443706272333
[2025-02-13 14:37:51,312][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 111
[2025-02-13 14:38:12,063][rgc][INFO] - 	Updating weights of batch 111
[2025-02-13 14:38:12,116][rgc][INFO] - Batch 111, avg loss per batch: 2.220716225356312
[2025-02-13 14:38:12,117][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 112
[2025-02-13 14:38:32,855][rgc][INFO] - 	Updating weights of batch 112
[2025-02-13 14:38:32,907][rgc][INFO] - Batch 112, avg loss per batch: 2.114267317908916
[2025-02-13 14:38:32,908][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 113
[2025-02-13 14:38:53,640][rgc][INFO] - 	Updating weights of batch 113
[2025-02-13 14:38:53,715][rgc][INFO] - Batch 113, avg loss per batch: 3.024291481688202
[2025-02-13 14:38:53,716][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 114
[2025-02-13 14:39:14,463][rgc][INFO] - 	Updating weights of batch 114
[2025-02-13 14:39:14,512][rgc][INFO] - Batch 114, avg loss per batch: 2.0053187857497696
[2025-02-13 14:39:14,513][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 115
[2025-02-13 14:39:35,262][rgc][INFO] - 	Updating weights of batch 115
[2025-02-13 14:39:35,316][rgc][INFO] - Batch 115, avg loss per batch: 3.841560163124596
[2025-02-13 14:39:35,317][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 116
[2025-02-13 14:39:56,001][rgc][INFO] - 	Updating weights of batch 116
[2025-02-13 14:39:56,051][rgc][INFO] - Batch 116, avg loss per batch: 3.6957315663406356
[2025-02-13 14:39:56,052][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 117
[2025-02-13 14:40:16,781][rgc][INFO] - 	Updating weights of batch 117
[2025-02-13 14:40:16,832][rgc][INFO] - Batch 117, avg loss per batch: 2.629825675592182
[2025-02-13 14:40:16,833][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 118
[2025-02-13 14:40:37,515][rgc][INFO] - 	Updating weights of batch 118
[2025-02-13 14:40:37,570][rgc][INFO] - Batch 118, avg loss per batch: 3.1992935850664592
[2025-02-13 14:40:37,571][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 119
[2025-02-13 14:40:58,264][rgc][INFO] - 	Updating weights of batch 119
[2025-02-13 14:40:58,321][rgc][INFO] - Batch 119, avg loss per batch: 3.3901332831601785
[2025-02-13 14:40:58,322][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 120
[2025-02-13 14:41:19,070][rgc][INFO] - 	Updating weights of batch 120
[2025-02-13 14:41:19,125][rgc][INFO] - Batch 120, avg loss per batch: 3.386778224224099
[2025-02-13 14:41:19,126][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 121
[2025-02-13 14:41:39,864][rgc][INFO] - 	Updating weights of batch 121
[2025-02-13 14:41:39,915][rgc][INFO] - Batch 121, avg loss per batch: 4.8583912052435885
[2025-02-13 14:41:39,915][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 122
[2025-02-13 14:42:00,594][rgc][INFO] - 	Updating weights of batch 122
[2025-02-13 14:42:00,646][rgc][INFO] - Batch 122, avg loss per batch: 1.5455590648168562
[2025-02-13 14:42:00,647][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 123
[2025-02-13 14:42:21,352][rgc][INFO] - 	Updating weights of batch 123
[2025-02-13 14:42:21,402][rgc][INFO] - Batch 123, avg loss per batch: 2.103676115106806
[2025-02-13 14:42:21,403][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 124
[2025-02-13 14:42:42,150][rgc][INFO] - 	Updating weights of batch 124
[2025-02-13 14:42:42,200][rgc][INFO] - Batch 124, avg loss per batch: 2.2181434499258623
[2025-02-13 14:42:42,201][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 125
[2025-02-13 14:43:02,949][rgc][INFO] - 	Updating weights of batch 125
[2025-02-13 14:43:03,000][rgc][INFO] - Batch 125, avg loss per batch: 2.6186740608972796
[2025-02-13 14:43:03,001][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 126
[2025-02-13 14:43:23,740][rgc][INFO] - 	Updating weights of batch 126
[2025-02-13 14:43:23,813][rgc][INFO] - Batch 126, avg loss per batch: 2.471995484041427
[2025-02-13 14:43:23,814][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 127
[2025-02-13 14:43:44,556][rgc][INFO] - 	Updating weights of batch 127
[2025-02-13 14:43:44,612][rgc][INFO] - Batch 127, avg loss per batch: 2.933821822741286
[2025-02-13 14:43:44,613][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 128
[2025-02-13 14:44:05,376][rgc][INFO] - 	Updating weights of batch 128
[2025-02-13 14:44:05,426][rgc][INFO] - Batch 128, avg loss per batch: 2.5664184691883754
[2025-02-13 14:44:05,426][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 129
[2025-02-13 14:44:26,166][rgc][INFO] - 	Updating weights of batch 129
[2025-02-13 14:44:26,220][rgc][INFO] - Batch 129, avg loss per batch: 1.9411820761909375
[2025-02-13 14:44:26,221][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 130
[2025-02-13 14:44:46,913][rgc][INFO] - 	Updating weights of batch 130
[2025-02-13 14:44:46,965][rgc][INFO] - Batch 130, avg loss per batch: 2.4425651885491275
[2025-02-13 14:44:46,966][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 131
[2025-02-13 14:45:07,690][rgc][INFO] - 	Updating weights of batch 131
[2025-02-13 14:45:07,742][rgc][INFO] - Batch 131, avg loss per batch: 2.1130442639229052
[2025-02-13 14:45:07,743][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 132
[2025-02-13 14:45:28,427][rgc][INFO] - 	Updating weights of batch 132
[2025-02-13 14:45:28,490][rgc][INFO] - Batch 132, avg loss per batch: 1.0966033091270104
[2025-02-13 14:45:28,491][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 133
[2025-02-13 14:45:49,248][rgc][INFO] - 	Updating weights of batch 133
[2025-02-13 14:45:49,305][rgc][INFO] - Batch 133, avg loss per batch: 4.814896052192276
[2025-02-13 14:45:49,306][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 134
[2025-02-13 14:46:10,039][rgc][INFO] - 	Updating weights of batch 134
[2025-02-13 14:46:10,094][rgc][INFO] - Batch 134, avg loss per batch: 3.14208658420946
[2025-02-13 14:46:10,095][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 135
[2025-02-13 14:46:30,849][rgc][INFO] - 	Updating weights of batch 135
[2025-02-13 14:46:30,900][rgc][INFO] - Batch 135, avg loss per batch: 2.121499108033343
[2025-02-13 14:46:30,901][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 136
[2025-02-13 14:46:51,662][rgc][INFO] - 	Updating weights of batch 136
[2025-02-13 14:46:51,719][rgc][INFO] - Batch 136, avg loss per batch: 2.0021002031262976
[2025-02-13 14:46:51,719][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 137
[2025-02-13 14:47:12,404][rgc][INFO] - 	Updating weights of batch 137
[2025-02-13 14:47:12,486][rgc][INFO] - Batch 137, avg loss per batch: 3.522003002224899
[2025-02-13 14:47:12,488][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 138
[2025-02-13 14:47:33,162][rgc][INFO] - 	Updating weights of batch 138
[2025-02-13 14:47:33,214][rgc][INFO] - Batch 138, avg loss per batch: 1.9808451494353023
[2025-02-13 14:47:33,215][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 139
[2025-02-13 14:47:53,973][rgc][INFO] - 	Updating weights of batch 139
[2025-02-13 14:47:54,026][rgc][INFO] - Batch 139, avg loss per batch: 1.6244394952390784
[2025-02-13 14:47:54,027][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 140
[2025-02-13 14:48:14,751][rgc][INFO] - 	Updating weights of batch 140
[2025-02-13 14:48:14,835][rgc][INFO] - Batch 140, avg loss per batch: 1.9931047416965826
[2025-02-13 14:48:14,836][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 141
[2025-02-13 14:48:35,515][rgc][INFO] - 	Updating weights of batch 141
[2025-02-13 14:48:35,565][rgc][INFO] - Batch 141, avg loss per batch: 2.8934333722078893
[2025-02-13 14:48:35,566][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 142
[2025-02-13 14:48:56,318][rgc][INFO] - 	Updating weights of batch 142
[2025-02-13 14:48:56,388][rgc][INFO] - Batch 142, avg loss per batch: 3.2184232973654208
[2025-02-13 14:48:56,389][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 143
[2025-02-13 14:49:17,139][rgc][INFO] - 	Updating weights of batch 143
[2025-02-13 14:49:17,191][rgc][INFO] - Batch 143, avg loss per batch: 1.891631386542925
[2025-02-13 14:49:17,192][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 144
[2025-02-13 14:49:37,928][rgc][INFO] - 	Updating weights of batch 144
[2025-02-13 14:49:38,008][rgc][INFO] - Batch 144, avg loss per batch: 3.9465630192132037
[2025-02-13 14:49:38,010][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 145
[2025-02-13 14:49:58,704][rgc][INFO] - 	Updating weights of batch 145
[2025-02-13 14:49:58,762][rgc][INFO] - Batch 145, avg loss per batch: 3.122875834268203
[2025-02-13 14:49:58,763][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 146
[2025-02-13 14:50:19,495][rgc][INFO] - 	Updating weights of batch 146
[2025-02-13 14:50:19,547][rgc][INFO] - Batch 146, avg loss per batch: 3.1406493920868206
[2025-02-13 14:50:19,548][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 147
[2025-02-13 14:50:40,278][rgc][INFO] - 	Updating weights of batch 147
[2025-02-13 14:50:40,331][rgc][INFO] - Batch 147, avg loss per batch: 2.6226104638047296
[2025-02-13 14:50:40,332][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 148
[2025-02-13 14:51:01,103][rgc][INFO] - 	Updating weights of batch 148
[2025-02-13 14:51:01,158][rgc][INFO] - Batch 148, avg loss per batch: 2.6645421139287135
[2025-02-13 14:51:01,159][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 149
[2025-02-13 14:51:21,857][rgc][INFO] - 	Updating weights of batch 149
[2025-02-13 14:51:21,911][rgc][INFO] - Batch 149, avg loss per batch: 2.7519183283725517
[2025-02-13 14:51:21,912][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 150
[2025-02-13 14:51:42,604][rgc][INFO] - 	Updating weights of batch 150
[2025-02-13 14:51:42,670][rgc][INFO] - Batch 150, avg loss per batch: 2.233214677742665
[2025-02-13 14:51:42,671][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 151
[2025-02-13 14:52:03,418][rgc][INFO] - 	Updating weights of batch 151
[2025-02-13 14:52:03,474][rgc][INFO] - Batch 151, avg loss per batch: 2.6144516951090404
[2025-02-13 14:52:03,475][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 152
[2025-02-13 14:52:24,198][rgc][INFO] - 	Updating weights of batch 152
[2025-02-13 14:52:24,250][rgc][INFO] - Batch 152, avg loss per batch: 2.4091590801001406
[2025-02-13 14:52:24,251][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 153
[2025-02-13 14:52:44,933][rgc][INFO] - 	Updating weights of batch 153
[2025-02-13 14:52:44,987][rgc][INFO] - Batch 153, avg loss per batch: 2.6875299988650183
[2025-02-13 14:52:44,989][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 154
[2025-02-13 14:53:05,732][rgc][INFO] - 	Updating weights of batch 154
[2025-02-13 14:53:05,801][rgc][INFO] - Batch 154, avg loss per batch: 2.4831812420165598
[2025-02-13 14:53:05,802][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 155
[2025-02-13 14:53:26,555][rgc][INFO] - 	Updating weights of batch 155
[2025-02-13 14:53:26,604][rgc][INFO] - Batch 155, avg loss per batch: 3.2179302883590175
[2025-02-13 14:53:26,606][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 156
[2025-02-13 14:53:47,350][rgc][INFO] - 	Updating weights of batch 156
[2025-02-13 14:53:47,409][rgc][INFO] - Batch 156, avg loss per batch: 1.84793400022415
[2025-02-13 14:53:47,410][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 157
[2025-02-13 14:54:08,140][rgc][INFO] - 	Updating weights of batch 157
[2025-02-13 14:54:08,208][rgc][INFO] - Batch 157, avg loss per batch: 2.0535944193220264
[2025-02-13 14:54:08,210][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 158
[2025-02-13 14:54:28,973][rgc][INFO] - 	Updating weights of batch 158
[2025-02-13 14:54:29,025][rgc][INFO] - Batch 158, avg loss per batch: 3.6642772433857025
[2025-02-13 14:54:29,026][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 159
[2025-02-13 14:54:49,747][rgc][INFO] - 	Updating weights of batch 159
[2025-02-13 14:54:49,799][rgc][INFO] - Batch 159, avg loss per batch: 4.471289923218442
[2025-02-13 14:54:49,799][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 160
[2025-02-13 14:55:10,479][rgc][INFO] - 	Updating weights of batch 160
[2025-02-13 14:55:10,545][rgc][INFO] - Batch 160, avg loss per batch: 3.347512725682194
[2025-02-13 14:55:10,546][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 161
[2025-02-13 14:55:31,275][rgc][INFO] - 	Updating weights of batch 161
[2025-02-13 14:55:31,324][rgc][INFO] - Batch 161, avg loss per batch: 2.4461773864471943
[2025-02-13 14:55:31,325][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 162
[2025-02-13 14:55:52,057][rgc][INFO] - 	Updating weights of batch 162
[2025-02-13 14:55:52,109][rgc][INFO] - Batch 162, avg loss per batch: 4.3936909736769785
[2025-02-13 14:55:52,110][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 163
[2025-02-13 14:56:12,859][rgc][INFO] - 	Updating weights of batch 163
[2025-02-13 14:56:12,913][rgc][INFO] - Batch 163, avg loss per batch: 1.6089095597898448
[2025-02-13 14:56:12,915][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 164
[2025-02-13 14:56:33,663][rgc][INFO] - 	Updating weights of batch 164
[2025-02-13 14:56:33,725][rgc][INFO] - Batch 164, avg loss per batch: 1.4269667058055115
[2025-02-13 14:56:33,727][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 165
[2025-02-13 14:56:54,416][rgc][INFO] - 	Updating weights of batch 165
[2025-02-13 14:56:54,469][rgc][INFO] - Batch 165, avg loss per batch: 2.532288776566869
[2025-02-13 14:56:54,470][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 166
[2025-02-13 14:57:15,196][rgc][INFO] - 	Updating weights of batch 166
[2025-02-13 14:57:15,259][rgc][INFO] - Batch 166, avg loss per batch: 2.2352983110230014
[2025-02-13 14:57:15,260][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 167
[2025-02-13 14:57:36,029][rgc][INFO] - 	Updating weights of batch 167
[2025-02-13 14:57:36,080][rgc][INFO] - Batch 167, avg loss per batch: 3.13585342823058
[2025-02-13 14:57:36,081][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 168
[2025-02-13 14:57:56,828][rgc][INFO] - 	Updating weights of batch 168
[2025-02-13 14:57:56,883][rgc][INFO] - Batch 168, avg loss per batch: 2.2637158571143616
[2025-02-13 14:57:56,884][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 169
[2025-02-13 14:58:17,551][rgc][INFO] - 	Updating weights of batch 169
[2025-02-13 14:58:17,606][rgc][INFO] - Batch 169, avg loss per batch: 2.366421641633643
[2025-02-13 14:58:17,607][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 170
[2025-02-13 14:58:38,286][rgc][INFO] - 	Updating weights of batch 170
[2025-02-13 14:58:38,338][rgc][INFO] - Batch 170, avg loss per batch: 2.9405996439177233
[2025-02-13 14:58:38,339][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 171
[2025-02-13 14:58:59,083][rgc][INFO] - 	Updating weights of batch 171
[2025-02-13 14:58:59,133][rgc][INFO] - Batch 171, avg loss per batch: 1.7039240505832194
[2025-02-13 14:58:59,134][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 172
[2025-02-13 14:59:19,870][rgc][INFO] - 	Updating weights of batch 172
[2025-02-13 14:59:19,923][rgc][INFO] - Batch 172, avg loss per batch: 2.4158267817031986
[2025-02-13 14:59:19,924][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 173
[2025-02-13 14:59:40,668][rgc][INFO] - 	Updating weights of batch 173
[2025-02-13 14:59:40,724][rgc][INFO] - Batch 173, avg loss per batch: 5.202811809401973
[2025-02-13 14:59:40,726][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 174
[2025-02-13 15:00:01,473][rgc][INFO] - 	Updating weights of batch 174
[2025-02-13 15:00:01,532][rgc][INFO] - Batch 174, avg loss per batch: 2.833789485231686
[2025-02-13 15:00:01,533][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 175
[2025-02-13 15:00:22,220][rgc][INFO] - 	Updating weights of batch 175
[2025-02-13 15:00:22,276][rgc][INFO] - Batch 175, avg loss per batch: 1.6079043504994674
[2025-02-13 15:00:22,277][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 176
[2025-02-13 15:00:43,007][rgc][INFO] - 	Updating weights of batch 176
[2025-02-13 15:00:43,062][rgc][INFO] - Batch 176, avg loss per batch: 2.585020504550836
[2025-02-13 15:00:43,063][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 177
[2025-02-13 15:01:03,754][rgc][INFO] - 	Updating weights of batch 177
[2025-02-13 15:01:03,809][rgc][INFO] - Batch 177, avg loss per batch: 1.6588220280834973
[2025-02-13 15:01:03,810][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 178
[2025-02-13 15:01:24,553][rgc][INFO] - 	Updating weights of batch 178
[2025-02-13 15:01:24,606][rgc][INFO] - Batch 178, avg loss per batch: 3.7028545204636005
[2025-02-13 15:01:24,607][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 179
[2025-02-13 15:01:45,346][rgc][INFO] - 	Updating weights of batch 179
[2025-02-13 15:01:45,394][rgc][INFO] - Batch 179, avg loss per batch: 2.825337502318459
[2025-02-13 15:01:45,396][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 180
[2025-02-13 15:02:06,083][rgc][INFO] - 	Updating weights of batch 180
[2025-02-13 15:02:06,133][rgc][INFO] - Batch 180, avg loss per batch: 2.3518191764018925
[2025-02-13 15:02:06,134][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 181
[2025-02-13 15:02:26,821][rgc][INFO] - 	Updating weights of batch 181
[2025-02-13 15:02:26,875][rgc][INFO] - Batch 181, avg loss per batch: 3.235642650692932
[2025-02-13 15:02:26,876][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 182
[2025-02-13 15:02:47,578][rgc][INFO] - 	Updating weights of batch 182
[2025-02-13 15:02:47,630][rgc][INFO] - Batch 182, avg loss per batch: 3.0321731654448465
[2025-02-13 15:02:47,630][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 183
[2025-02-13 15:03:08,359][rgc][INFO] - 	Updating weights of batch 183
[2025-02-13 15:03:08,414][rgc][INFO] - Batch 183, avg loss per batch: 2.9627658839525512
[2025-02-13 15:03:08,415][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 184
[2025-02-13 15:03:29,158][rgc][INFO] - 	Updating weights of batch 184
[2025-02-13 15:03:29,211][rgc][INFO] - Batch 184, avg loss per batch: 3.04360400254131
[2025-02-13 15:03:29,212][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 185
[2025-02-13 15:03:49,906][rgc][INFO] - 	Updating weights of batch 185
[2025-02-13 15:03:49,959][rgc][INFO] - Batch 185, avg loss per batch: 2.9479761257515893
[2025-02-13 15:03:49,960][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 186
[2025-02-13 15:04:10,662][rgc][INFO] - 	Updating weights of batch 186
[2025-02-13 15:04:10,719][rgc][INFO] - Batch 186, avg loss per batch: 2.3801244961706205
[2025-02-13 15:04:10,720][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 187
[2025-02-13 15:04:31,500][rgc][INFO] - 	Updating weights of batch 187
[2025-02-13 15:04:31,552][rgc][INFO] - Batch 187, avg loss per batch: 1.7208754718368942
[2025-02-13 15:04:31,553][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 188
[2025-02-13 15:04:52,318][rgc][INFO] - 	Updating weights of batch 188
[2025-02-13 15:04:52,371][rgc][INFO] - Batch 188, avg loss per batch: 2.227732578981215
[2025-02-13 15:04:52,371][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 189
[2025-02-13 15:05:13,055][rgc][INFO] - 	Updating weights of batch 189
[2025-02-13 15:05:13,110][rgc][INFO] - Batch 189, avg loss per batch: 3.2698935393899804
[2025-02-13 15:05:13,112][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 190
[2025-02-13 15:05:33,869][rgc][INFO] - 	Updating weights of batch 190
[2025-02-13 15:05:33,920][rgc][INFO] - Batch 190, avg loss per batch: 2.624657682543582
[2025-02-13 15:05:33,921][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 191
[2025-02-13 15:05:54,604][rgc][INFO] - 	Updating weights of batch 191
[2025-02-13 15:05:54,660][rgc][INFO] - Batch 191, avg loss per batch: 1.7417534492926925
[2025-02-13 15:05:54,661][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 192
[2025-02-13 15:06:15,337][rgc][INFO] - 	Updating weights of batch 192
[2025-02-13 15:06:15,390][rgc][INFO] - Batch 192, avg loss per batch: 2.5241716367589193
[2025-02-13 15:06:15,390][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 193
[2025-02-13 15:06:36,108][rgc][INFO] - 	Updating weights of batch 193
[2025-02-13 15:06:36,190][rgc][INFO] - Batch 193, avg loss per batch: 3.331042046814475
[2025-02-13 15:06:36,191][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 194
[2025-02-13 15:06:56,927][rgc][INFO] - 	Updating weights of batch 194
[2025-02-13 15:06:56,975][rgc][INFO] - Batch 194, avg loss per batch: 1.6878009265559333
[2025-02-13 15:06:56,975][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 195
[2025-02-13 15:07:17,716][rgc][INFO] - 	Updating weights of batch 195
[2025-02-13 15:07:17,767][rgc][INFO] - Batch 195, avg loss per batch: 2.5545901992794806
[2025-02-13 15:07:17,768][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 196
[2025-02-13 15:07:38,453][rgc][INFO] - 	Updating weights of batch 196
[2025-02-13 15:07:38,506][rgc][INFO] - Batch 196, avg loss per batch: 3.6989525056388137
[2025-02-13 15:07:38,507][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 197
[2025-02-13 15:07:59,250][rgc][INFO] - 	Updating weights of batch 197
[2025-02-13 15:07:59,300][rgc][INFO] - Batch 197, avg loss per batch: 1.7997603990577613
[2025-02-13 15:07:59,301][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 198
[2025-02-13 15:08:20,069][rgc][INFO] - 	Updating weights of batch 198
[2025-02-13 15:08:20,123][rgc][INFO] - Batch 198, avg loss per batch: 1.9865024358832417
[2025-02-13 15:08:20,123][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 199
[2025-02-13 15:08:40,853][rgc][INFO] - 	Updating weights of batch 199
[2025-02-13 15:08:40,903][rgc][INFO] - Batch 199, avg loss per batch: 0.6495145031216274
[2025-02-13 15:08:52,438][rgc][INFO] - AVG rho on val data: 0.25492174264585765
[2025-02-13 15:08:52,439][rgc][INFO] - AVG mae on val data: 0.5311764296894301
[2025-02-13 15:09:03,626][rgc][INFO] - AVG rho on test data: 0.31336071033448837
[2025-02-13 15:09:03,626][rgc][INFO] - AVG mae on test data: 0.5584892740595724
[2025-02-13 15:09:16,466][rgc][INFO] - AVG rho on train data: 0.21508652433523917
[2025-02-13 15:09:16,467][rgc][INFO] - AVG mae on train data: 0.5509156106230522
[2025-02-13 15:09:16,467][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 15:09:16,470][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 200
[2025-02-13 15:09:37,155][rgc][INFO] - 	Updating weights of batch 200
[2025-02-13 15:09:37,207][rgc][INFO] - Batch 200, avg loss per batch: 2.1534806962828377
[2025-02-13 15:09:37,208][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 201
[2025-02-13 15:09:57,930][rgc][INFO] - 	Updating weights of batch 201
[2025-02-13 15:09:57,993][rgc][INFO] - Batch 201, avg loss per batch: 2.822499555703791
[2025-02-13 15:09:57,994][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 202
[2025-02-13 15:10:18,737][rgc][INFO] - 	Updating weights of batch 202
[2025-02-13 15:10:18,796][rgc][INFO] - Batch 202, avg loss per batch: 2.963617598601659
[2025-02-13 15:10:18,797][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 203
[2025-02-13 15:10:39,480][rgc][INFO] - 	Updating weights of batch 203
[2025-02-13 15:10:39,533][rgc][INFO] - Batch 203, avg loss per batch: 3.7540658964935587
[2025-02-13 15:10:39,534][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 204
[2025-02-13 15:11:00,218][rgc][INFO] - 	Updating weights of batch 204
[2025-02-13 15:11:00,268][rgc][INFO] - Batch 204, avg loss per batch: 1.4344746843479932
[2025-02-13 15:11:00,269][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 205
[2025-02-13 15:11:20,960][rgc][INFO] - 	Updating weights of batch 205
[2025-02-13 15:11:21,010][rgc][INFO] - Batch 205, avg loss per batch: 2.7689002480887925
[2025-02-13 15:11:21,011][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 206
[2025-02-13 15:11:41,696][rgc][INFO] - 	Updating weights of batch 206
[2025-02-13 15:11:41,752][rgc][INFO] - Batch 206, avg loss per batch: 3.8707148730039083
[2025-02-13 15:11:41,753][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 207
[2025-02-13 15:12:02,445][rgc][INFO] - 	Updating weights of batch 207
[2025-02-13 15:12:02,499][rgc][INFO] - Batch 207, avg loss per batch: 1.968970315913898
[2025-02-13 15:12:02,500][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 208
[2025-02-13 15:12:23,169][rgc][INFO] - 	Updating weights of batch 208
[2025-02-13 15:12:23,225][rgc][INFO] - Batch 208, avg loss per batch: 2.5350294390270656
[2025-02-13 15:12:23,227][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 209
[2025-02-13 15:12:43,924][rgc][INFO] - 	Updating weights of batch 209
[2025-02-13 15:12:43,975][rgc][INFO] - Batch 209, avg loss per batch: 3.3762676843994903
[2025-02-13 15:12:43,976][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 210
[2025-02-13 15:13:04,666][rgc][INFO] - 	Updating weights of batch 210
[2025-02-13 15:13:04,717][rgc][INFO] - Batch 210, avg loss per batch: 3.0372178153209615
[2025-02-13 15:13:04,719][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 211
[2025-02-13 15:13:25,458][rgc][INFO] - 	Updating weights of batch 211
[2025-02-13 15:13:25,510][rgc][INFO] - Batch 211, avg loss per batch: 2.682442746706193
[2025-02-13 15:13:25,510][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 212
[2025-02-13 15:13:46,260][rgc][INFO] - 	Updating weights of batch 212
[2025-02-13 15:13:46,312][rgc][INFO] - Batch 212, avg loss per batch: 2.7167890156236774
[2025-02-13 15:13:46,312][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 213
[2025-02-13 15:14:07,003][rgc][INFO] - 	Updating weights of batch 213
[2025-02-13 15:14:07,060][rgc][INFO] - Batch 213, avg loss per batch: 2.3234399695916883
[2025-02-13 15:14:07,061][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 214
[2025-02-13 15:14:27,810][rgc][INFO] - 	Updating weights of batch 214
[2025-02-13 15:14:27,863][rgc][INFO] - Batch 214, avg loss per batch: 1.85828104592319
[2025-02-13 15:14:27,864][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 215
[2025-02-13 15:14:48,610][rgc][INFO] - 	Updating weights of batch 215
[2025-02-13 15:14:48,664][rgc][INFO] - Batch 215, avg loss per batch: 3.0917798562782797
[2025-02-13 15:14:48,665][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 216
[2025-02-13 15:15:09,351][rgc][INFO] - 	Updating weights of batch 216
[2025-02-13 15:15:09,417][rgc][INFO] - Batch 216, avg loss per batch: 2.531511792544849
[2025-02-13 15:15:09,418][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 217
[2025-02-13 15:15:30,160][rgc][INFO] - 	Updating weights of batch 217
[2025-02-13 15:15:30,212][rgc][INFO] - Batch 217, avg loss per batch: 5.202354955975938
[2025-02-13 15:15:30,213][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 218
[2025-02-13 15:15:50,958][rgc][INFO] - 	Updating weights of batch 218
[2025-02-13 15:15:51,010][rgc][INFO] - Batch 218, avg loss per batch: 3.005411400249119
[2025-02-13 15:15:51,011][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 219
[2025-02-13 15:16:11,753][rgc][INFO] - 	Updating weights of batch 219
[2025-02-13 15:16:11,806][rgc][INFO] - Batch 219, avg loss per batch: 3.3720861198156644
[2025-02-13 15:16:11,807][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 220
[2025-02-13 15:16:32,545][rgc][INFO] - 	Updating weights of batch 220
[2025-02-13 15:16:32,596][rgc][INFO] - Batch 220, avg loss per batch: 1.37965376544273
[2025-02-13 15:16:32,597][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 221
[2025-02-13 15:16:53,340][rgc][INFO] - 	Updating weights of batch 221
[2025-02-13 15:16:53,392][rgc][INFO] - Batch 221, avg loss per batch: 2.231284166567683
[2025-02-13 15:16:53,393][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 222
[2025-02-13 15:17:14,054][rgc][INFO] - 	Updating weights of batch 222
[2025-02-13 15:17:14,104][rgc][INFO] - Batch 222, avg loss per batch: 1.6235038759676779
[2025-02-13 15:17:14,104][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 223
[2025-02-13 15:17:34,786][rgc][INFO] - 	Updating weights of batch 223
[2025-02-13 15:17:34,837][rgc][INFO] - Batch 223, avg loss per batch: 1.7669412840753793
[2025-02-13 15:17:34,838][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 224
[2025-02-13 15:17:55,572][rgc][INFO] - 	Updating weights of batch 224
[2025-02-13 15:17:55,624][rgc][INFO] - Batch 224, avg loss per batch: 5.3495841558827495
[2025-02-13 15:17:55,626][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 225
[2025-02-13 15:18:16,364][rgc][INFO] - 	Updating weights of batch 225
[2025-02-13 15:18:16,418][rgc][INFO] - Batch 225, avg loss per batch: 2.677245362581197
[2025-02-13 15:18:16,419][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 226
[2025-02-13 15:18:37,166][rgc][INFO] - 	Updating weights of batch 226
[2025-02-13 15:18:37,217][rgc][INFO] - Batch 226, avg loss per batch: 3.0863571334102287
[2025-02-13 15:18:37,217][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 227
[2025-02-13 15:18:57,962][rgc][INFO] - 	Updating weights of batch 227
[2025-02-13 15:18:58,015][rgc][INFO] - Batch 227, avg loss per batch: 3.8317623560709615
[2025-02-13 15:18:58,016][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 228
[2025-02-13 15:19:18,737][rgc][INFO] - 	Updating weights of batch 228
[2025-02-13 15:19:18,790][rgc][INFO] - Batch 228, avg loss per batch: 2.670870488980361
[2025-02-13 15:19:18,792][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 229
[2025-02-13 15:19:39,458][rgc][INFO] - 	Updating weights of batch 229
[2025-02-13 15:19:39,519][rgc][INFO] - Batch 229, avg loss per batch: 1.778575536701269
[2025-02-13 15:19:39,519][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 230
[2025-02-13 15:20:00,189][rgc][INFO] - 	Updating weights of batch 230
[2025-02-13 15:20:00,241][rgc][INFO] - Batch 230, avg loss per batch: 2.402832587358386
[2025-02-13 15:20:00,241][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 231
[2025-02-13 15:20:20,896][rgc][INFO] - 	Updating weights of batch 231
[2025-02-13 15:20:20,946][rgc][INFO] - Batch 231, avg loss per batch: 3.1482234315842454
[2025-02-13 15:20:20,947][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 232
[2025-02-13 15:20:41,639][rgc][INFO] - 	Updating weights of batch 232
[2025-02-13 15:20:41,694][rgc][INFO] - Batch 232, avg loss per batch: 2.6068136999491536
[2025-02-13 15:20:41,696][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 233
[2025-02-13 15:21:02,449][rgc][INFO] - 	Updating weights of batch 233
[2025-02-13 15:21:02,506][rgc][INFO] - Batch 233, avg loss per batch: 2.8527846120198133
[2025-02-13 15:21:02,507][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 234
[2025-02-13 15:21:23,177][rgc][INFO] - 	Updating weights of batch 234
[2025-02-13 15:21:23,233][rgc][INFO] - Batch 234, avg loss per batch: 2.399995487110659
[2025-02-13 15:21:23,233][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 235
[2025-02-13 15:21:43,979][rgc][INFO] - 	Updating weights of batch 235
[2025-02-13 15:21:44,055][rgc][INFO] - Batch 235, avg loss per batch: 3.1274984953536586
[2025-02-13 15:21:44,057][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 236
[2025-02-13 15:22:04,806][rgc][INFO] - 	Updating weights of batch 236
[2025-02-13 15:22:04,866][rgc][INFO] - Batch 236, avg loss per batch: 2.364356655052549
[2025-02-13 15:22:04,867][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 237
[2025-02-13 15:22:25,620][rgc][INFO] - 	Updating weights of batch 237
[2025-02-13 15:22:25,671][rgc][INFO] - Batch 237, avg loss per batch: 2.939385985590435
[2025-02-13 15:22:25,671][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 238
[2025-02-13 15:22:46,349][rgc][INFO] - 	Updating weights of batch 238
[2025-02-13 15:22:46,433][rgc][INFO] - Batch 238, avg loss per batch: 4.218137118648538
[2025-02-13 15:22:46,435][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 239
[2025-02-13 15:23:07,184][rgc][INFO] - 	Updating weights of batch 239
[2025-02-13 15:23:07,238][rgc][INFO] - Batch 239, avg loss per batch: 2.663253916952262
[2025-02-13 15:23:07,239][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 240
[2025-02-13 15:23:27,958][rgc][INFO] - 	Updating weights of batch 240
[2025-02-13 15:23:28,017][rgc][INFO] - Batch 240, avg loss per batch: 2.4012717378088895
[2025-02-13 15:23:28,019][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 241
[2025-02-13 15:23:48,722][rgc][INFO] - 	Updating weights of batch 241
[2025-02-13 15:23:48,779][rgc][INFO] - Batch 241, avg loss per batch: 1.8713970470524595
[2025-02-13 15:23:48,780][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 242
[2025-02-13 15:24:09,453][rgc][INFO] - 	Updating weights of batch 242
[2025-02-13 15:24:09,507][rgc][INFO] - Batch 242, avg loss per batch: 1.755850014326197
[2025-02-13 15:24:09,508][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 243
[2025-02-13 15:24:30,270][rgc][INFO] - 	Updating weights of batch 243
[2025-02-13 15:24:30,331][rgc][INFO] - Batch 243, avg loss per batch: 3.1679411215519924
[2025-02-13 15:24:30,332][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 244
[2025-02-13 15:24:51,023][rgc][INFO] - 	Updating weights of batch 244
[2025-02-13 15:24:51,088][rgc][INFO] - Batch 244, avg loss per batch: 2.787644178298667
[2025-02-13 15:24:51,090][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 245
[2025-02-13 15:25:11,813][rgc][INFO] - 	Updating weights of batch 245
[2025-02-13 15:25:11,891][rgc][INFO] - Batch 245, avg loss per batch: 1.6698341125503215
[2025-02-13 15:25:11,892][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 246
[2025-02-13 15:25:32,614][rgc][INFO] - 	Updating weights of batch 246
[2025-02-13 15:25:32,668][rgc][INFO] - Batch 246, avg loss per batch: 1.9450311049329532
[2025-02-13 15:25:32,670][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 247
[2025-02-13 15:25:53,406][rgc][INFO] - 	Updating weights of batch 247
[2025-02-13 15:25:53,459][rgc][INFO] - Batch 247, avg loss per batch: 2.3377904005726533
[2025-02-13 15:25:53,459][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 248
[2025-02-13 15:26:14,181][rgc][INFO] - 	Updating weights of batch 248
[2025-02-13 15:26:14,232][rgc][INFO] - Batch 248, avg loss per batch: 3.7653516077495226
[2025-02-13 15:26:14,233][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 249
[2025-02-13 15:26:34,981][rgc][INFO] - 	Updating weights of batch 249
[2025-02-13 15:26:35,031][rgc][INFO] - Batch 249, avg loss per batch: 2.6085837323136882
[2025-02-13 15:26:35,032][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 250
[2025-02-13 15:26:55,772][rgc][INFO] - 	Updating weights of batch 250
[2025-02-13 15:26:55,823][rgc][INFO] - Batch 250, avg loss per batch: 4.197077227780185
[2025-02-13 15:26:55,824][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 251
[2025-02-13 15:27:16,483][rgc][INFO] - 	Updating weights of batch 251
[2025-02-13 15:27:16,534][rgc][INFO] - Batch 251, avg loss per batch: 2.469951589827946
[2025-02-13 15:27:16,535][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 252
[2025-02-13 15:27:37,201][rgc][INFO] - 	Updating weights of batch 252
[2025-02-13 15:27:37,252][rgc][INFO] - Batch 252, avg loss per batch: 4.323971141523992
[2025-02-13 15:27:37,253][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 253
[2025-02-13 15:27:57,917][rgc][INFO] - 	Updating weights of batch 253
[2025-02-13 15:27:57,980][rgc][INFO] - Batch 253, avg loss per batch: 2.2504761671150884
[2025-02-13 15:27:57,980][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 254
[2025-02-13 15:28:18,731][rgc][INFO] - 	Updating weights of batch 254
[2025-02-13 15:28:18,784][rgc][INFO] - Batch 254, avg loss per batch: 2.757555212725935
[2025-02-13 15:28:18,785][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 255
[2025-02-13 15:28:39,468][rgc][INFO] - 	Updating weights of batch 255
[2025-02-13 15:28:39,525][rgc][INFO] - Batch 255, avg loss per batch: 1.806132778067463
[2025-02-13 15:28:39,526][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 256
[2025-02-13 15:29:00,199][rgc][INFO] - 	Updating weights of batch 256
[2025-02-13 15:29:00,254][rgc][INFO] - Batch 256, avg loss per batch: 2.6651371489052345
[2025-02-13 15:29:00,255][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 257
[2025-02-13 15:29:21,007][rgc][INFO] - 	Updating weights of batch 257
[2025-02-13 15:29:21,059][rgc][INFO] - Batch 257, avg loss per batch: 4.326028852950128
[2025-02-13 15:29:21,059][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 258
[2025-02-13 15:29:41,751][rgc][INFO] - 	Updating weights of batch 258
[2025-02-13 15:29:41,805][rgc][INFO] - Batch 258, avg loss per batch: 2.3164391906974813
[2025-02-13 15:29:41,806][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 259
[2025-02-13 15:30:02,469][rgc][INFO] - 	Updating weights of batch 259
[2025-02-13 15:30:02,521][rgc][INFO] - Batch 259, avg loss per batch: 2.3867353033762715
[2025-02-13 15:30:02,522][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 260
[2025-02-13 15:30:23,195][rgc][INFO] - 	Updating weights of batch 260
[2025-02-13 15:30:23,244][rgc][INFO] - Batch 260, avg loss per batch: 2.4283297484949564
[2025-02-13 15:30:23,245][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 261
[2025-02-13 15:30:43,988][rgc][INFO] - 	Updating weights of batch 261
[2025-02-13 15:30:44,056][rgc][INFO] - Batch 261, avg loss per batch: 2.2565713218226815
[2025-02-13 15:30:44,056][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 262
[2025-02-13 15:31:04,742][rgc][INFO] - 	Updating weights of batch 262
[2025-02-13 15:31:04,793][rgc][INFO] - Batch 262, avg loss per batch: 2.4701980937898287
[2025-02-13 15:31:04,794][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 263
[2025-02-13 15:31:25,477][rgc][INFO] - 	Updating weights of batch 263
[2025-02-13 15:31:25,528][rgc][INFO] - Batch 263, avg loss per batch: 1.6416084807812965
[2025-02-13 15:31:25,529][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 264
[2025-02-13 15:31:46,276][rgc][INFO] - 	Updating weights of batch 264
[2025-02-13 15:31:46,329][rgc][INFO] - Batch 264, avg loss per batch: 4.9538364688115815
[2025-02-13 15:31:46,329][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 265
[2025-02-13 15:32:07,028][rgc][INFO] - 	Updating weights of batch 265
[2025-02-13 15:32:07,083][rgc][INFO] - Batch 265, avg loss per batch: 2.384120194330774
[2025-02-13 15:32:07,083][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 266
[2025-02-13 15:32:27,766][rgc][INFO] - 	Updating weights of batch 266
[2025-02-13 15:32:27,818][rgc][INFO] - Batch 266, avg loss per batch: 1.660506908168449
[2025-02-13 15:32:27,819][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 267
[2025-02-13 15:32:48,566][rgc][INFO] - 	Updating weights of batch 267
[2025-02-13 15:32:48,620][rgc][INFO] - Batch 267, avg loss per batch: 3.535623264848494
[2025-02-13 15:32:48,621][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 268
[2025-02-13 15:33:09,364][rgc][INFO] - 	Updating weights of batch 268
[2025-02-13 15:33:09,415][rgc][INFO] - Batch 268, avg loss per batch: 3.985852252585305
[2025-02-13 15:33:09,415][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 269
[2025-02-13 15:33:30,161][rgc][INFO] - 	Updating weights of batch 269
[2025-02-13 15:33:30,215][rgc][INFO] - Batch 269, avg loss per batch: 2.608857242210712
[2025-02-13 15:33:30,215][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 270
[2025-02-13 15:33:50,883][rgc][INFO] - 	Updating weights of batch 270
[2025-02-13 15:33:50,937][rgc][INFO] - Batch 270, avg loss per batch: 1.665342433955005
[2025-02-13 15:33:50,938][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 271
[2025-02-13 15:34:11,673][rgc][INFO] - 	Updating weights of batch 271
[2025-02-13 15:34:11,726][rgc][INFO] - Batch 271, avg loss per batch: 3.314407563150401
[2025-02-13 15:34:11,727][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 272
[2025-02-13 15:34:32,493][rgc][INFO] - 	Updating weights of batch 272
[2025-02-13 15:34:32,544][rgc][INFO] - Batch 272, avg loss per batch: 2.188863840053512
[2025-02-13 15:34:32,545][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 273
[2025-02-13 15:34:53,206][rgc][INFO] - 	Updating weights of batch 273
[2025-02-13 15:34:53,257][rgc][INFO] - Batch 273, avg loss per batch: 3.091007556114161
[2025-02-13 15:34:53,257][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 274
[2025-02-13 15:35:13,993][rgc][INFO] - 	Updating weights of batch 274
[2025-02-13 15:35:14,043][rgc][INFO] - Batch 274, avg loss per batch: 2.1603394706288865
[2025-02-13 15:35:14,044][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 275
[2025-02-13 15:35:34,724][rgc][INFO] - 	Updating weights of batch 275
[2025-02-13 15:35:34,793][rgc][INFO] - Batch 275, avg loss per batch: 2.0350491520142566
[2025-02-13 15:35:34,794][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 276
[2025-02-13 15:35:55,467][rgc][INFO] - 	Updating weights of batch 276
[2025-02-13 15:35:55,519][rgc][INFO] - Batch 276, avg loss per batch: 2.270862041377644
[2025-02-13 15:35:55,520][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 277
[2025-02-13 15:36:16,215][rgc][INFO] - 	Updating weights of batch 277
[2025-02-13 15:36:16,267][rgc][INFO] - Batch 277, avg loss per batch: 3.2554310096702017
[2025-02-13 15:36:16,268][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 278
[2025-02-13 15:36:37,041][rgc][INFO] - 	Updating weights of batch 278
[2025-02-13 15:36:37,097][rgc][INFO] - Batch 278, avg loss per batch: 1.6590930985279968
[2025-02-13 15:36:37,098][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 279
[2025-02-13 15:36:57,842][rgc][INFO] - 	Updating weights of batch 279
[2025-02-13 15:36:57,894][rgc][INFO] - Batch 279, avg loss per batch: 2.7111064232611684
[2025-02-13 15:36:57,895][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 280
[2025-02-13 15:37:18,644][rgc][INFO] - 	Updating weights of batch 280
[2025-02-13 15:37:18,728][rgc][INFO] - Batch 280, avg loss per batch: 2.9316774475560585
[2025-02-13 15:37:18,730][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 281
[2025-02-13 15:37:39,477][rgc][INFO] - 	Updating weights of batch 281
[2025-02-13 15:37:39,527][rgc][INFO] - Batch 281, avg loss per batch: 2.6642828839702375
[2025-02-13 15:37:39,528][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 282
[2025-02-13 15:38:00,276][rgc][INFO] - 	Updating weights of batch 282
[2025-02-13 15:38:00,325][rgc][INFO] - Batch 282, avg loss per batch: 4.651110264687585
[2025-02-13 15:38:00,326][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 283
[2025-02-13 15:38:21,060][rgc][INFO] - 	Updating weights of batch 283
[2025-02-13 15:38:21,116][rgc][INFO] - Batch 283, avg loss per batch: 2.378462563440153
[2025-02-13 15:38:21,117][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 284
[2025-02-13 15:38:41,795][rgc][INFO] - 	Updating weights of batch 284
[2025-02-13 15:38:41,843][rgc][INFO] - Batch 284, avg loss per batch: 7.101287047186872
[2025-02-13 15:38:41,844][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 285
[2025-02-13 15:39:02,591][rgc][INFO] - 	Updating weights of batch 285
[2025-02-13 15:39:02,642][rgc][INFO] - Batch 285, avg loss per batch: 2.123010309882848
[2025-02-13 15:39:02,643][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 286
[2025-02-13 15:39:23,395][rgc][INFO] - 	Updating weights of batch 286
[2025-02-13 15:39:23,448][rgc][INFO] - Batch 286, avg loss per batch: 2.8381265113525784
[2025-02-13 15:39:23,448][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 287
[2025-02-13 15:39:44,210][rgc][INFO] - 	Updating weights of batch 287
[2025-02-13 15:39:44,260][rgc][INFO] - Batch 287, avg loss per batch: 2.8232369486592392
[2025-02-13 15:39:44,260][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 288
[2025-02-13 15:40:04,979][rgc][INFO] - 	Updating weights of batch 288
[2025-02-13 15:40:05,028][rgc][INFO] - Batch 288, avg loss per batch: 3.273138663940663
[2025-02-13 15:40:05,029][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 289
[2025-02-13 15:40:25,784][rgc][INFO] - 	Updating weights of batch 289
[2025-02-13 15:40:25,835][rgc][INFO] - Batch 289, avg loss per batch: 2.469315162485053
[2025-02-13 15:40:25,836][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 290
[2025-02-13 15:40:46,564][rgc][INFO] - 	Updating weights of batch 290
[2025-02-13 15:40:46,616][rgc][INFO] - Batch 290, avg loss per batch: 3.6250665680590015
[2025-02-13 15:40:46,617][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 291
[2025-02-13 15:41:07,357][rgc][INFO] - 	Updating weights of batch 291
[2025-02-13 15:41:07,415][rgc][INFO] - Batch 291, avg loss per batch: 2.63243962950931
[2025-02-13 15:41:07,416][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 292
[2025-02-13 15:41:28,159][rgc][INFO] - 	Updating weights of batch 292
[2025-02-13 15:41:28,216][rgc][INFO] - Batch 292, avg loss per batch: 2.5457805944592726
[2025-02-13 15:41:28,217][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 293
[2025-02-13 15:41:48,988][rgc][INFO] - 	Updating weights of batch 293
[2025-02-13 15:41:49,043][rgc][INFO] - Batch 293, avg loss per batch: 2.451818383275889
[2025-02-13 15:41:49,045][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 294
[2025-02-13 15:42:09,756][rgc][INFO] - 	Updating weights of batch 294
[2025-02-13 15:42:09,807][rgc][INFO] - Batch 294, avg loss per batch: 2.1271509111670754
[2025-02-13 15:42:09,808][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 295
[2025-02-13 15:42:30,509][rgc][INFO] - 	Updating weights of batch 295
[2025-02-13 15:42:30,560][rgc][INFO] - Batch 295, avg loss per batch: 2.4965326993313375
[2025-02-13 15:42:30,562][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 296
[2025-02-13 15:42:51,324][rgc][INFO] - 	Updating weights of batch 296
[2025-02-13 15:42:51,378][rgc][INFO] - Batch 296, avg loss per batch: 1.4159298523325872
[2025-02-13 15:42:51,379][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 297
[2025-02-13 15:43:12,113][rgc][INFO] - 	Updating weights of batch 297
[2025-02-13 15:43:12,165][rgc][INFO] - Batch 297, avg loss per batch: 4.362562158686504
[2025-02-13 15:43:12,165][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 298
[2025-02-13 15:43:32,918][rgc][INFO] - 	Updating weights of batch 298
[2025-02-13 15:43:32,970][rgc][INFO] - Batch 298, avg loss per batch: 1.962125336626373
[2025-02-13 15:43:32,971][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 299
[2025-02-13 15:43:53,751][rgc][INFO] - 	Updating weights of batch 299
[2025-02-13 15:43:53,820][rgc][INFO] - Batch 299, avg loss per batch: 1.8854309322874416
[2025-02-13 15:44:05,385][rgc][INFO] - AVG rho on val data: 0.272577753287904
[2025-02-13 15:44:05,385][rgc][INFO] - AVG mae on val data: 0.5263114312869269
[2025-02-13 15:44:16,558][rgc][INFO] - AVG rho on test data: 0.3105708379570947
[2025-02-13 15:44:16,558][rgc][INFO] - AVG mae on test data: 0.5620109996310964
[2025-02-13 15:44:29,398][rgc][INFO] - AVG rho on train data: 0.22406024378926478
[2025-02-13 15:44:29,398][rgc][INFO] - AVG mae on train data: 0.546389174517973
[2025-02-13 15:44:29,399][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 15:44:29,400][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 300
[2025-02-13 15:44:50,145][rgc][INFO] - 	Updating weights of batch 300
[2025-02-13 15:44:50,198][rgc][INFO] - Batch 300, avg loss per batch: 3.1041543901233304
[2025-02-13 15:44:50,200][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 301
[2025-02-13 15:45:10,857][rgc][INFO] - 	Updating weights of batch 301
[2025-02-13 15:45:10,911][rgc][INFO] - Batch 301, avg loss per batch: 2.593221251735507
[2025-02-13 15:45:10,912][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 302
[2025-02-13 15:45:31,645][rgc][INFO] - 	Updating weights of batch 302
[2025-02-13 15:45:31,706][rgc][INFO] - Batch 302, avg loss per batch: 2.270089269331516
[2025-02-13 15:45:31,708][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 303
[2025-02-13 15:45:52,388][rgc][INFO] - 	Updating weights of batch 303
[2025-02-13 15:45:52,442][rgc][INFO] - Batch 303, avg loss per batch: 2.117228861008665
[2025-02-13 15:45:52,443][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 304
[2025-02-13 15:46:13,200][rgc][INFO] - 	Updating weights of batch 304
[2025-02-13 15:46:13,279][rgc][INFO] - Batch 304, avg loss per batch: 3.258614497107318
[2025-02-13 15:46:13,281][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 305
[2025-02-13 15:46:34,013][rgc][INFO] - 	Updating weights of batch 305
[2025-02-13 15:46:34,065][rgc][INFO] - Batch 305, avg loss per batch: 2.1694070917019785
[2025-02-13 15:46:34,066][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 306
[2025-02-13 15:46:54,755][rgc][INFO] - 	Updating weights of batch 306
[2025-02-13 15:46:54,808][rgc][INFO] - Batch 306, avg loss per batch: 1.154006421429611
[2025-02-13 15:46:54,809][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 307
[2025-02-13 15:47:15,468][rgc][INFO] - 	Updating weights of batch 307
[2025-02-13 15:47:15,521][rgc][INFO] - Batch 307, avg loss per batch: 3.1171936554710693
[2025-02-13 15:47:15,522][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 308
[2025-02-13 15:47:36,297][rgc][INFO] - 	Updating weights of batch 308
[2025-02-13 15:47:36,374][rgc][INFO] - Batch 308, avg loss per batch: 2.0776385970202065
[2025-02-13 15:47:36,375][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 309
[2025-02-13 15:47:57,141][rgc][INFO] - 	Updating weights of batch 309
[2025-02-13 15:47:57,198][rgc][INFO] - Batch 309, avg loss per batch: 1.4117446984844355
[2025-02-13 15:47:57,198][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 310
[2025-02-13 15:48:17,952][rgc][INFO] - 	Updating weights of batch 310
[2025-02-13 15:48:18,002][rgc][INFO] - Batch 310, avg loss per batch: 2.1062689751741144
[2025-02-13 15:48:18,003][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 311
[2025-02-13 15:48:38,758][rgc][INFO] - 	Updating weights of batch 311
[2025-02-13 15:48:38,817][rgc][INFO] - Batch 311, avg loss per batch: 2.732245472714594
[2025-02-13 15:48:38,818][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 312
[2025-02-13 15:48:59,504][rgc][INFO] - 	Updating weights of batch 312
[2025-02-13 15:48:59,557][rgc][INFO] - Batch 312, avg loss per batch: 4.5595076592522545
[2025-02-13 15:48:59,558][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 313
[2025-02-13 15:49:20,244][rgc][INFO] - 	Updating weights of batch 313
[2025-02-13 15:49:20,294][rgc][INFO] - Batch 313, avg loss per batch: 3.7779834202361293
[2025-02-13 15:49:20,294][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 314
[2025-02-13 15:49:41,039][rgc][INFO] - 	Updating weights of batch 314
[2025-02-13 15:49:41,090][rgc][INFO] - Batch 314, avg loss per batch: 2.27779876716718
[2025-02-13 15:49:41,092][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 315
[2025-02-13 15:50:01,853][rgc][INFO] - 	Updating weights of batch 315
[2025-02-13 15:50:01,912][rgc][INFO] - Batch 315, avg loss per batch: 2.8112178488956685
[2025-02-13 15:50:01,913][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 316
[2025-02-13 15:50:22,603][rgc][INFO] - 	Updating weights of batch 316
[2025-02-13 15:50:22,656][rgc][INFO] - Batch 316, avg loss per batch: 2.604236540664235
[2025-02-13 15:50:22,657][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 317
[2025-02-13 15:50:43,327][rgc][INFO] - 	Updating weights of batch 317
[2025-02-13 15:50:43,376][rgc][INFO] - Batch 317, avg loss per batch: 2.955648069667421
[2025-02-13 15:50:43,377][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 318
[2025-02-13 15:51:04,121][rgc][INFO] - 	Updating weights of batch 318
[2025-02-13 15:51:04,170][rgc][INFO] - Batch 318, avg loss per batch: 2.0934213028755613
[2025-02-13 15:51:04,171][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 319
[2025-02-13 15:51:24,860][rgc][INFO] - 	Updating weights of batch 319
[2025-02-13 15:51:24,911][rgc][INFO] - Batch 319, avg loss per batch: 4.680110880100456
[2025-02-13 15:51:24,911][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 320
[2025-02-13 15:51:45,645][rgc][INFO] - 	Updating weights of batch 320
[2025-02-13 15:51:45,697][rgc][INFO] - Batch 320, avg loss per batch: 2.8181857872499414
[2025-02-13 15:51:45,698][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 321
[2025-02-13 15:52:06,406][rgc][INFO] - 	Updating weights of batch 321
[2025-02-13 15:52:06,485][rgc][INFO] - Batch 321, avg loss per batch: 1.326649485360303
[2025-02-13 15:52:06,487][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 322
[2025-02-13 15:52:27,231][rgc][INFO] - 	Updating weights of batch 322
[2025-02-13 15:52:27,283][rgc][INFO] - Batch 322, avg loss per batch: 2.4100457315947166
[2025-02-13 15:52:27,284][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 323
[2025-02-13 15:52:48,026][rgc][INFO] - 	Updating weights of batch 323
[2025-02-13 15:52:48,077][rgc][INFO] - Batch 323, avg loss per batch: 2.8978417240171632
[2025-02-13 15:52:48,078][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 324
[2025-02-13 15:53:08,838][rgc][INFO] - 	Updating weights of batch 324
[2025-02-13 15:53:08,889][rgc][INFO] - Batch 324, avg loss per batch: 1.9048242910789202
[2025-02-13 15:53:08,890][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 325
[2025-02-13 15:53:29,586][rgc][INFO] - 	Updating weights of batch 325
[2025-02-13 15:53:29,656][rgc][INFO] - Batch 325, avg loss per batch: 2.7586754515685343
[2025-02-13 15:53:29,657][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 326
[2025-02-13 15:53:50,384][rgc][INFO] - 	Updating weights of batch 326
[2025-02-13 15:53:50,435][rgc][INFO] - Batch 326, avg loss per batch: 3.40404326811439
[2025-02-13 15:53:50,436][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 327
[2025-02-13 15:54:11,116][rgc][INFO] - 	Updating weights of batch 327
[2025-02-13 15:54:11,168][rgc][INFO] - Batch 327, avg loss per batch: 3.8048297071780133
[2025-02-13 15:54:11,169][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 328
[2025-02-13 15:54:31,919][rgc][INFO] - 	Updating weights of batch 328
[2025-02-13 15:54:31,971][rgc][INFO] - Batch 328, avg loss per batch: 1.7089850658905255
[2025-02-13 15:54:31,972][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 329
[2025-02-13 15:54:52,722][rgc][INFO] - 	Updating weights of batch 329
[2025-02-13 15:54:52,783][rgc][INFO] - Batch 329, avg loss per batch: 1.6857770314639104
[2025-02-13 15:54:52,785][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 330
[2025-02-13 15:55:13,508][rgc][INFO] - 	Updating weights of batch 330
[2025-02-13 15:55:13,559][rgc][INFO] - Batch 330, avg loss per batch: 3.712118656711296
[2025-02-13 15:55:13,560][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 331
[2025-02-13 15:55:34,300][rgc][INFO] - 	Updating weights of batch 331
[2025-02-13 15:55:34,349][rgc][INFO] - Batch 331, avg loss per batch: 2.411145378317799
[2025-02-13 15:55:34,350][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 332
[2025-02-13 15:55:55,036][rgc][INFO] - 	Updating weights of batch 332
[2025-02-13 15:55:55,094][rgc][INFO] - Batch 332, avg loss per batch: 3.816800691386792
[2025-02-13 15:55:55,095][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 333
[2025-02-13 15:56:15,763][rgc][INFO] - 	Updating weights of batch 333
[2025-02-13 15:56:15,814][rgc][INFO] - Batch 333, avg loss per batch: 1.6958955348886022
[2025-02-13 15:56:15,815][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 334
[2025-02-13 15:56:36,554][rgc][INFO] - 	Updating weights of batch 334
[2025-02-13 15:56:36,609][rgc][INFO] - Batch 334, avg loss per batch: 4.86499358644927
[2025-02-13 15:56:36,610][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 335
[2025-02-13 15:56:57,279][rgc][INFO] - 	Updating weights of batch 335
[2025-02-13 15:56:57,329][rgc][INFO] - Batch 335, avg loss per batch: 1.590633156197429
[2025-02-13 15:56:57,331][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 336
[2025-02-13 15:57:18,047][rgc][INFO] - 	Updating weights of batch 336
[2025-02-13 15:57:18,101][rgc][INFO] - Batch 336, avg loss per batch: 2.1684862955423863
[2025-02-13 15:57:18,102][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 337
[2025-02-13 15:57:38,773][rgc][INFO] - 	Updating weights of batch 337
[2025-02-13 15:57:38,830][rgc][INFO] - Batch 337, avg loss per batch: 1.431914847195773
[2025-02-13 15:57:38,831][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 338
[2025-02-13 15:57:59,584][rgc][INFO] - 	Updating weights of batch 338
[2025-02-13 15:57:59,636][rgc][INFO] - Batch 338, avg loss per batch: 2.2129971415232306
[2025-02-13 15:57:59,638][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 339
[2025-02-13 15:58:20,324][rgc][INFO] - 	Updating weights of batch 339
[2025-02-13 15:58:20,375][rgc][INFO] - Batch 339, avg loss per batch: 3.679063611844236
[2025-02-13 15:58:20,376][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 340
[2025-02-13 15:58:41,124][rgc][INFO] - 	Updating weights of batch 340
[2025-02-13 15:58:41,174][rgc][INFO] - Batch 340, avg loss per batch: 2.5107197087335473
[2025-02-13 15:58:41,175][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 341
[2025-02-13 15:59:01,952][rgc][INFO] - 	Updating weights of batch 341
[2025-02-13 15:59:02,009][rgc][INFO] - Batch 341, avg loss per batch: 3.690234678977631
[2025-02-13 15:59:02,010][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 342
[2025-02-13 15:59:22,781][rgc][INFO] - 	Updating weights of batch 342
[2025-02-13 15:59:22,838][rgc][INFO] - Batch 342, avg loss per batch: 3.4047708227019253
[2025-02-13 15:59:22,839][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 343
[2025-02-13 15:59:43,581][rgc][INFO] - 	Updating weights of batch 343
[2025-02-13 15:59:43,636][rgc][INFO] - Batch 343, avg loss per batch: 2.056932054751479
[2025-02-13 15:59:43,637][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 344
[2025-02-13 16:00:04,398][rgc][INFO] - 	Updating weights of batch 344
[2025-02-13 16:00:04,449][rgc][INFO] - Batch 344, avg loss per batch: 2.1280067913623326
[2025-02-13 16:00:04,450][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 345
[2025-02-13 16:00:25,140][rgc][INFO] - 	Updating weights of batch 345
[2025-02-13 16:00:25,197][rgc][INFO] - Batch 345, avg loss per batch: 4.5177512769605865
[2025-02-13 16:00:25,199][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 346
[2025-02-13 16:00:45,962][rgc][INFO] - 	Updating weights of batch 346
[2025-02-13 16:00:46,021][rgc][INFO] - Batch 346, avg loss per batch: 0.5628073219972709
[2025-02-13 16:00:46,021][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 347
[2025-02-13 16:01:06,752][rgc][INFO] - 	Updating weights of batch 347
[2025-02-13 16:01:06,804][rgc][INFO] - Batch 347, avg loss per batch: 2.1690757761270616
[2025-02-13 16:01:06,805][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 348
[2025-02-13 16:01:27,570][rgc][INFO] - 	Updating weights of batch 348
[2025-02-13 16:01:27,636][rgc][INFO] - Batch 348, avg loss per batch: 4.067159121215706
[2025-02-13 16:01:27,637][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 349
[2025-02-13 16:01:48,378][rgc][INFO] - 	Updating weights of batch 349
[2025-02-13 16:01:48,429][rgc][INFO] - Batch 349, avg loss per batch: 2.0596667669444875
[2025-02-13 16:01:48,429][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 350
[2025-02-13 16:02:09,166][rgc][INFO] - 	Updating weights of batch 350
[2025-02-13 16:02:09,233][rgc][INFO] - Batch 350, avg loss per batch: 2.7657471686648565
[2025-02-13 16:02:09,234][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 351
[2025-02-13 16:02:29,982][rgc][INFO] - 	Updating weights of batch 351
[2025-02-13 16:02:30,034][rgc][INFO] - Batch 351, avg loss per batch: 2.0346547467968805
[2025-02-13 16:02:30,035][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 352
[2025-02-13 16:02:50,773][rgc][INFO] - 	Updating weights of batch 352
[2025-02-13 16:02:50,823][rgc][INFO] - Batch 352, avg loss per batch: 1.5322483748948459
[2025-02-13 16:02:50,824][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 353
[2025-02-13 16:03:11,563][rgc][INFO] - 	Updating weights of batch 353
[2025-02-13 16:03:11,616][rgc][INFO] - Batch 353, avg loss per batch: 1.530203100707715
[2025-02-13 16:03:11,617][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 354
[2025-02-13 16:03:32,286][rgc][INFO] - 	Updating weights of batch 354
[2025-02-13 16:03:32,340][rgc][INFO] - Batch 354, avg loss per batch: 2.8675476431454627
[2025-02-13 16:03:32,341][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 355
[2025-02-13 16:03:53,012][rgc][INFO] - 	Updating weights of batch 355
[2025-02-13 16:03:53,064][rgc][INFO] - Batch 355, avg loss per batch: 1.5890307751327541
[2025-02-13 16:03:53,065][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 356
[2025-02-13 16:04:13,731][rgc][INFO] - 	Updating weights of batch 356
[2025-02-13 16:04:13,780][rgc][INFO] - Batch 356, avg loss per batch: 1.7663734409875478
[2025-02-13 16:04:13,781][rgc][INFO] - 	Applying batch grad function of epoch 2 and batch 357
[2025-02-13 16:04:34,515][rgc][INFO] - 	Updating weights of batch 357
[2025-02-13 16:04:34,572][rgc][INFO] - Batch 357, avg loss per batch: 3.086039167135029
[2025-02-13 16:04:34,580][rgc][INFO] - ================= Epoch 2, loss: 993.4193988740197 ===============
[2025-02-13 16:04:34,580][rgc][INFO] - Visualizing histograms
[2025-02-13 16:04:57,374][rgc][INFO] - AVG rho on val data: 0.28963061192679646
[2025-02-13 16:04:57,374][rgc][INFO] - AVG Mean Absolute Error on val data: 0.5274040505709889
[2025-02-13 16:05:08,545][rgc][INFO] - AVG rho on test data: 0.31094657847440227
[2025-02-13 16:05:08,546][rgc][INFO] - AVG Mean Absolute Error on test data: 0.5713588047021071
[2025-02-13 16:05:21,383][rgc][INFO] - AVG rho on train data: 0.2302596603476982
[2025-02-13 16:05:21,383][rgc][INFO] - AVG Mean Absolute Error on train data: 0.5482802819950175
[2025-02-13 16:05:21,384][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 16:05:21,394][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 0
[2025-02-13 16:05:42,066][rgc][INFO] - 	Updating weights of batch 0
[2025-02-13 16:05:42,124][rgc][INFO] - Batch 0, avg loss per batch: 1.6890336300496491
[2025-02-13 16:05:42,125][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 1
[2025-02-13 16:06:02,876][rgc][INFO] - 	Updating weights of batch 1
[2025-02-13 16:06:02,927][rgc][INFO] - Batch 1, avg loss per batch: 5.418168548976468
[2025-02-13 16:06:02,927][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 2
[2025-02-13 16:06:23,642][rgc][INFO] - 	Updating weights of batch 2
[2025-02-13 16:06:23,692][rgc][INFO] - Batch 2, avg loss per batch: 2.3378046169910798
[2025-02-13 16:06:23,693][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 3
[2025-02-13 16:06:44,446][rgc][INFO] - 	Updating weights of batch 3
[2025-02-13 16:06:44,496][rgc][INFO] - Batch 3, avg loss per batch: 3.3583988697162606
[2025-02-13 16:06:44,497][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 4
[2025-02-13 16:07:05,224][rgc][INFO] - 	Updating weights of batch 4
[2025-02-13 16:07:05,278][rgc][INFO] - Batch 4, avg loss per batch: 2.08957149841412
[2025-02-13 16:07:05,279][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 5
[2025-02-13 16:07:25,990][rgc][INFO] - 	Updating weights of batch 5
[2025-02-13 16:07:26,042][rgc][INFO] - Batch 5, avg loss per batch: 3.8041765152450315
[2025-02-13 16:07:26,043][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 6
[2025-02-13 16:07:46,790][rgc][INFO] - 	Updating weights of batch 6
[2025-02-13 16:07:46,841][rgc][INFO] - Batch 6, avg loss per batch: 1.5631263824420154
[2025-02-13 16:07:46,842][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 7
[2025-02-13 16:08:07,612][rgc][INFO] - 	Updating weights of batch 7
[2025-02-13 16:08:07,686][rgc][INFO] - Batch 7, avg loss per batch: 2.1793872887085555
[2025-02-13 16:08:07,687][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 8
[2025-02-13 16:08:28,373][rgc][INFO] - 	Updating weights of batch 8
[2025-02-13 16:08:28,433][rgc][INFO] - Batch 8, avg loss per batch: 1.5162334873698855
[2025-02-13 16:08:28,434][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 9
[2025-02-13 16:08:49,183][rgc][INFO] - 	Updating weights of batch 9
[2025-02-13 16:08:49,239][rgc][INFO] - Batch 9, avg loss per batch: 2.352011464446601
[2025-02-13 16:08:49,239][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 10
[2025-02-13 16:09:09,902][rgc][INFO] - 	Updating weights of batch 10
[2025-02-13 16:09:09,951][rgc][INFO] - Batch 10, avg loss per batch: 2.457761186079948
[2025-02-13 16:09:09,951][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 11
[2025-02-13 16:09:30,693][rgc][INFO] - 	Updating weights of batch 11
[2025-02-13 16:09:30,744][rgc][INFO] - Batch 11, avg loss per batch: 2.65760925182404
[2025-02-13 16:09:30,745][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 12
[2025-02-13 16:09:51,430][rgc][INFO] - 	Updating weights of batch 12
[2025-02-13 16:09:51,485][rgc][INFO] - Batch 12, avg loss per batch: 1.884115563818517
[2025-02-13 16:09:51,486][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 13
[2025-02-13 16:10:12,225][rgc][INFO] - 	Updating weights of batch 13
[2025-02-13 16:10:12,277][rgc][INFO] - Batch 13, avg loss per batch: 1.6261890303957134
[2025-02-13 16:10:12,278][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 14
[2025-02-13 16:10:32,975][rgc][INFO] - 	Updating weights of batch 14
[2025-02-13 16:10:33,027][rgc][INFO] - Batch 14, avg loss per batch: 2.5114916361261326
[2025-02-13 16:10:33,028][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 15
[2025-02-13 16:10:53,772][rgc][INFO] - 	Updating weights of batch 15
[2025-02-13 16:10:53,825][rgc][INFO] - Batch 15, avg loss per batch: 2.1790341078073823
[2025-02-13 16:10:53,827][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 16
[2025-02-13 16:11:14,522][rgc][INFO] - 	Updating weights of batch 16
[2025-02-13 16:11:14,578][rgc][INFO] - Batch 16, avg loss per batch: 3.3596812536349874
[2025-02-13 16:11:14,579][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 17
[2025-02-13 16:11:35,261][rgc][INFO] - 	Updating weights of batch 17
[2025-02-13 16:11:35,312][rgc][INFO] - Batch 17, avg loss per batch: 1.562659881174581
[2025-02-13 16:11:35,313][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 18
[2025-02-13 16:11:55,983][rgc][INFO] - 	Updating weights of batch 18
[2025-02-13 16:11:56,035][rgc][INFO] - Batch 18, avg loss per batch: 1.438817724683697
[2025-02-13 16:11:56,036][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 19
[2025-02-13 16:12:16,780][rgc][INFO] - 	Updating weights of batch 19
[2025-02-13 16:12:16,833][rgc][INFO] - Batch 19, avg loss per batch: 2.3212530672501686
[2025-02-13 16:12:16,834][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 20
[2025-02-13 16:12:37,506][rgc][INFO] - 	Updating weights of batch 20
[2025-02-13 16:12:37,581][rgc][INFO] - Batch 20, avg loss per batch: 2.9041362491978555
[2025-02-13 16:12:37,582][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 21
[2025-02-13 16:12:58,243][rgc][INFO] - 	Updating weights of batch 21
[2025-02-13 16:12:58,294][rgc][INFO] - Batch 21, avg loss per batch: 2.461290040953171
[2025-02-13 16:12:58,295][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 22
[2025-02-13 16:13:18,981][rgc][INFO] - 	Updating weights of batch 22
[2025-02-13 16:13:19,029][rgc][INFO] - Batch 22, avg loss per batch: 3.218055060119881
[2025-02-13 16:13:19,030][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 23
[2025-02-13 16:13:39,699][rgc][INFO] - 	Updating weights of batch 23
[2025-02-13 16:13:39,746][rgc][INFO] - Batch 23, avg loss per batch: 2.7113993666617175
[2025-02-13 16:13:39,747][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 24
[2025-02-13 16:14:00,415][rgc][INFO] - 	Updating weights of batch 24
[2025-02-13 16:14:00,465][rgc][INFO] - Batch 24, avg loss per batch: 2.061370961699162
[2025-02-13 16:14:00,466][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 25
[2025-02-13 16:14:21,199][rgc][INFO] - 	Updating weights of batch 25
[2025-02-13 16:14:21,253][rgc][INFO] - Batch 25, avg loss per batch: 2.77151988010216
[2025-02-13 16:14:21,253][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 26
[2025-02-13 16:14:41,994][rgc][INFO] - 	Updating weights of batch 26
[2025-02-13 16:14:42,067][rgc][INFO] - Batch 26, avg loss per batch: 1.9845195329125298
[2025-02-13 16:14:42,068][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 27
[2025-02-13 16:15:02,795][rgc][INFO] - 	Updating weights of batch 27
[2025-02-13 16:15:02,849][rgc][INFO] - Batch 27, avg loss per batch: 2.061628139012235
[2025-02-13 16:15:02,850][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 28
[2025-02-13 16:15:23,570][rgc][INFO] - 	Updating weights of batch 28
[2025-02-13 16:15:23,622][rgc][INFO] - Batch 28, avg loss per batch: 2.0183791994520868
[2025-02-13 16:15:23,623][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 29
[2025-02-13 16:15:44,296][rgc][INFO] - 	Updating weights of batch 29
[2025-02-13 16:15:44,349][rgc][INFO] - Batch 29, avg loss per batch: 4.436154563434749
[2025-02-13 16:15:44,350][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 30
[2025-02-13 16:16:05,017][rgc][INFO] - 	Updating weights of batch 30
[2025-02-13 16:16:05,066][rgc][INFO] - Batch 30, avg loss per batch: 2.9885798583764367
[2025-02-13 16:16:05,067][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 31
[2025-02-13 16:16:25,800][rgc][INFO] - 	Updating weights of batch 31
[2025-02-13 16:16:25,852][rgc][INFO] - Batch 31, avg loss per batch: 2.8554737535905472
[2025-02-13 16:16:25,853][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 32
[2025-02-13 16:16:46,521][rgc][INFO] - 	Updating weights of batch 32
[2025-02-13 16:16:46,574][rgc][INFO] - Batch 32, avg loss per batch: 2.4189465724068984
[2025-02-13 16:16:46,574][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 33
[2025-02-13 16:17:07,338][rgc][INFO] - 	Updating weights of batch 33
[2025-02-13 16:17:07,388][rgc][INFO] - Batch 33, avg loss per batch: 2.0794851098491822
[2025-02-13 16:17:07,390][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 34
[2025-02-13 16:17:28,133][rgc][INFO] - 	Updating weights of batch 34
[2025-02-13 16:17:28,187][rgc][INFO] - Batch 34, avg loss per batch: 1.4930393314109285
[2025-02-13 16:17:28,188][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 35
[2025-02-13 16:17:48,888][rgc][INFO] - 	Updating weights of batch 35
[2025-02-13 16:17:48,943][rgc][INFO] - Batch 35, avg loss per batch: 1.3682227400600535
[2025-02-13 16:17:48,944][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 36
[2025-02-13 16:18:09,640][rgc][INFO] - 	Updating weights of batch 36
[2025-02-13 16:18:09,694][rgc][INFO] - Batch 36, avg loss per batch: 2.180067006007004
[2025-02-13 16:18:09,695][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 37
[2025-02-13 16:18:30,380][rgc][INFO] - 	Updating weights of batch 37
[2025-02-13 16:18:30,435][rgc][INFO] - Batch 37, avg loss per batch: 3.3888729525361247
[2025-02-13 16:18:30,436][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 38
[2025-02-13 16:18:51,173][rgc][INFO] - 	Updating weights of batch 38
[2025-02-13 16:18:51,231][rgc][INFO] - Batch 38, avg loss per batch: 1.1742684936339052
[2025-02-13 16:18:51,231][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 39
[2025-02-13 16:19:11,915][rgc][INFO] - 	Updating weights of batch 39
[2025-02-13 16:19:11,970][rgc][INFO] - Batch 39, avg loss per batch: 2.3226147492538773
[2025-02-13 16:19:11,971][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 40
[2025-02-13 16:19:32,650][rgc][INFO] - 	Updating weights of batch 40
[2025-02-13 16:19:32,703][rgc][INFO] - Batch 40, avg loss per batch: 3.1913039438735713
[2025-02-13 16:19:32,704][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 41
[2025-02-13 16:19:53,382][rgc][INFO] - 	Updating weights of batch 41
[2025-02-13 16:19:53,434][rgc][INFO] - Batch 41, avg loss per batch: 2.1620625931885944
[2025-02-13 16:19:53,435][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 42
[2025-02-13 16:20:14,183][rgc][INFO] - 	Updating weights of batch 42
[2025-02-13 16:20:14,241][rgc][INFO] - Batch 42, avg loss per batch: 1.9264563690839682
[2025-02-13 16:20:14,242][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 43
[2025-02-13 16:20:34,946][rgc][INFO] - 	Updating weights of batch 43
[2025-02-13 16:20:35,015][rgc][INFO] - Batch 43, avg loss per batch: 1.7259616305606884
[2025-02-13 16:20:35,015][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 44
[2025-02-13 16:20:55,710][rgc][INFO] - 	Updating weights of batch 44
[2025-02-13 16:20:55,767][rgc][INFO] - Batch 44, avg loss per batch: 2.6448890538208674
[2025-02-13 16:20:55,768][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 45
[2025-02-13 16:21:16,512][rgc][INFO] - 	Updating weights of batch 45
[2025-02-13 16:21:16,563][rgc][INFO] - Batch 45, avg loss per batch: 2.8641389827388886
[2025-02-13 16:21:16,563][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 46
[2025-02-13 16:21:37,324][rgc][INFO] - 	Updating weights of batch 46
[2025-02-13 16:21:37,377][rgc][INFO] - Batch 46, avg loss per batch: 2.6903143476368725
[2025-02-13 16:21:37,378][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 47
[2025-02-13 16:21:58,127][rgc][INFO] - 	Updating weights of batch 47
[2025-02-13 16:21:58,180][rgc][INFO] - Batch 47, avg loss per batch: 2.7468192183270546
[2025-02-13 16:21:58,180][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 48
[2025-02-13 16:22:18,864][rgc][INFO] - 	Updating weights of batch 48
[2025-02-13 16:22:18,922][rgc][INFO] - Batch 48, avg loss per batch: 2.507068017538809
[2025-02-13 16:22:18,923][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 49
[2025-02-13 16:22:39,647][rgc][INFO] - 	Updating weights of batch 49
[2025-02-13 16:22:39,700][rgc][INFO] - Batch 49, avg loss per batch: 1.6002960135290794
[2025-02-13 16:22:39,701][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 50
[2025-02-13 16:23:00,458][rgc][INFO] - 	Updating weights of batch 50
[2025-02-13 16:23:00,513][rgc][INFO] - Batch 50, avg loss per batch: 1.8860497016288906
[2025-02-13 16:23:00,514][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 51
[2025-02-13 16:23:21,259][rgc][INFO] - 	Updating weights of batch 51
[2025-02-13 16:23:21,308][rgc][INFO] - Batch 51, avg loss per batch: 3.4363993818150034
[2025-02-13 16:23:21,309][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 52
[2025-02-13 16:23:42,073][rgc][INFO] - 	Updating weights of batch 52
[2025-02-13 16:23:42,127][rgc][INFO] - Batch 52, avg loss per batch: 3.4315466358340454
[2025-02-13 16:23:42,127][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 53
[2025-02-13 16:24:02,872][rgc][INFO] - 	Updating weights of batch 53
[2025-02-13 16:24:02,922][rgc][INFO] - Batch 53, avg loss per batch: 3.2242155630366796
[2025-02-13 16:24:02,924][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 54
[2025-02-13 16:24:23,604][rgc][INFO] - 	Updating weights of batch 54
[2025-02-13 16:24:23,656][rgc][INFO] - Batch 54, avg loss per batch: 1.4880961188034496
[2025-02-13 16:24:23,657][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 55
[2025-02-13 16:24:44,425][rgc][INFO] - 	Updating weights of batch 55
[2025-02-13 16:24:44,476][rgc][INFO] - Batch 55, avg loss per batch: 1.8589327943688
[2025-02-13 16:24:44,476][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 56
[2025-02-13 16:25:05,221][rgc][INFO] - 	Updating weights of batch 56
[2025-02-13 16:25:05,272][rgc][INFO] - Batch 56, avg loss per batch: 2.5366460119020244
[2025-02-13 16:25:05,273][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 57
[2025-02-13 16:25:26,031][rgc][INFO] - 	Updating weights of batch 57
[2025-02-13 16:25:26,086][rgc][INFO] - Batch 57, avg loss per batch: 1.5990043159078897
[2025-02-13 16:25:26,087][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 58
[2025-02-13 16:25:46,842][rgc][INFO] - 	Updating weights of batch 58
[2025-02-13 16:25:46,893][rgc][INFO] - Batch 58, avg loss per batch: 2.9272424842975484
[2025-02-13 16:25:46,894][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 59
[2025-02-13 16:26:07,616][rgc][INFO] - 	Updating weights of batch 59
[2025-02-13 16:26:07,669][rgc][INFO] - Batch 59, avg loss per batch: 2.262731248863366
[2025-02-13 16:26:07,670][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 60
[2025-02-13 16:26:28,424][rgc][INFO] - 	Updating weights of batch 60
[2025-02-13 16:26:28,474][rgc][INFO] - Batch 60, avg loss per batch: 3.1190768559711866
[2025-02-13 16:26:28,475][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 61
[2025-02-13 16:26:49,196][rgc][INFO] - 	Updating weights of batch 61
[2025-02-13 16:26:49,260][rgc][INFO] - Batch 61, avg loss per batch: 3.244620832584185
[2025-02-13 16:26:49,261][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 62
[2025-02-13 16:27:09,971][rgc][INFO] - 	Updating weights of batch 62
[2025-02-13 16:27:10,022][rgc][INFO] - Batch 62, avg loss per batch: 3.055612160618157
[2025-02-13 16:27:10,022][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 63
[2025-02-13 16:27:30,725][rgc][INFO] - 	Updating weights of batch 63
[2025-02-13 16:27:30,778][rgc][INFO] - Batch 63, avg loss per batch: 3.6245909373520497
[2025-02-13 16:27:30,779][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 64
[2025-02-13 16:27:51,460][rgc][INFO] - 	Updating weights of batch 64
[2025-02-13 16:27:51,514][rgc][INFO] - Batch 64, avg loss per batch: 2.8624260815540516
[2025-02-13 16:27:51,514][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 65
[2025-02-13 16:28:12,283][rgc][INFO] - 	Updating weights of batch 65
[2025-02-13 16:28:12,338][rgc][INFO] - Batch 65, avg loss per batch: 2.5796419458101125
[2025-02-13 16:28:12,339][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 66
[2025-02-13 16:28:33,027][rgc][INFO] - 	Updating weights of batch 66
[2025-02-13 16:28:33,078][rgc][INFO] - Batch 66, avg loss per batch: 2.687229020446496
[2025-02-13 16:28:33,078][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 67
[2025-02-13 16:28:53,744][rgc][INFO] - 	Updating weights of batch 67
[2025-02-13 16:28:53,793][rgc][INFO] - Batch 67, avg loss per batch: 3.5679306786253635
[2025-02-13 16:28:53,793][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 68
[2025-02-13 16:29:14,480][rgc][INFO] - 	Updating weights of batch 68
[2025-02-13 16:29:14,536][rgc][INFO] - Batch 68, avg loss per batch: 2.1304156472300657
[2025-02-13 16:29:14,537][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 69
[2025-02-13 16:29:35,262][rgc][INFO] - 	Updating weights of batch 69
[2025-02-13 16:29:35,318][rgc][INFO] - Batch 69, avg loss per batch: 4.77683710692675
[2025-02-13 16:29:35,319][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 70
[2025-02-13 16:29:56,046][rgc][INFO] - 	Updating weights of batch 70
[2025-02-13 16:29:56,100][rgc][INFO] - Batch 70, avg loss per batch: 3.304365816952154
[2025-02-13 16:29:56,101][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 71
[2025-02-13 16:30:16,826][rgc][INFO] - 	Updating weights of batch 71
[2025-02-13 16:30:16,881][rgc][INFO] - Batch 71, avg loss per batch: 3.48774264056438
[2025-02-13 16:30:16,882][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 72
[2025-02-13 16:30:37,618][rgc][INFO] - 	Updating weights of batch 72
[2025-02-13 16:30:37,673][rgc][INFO] - Batch 72, avg loss per batch: 1.9822372150537786
[2025-02-13 16:30:37,674][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 73
[2025-02-13 16:30:58,414][rgc][INFO] - 	Updating weights of batch 73
[2025-02-13 16:30:58,468][rgc][INFO] - Batch 73, avg loss per batch: 1.587624173361363
[2025-02-13 16:30:58,469][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 74
[2025-02-13 16:31:19,213][rgc][INFO] - 	Updating weights of batch 74
[2025-02-13 16:31:19,278][rgc][INFO] - Batch 74, avg loss per batch: 1.9983385072697013
[2025-02-13 16:31:19,278][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 75
[2025-02-13 16:31:40,024][rgc][INFO] - 	Updating weights of batch 75
[2025-02-13 16:31:40,077][rgc][INFO] - Batch 75, avg loss per batch: 4.7282377078877875
[2025-02-13 16:31:40,077][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 76
[2025-02-13 16:32:00,754][rgc][INFO] - 	Updating weights of batch 76
[2025-02-13 16:32:00,808][rgc][INFO] - Batch 76, avg loss per batch: 1.5382510261835831
[2025-02-13 16:32:00,809][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 77
[2025-02-13 16:32:21,488][rgc][INFO] - 	Updating weights of batch 77
[2025-02-13 16:32:21,543][rgc][INFO] - Batch 77, avg loss per batch: 3.7032279688201006
[2025-02-13 16:32:21,544][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 78
[2025-02-13 16:32:42,292][rgc][INFO] - 	Updating weights of batch 78
[2025-02-13 16:32:42,350][rgc][INFO] - Batch 78, avg loss per batch: 2.41074573005653
[2025-02-13 16:32:42,350][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 79
[2025-02-13 16:33:03,014][rgc][INFO] - 	Updating weights of batch 79
[2025-02-13 16:33:03,070][rgc][INFO] - Batch 79, avg loss per batch: 1.9705129342700676
[2025-02-13 16:33:03,071][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 80
[2025-02-13 16:33:23,743][rgc][INFO] - 	Updating weights of batch 80
[2025-02-13 16:33:23,795][rgc][INFO] - Batch 80, avg loss per batch: 2.8115306460562137
[2025-02-13 16:33:23,796][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 81
[2025-02-13 16:33:44,518][rgc][INFO] - 	Updating weights of batch 81
[2025-02-13 16:33:44,568][rgc][INFO] - Batch 81, avg loss per batch: 4.886541503673325
[2025-02-13 16:33:44,569][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 82
[2025-02-13 16:34:05,262][rgc][INFO] - 	Updating weights of batch 82
[2025-02-13 16:34:05,315][rgc][INFO] - Batch 82, avg loss per batch: 4.599592492339053
[2025-02-13 16:34:05,315][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 83
[2025-02-13 16:34:25,989][rgc][INFO] - 	Updating weights of batch 83
[2025-02-13 16:34:26,042][rgc][INFO] - Batch 83, avg loss per batch: 2.694666993687373
[2025-02-13 16:34:26,043][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 84
[2025-02-13 16:34:46,705][rgc][INFO] - 	Updating weights of batch 84
[2025-02-13 16:34:46,759][rgc][INFO] - Batch 84, avg loss per batch: 3.156425758349469
[2025-02-13 16:34:46,760][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 85
[2025-02-13 16:35:07,498][rgc][INFO] - 	Updating weights of batch 85
[2025-02-13 16:35:07,550][rgc][INFO] - Batch 85, avg loss per batch: 2.9972856555735685
[2025-02-13 16:35:07,551][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 86
[2025-02-13 16:35:28,238][rgc][INFO] - 	Updating weights of batch 86
[2025-02-13 16:35:28,299][rgc][INFO] - Batch 86, avg loss per batch: 3.5898607780930454
[2025-02-13 16:35:28,300][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 87
[2025-02-13 16:35:48,986][rgc][INFO] - 	Updating weights of batch 87
[2025-02-13 16:35:49,045][rgc][INFO] - Batch 87, avg loss per batch: 1.7551297108010941
[2025-02-13 16:35:49,046][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 88
[2025-02-13 16:36:09,738][rgc][INFO] - 	Updating weights of batch 88
[2025-02-13 16:36:09,804][rgc][INFO] - Batch 88, avg loss per batch: 2.1579576326900103
[2025-02-13 16:36:09,805][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 89
[2025-02-13 16:36:30,498][rgc][INFO] - 	Updating weights of batch 89
[2025-02-13 16:36:30,552][rgc][INFO] - Batch 89, avg loss per batch: 0.9000025206919233
[2025-02-13 16:36:30,552][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 90
[2025-02-13 16:36:51,244][rgc][INFO] - 	Updating weights of batch 90
[2025-02-13 16:36:51,297][rgc][INFO] - Batch 90, avg loss per batch: 3.805971417836857
[2025-02-13 16:36:51,298][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 91
[2025-02-13 16:37:12,032][rgc][INFO] - 	Updating weights of batch 91
[2025-02-13 16:37:12,097][rgc][INFO] - Batch 91, avg loss per batch: 2.289985239797367
[2025-02-13 16:37:12,098][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 92
[2025-02-13 16:37:32,848][rgc][INFO] - 	Updating weights of batch 92
[2025-02-13 16:37:32,901][rgc][INFO] - Batch 92, avg loss per batch: 0.4589354605411651
[2025-02-13 16:37:32,902][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 93
[2025-02-13 16:37:53,577][rgc][INFO] - 	Updating weights of batch 93
[2025-02-13 16:37:53,626][rgc][INFO] - Batch 93, avg loss per batch: 2.526374441044556
[2025-02-13 16:37:53,627][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 94
[2025-02-13 16:38:14,348][rgc][INFO] - 	Updating weights of batch 94
[2025-02-13 16:38:14,401][rgc][INFO] - Batch 94, avg loss per batch: 3.6968453324104305
[2025-02-13 16:38:14,403][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 95
[2025-02-13 16:38:35,140][rgc][INFO] - 	Updating weights of batch 95
[2025-02-13 16:38:35,188][rgc][INFO] - Batch 95, avg loss per batch: 4.348932553835609
[2025-02-13 16:38:35,188][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 96
[2025-02-13 16:38:55,944][rgc][INFO] - 	Updating weights of batch 96
[2025-02-13 16:38:55,996][rgc][INFO] - Batch 96, avg loss per batch: 1.9511624142946478
[2025-02-13 16:38:55,998][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 97
[2025-02-13 16:39:16,739][rgc][INFO] - 	Updating weights of batch 97
[2025-02-13 16:39:16,793][rgc][INFO] - Batch 97, avg loss per batch: 1.9724296357318531
[2025-02-13 16:39:16,794][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 98
[2025-02-13 16:39:37,468][rgc][INFO] - 	Updating weights of batch 98
[2025-02-13 16:39:37,533][rgc][INFO] - Batch 98, avg loss per batch: 3.8529982777171856
[2025-02-13 16:39:37,534][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 99
[2025-02-13 16:39:58,206][rgc][INFO] - 	Updating weights of batch 99
[2025-02-13 16:39:58,255][rgc][INFO] - Batch 99, avg loss per batch: 3.3312495913245193
[2025-02-13 16:40:09,876][rgc][INFO] - AVG rho on val data: 0.27226489917736185
[2025-02-13 16:40:09,876][rgc][INFO] - AVG mae on val data: 0.5254104675290867
[2025-02-13 16:40:21,208][rgc][INFO] - AVG rho on test data: 0.3090598084682139
[2025-02-13 16:40:21,209][rgc][INFO] - AVG mae on test data: 0.5609318702451601
[2025-02-13 16:40:34,056][rgc][INFO] - AVG rho on train data: 0.2209401586876175
[2025-02-13 16:40:34,056][rgc][INFO] - AVG mae on train data: 0.5462379746470474
[2025-02-13 16:40:34,060][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 16:40:34,062][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 100
[2025-02-13 16:40:54,826][rgc][INFO] - 	Updating weights of batch 100
[2025-02-13 16:40:54,883][rgc][INFO] - Batch 100, avg loss per batch: 4.137325422248825
[2025-02-13 16:40:54,884][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 101
[2025-02-13 16:41:15,596][rgc][INFO] - 	Updating weights of batch 101
[2025-02-13 16:41:15,676][rgc][INFO] - Batch 101, avg loss per batch: 2.396161376930747
[2025-02-13 16:41:15,677][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 102
[2025-02-13 16:41:36,410][rgc][INFO] - 	Updating weights of batch 102
[2025-02-13 16:41:36,478][rgc][INFO] - Batch 102, avg loss per batch: 2.3909375572598894
[2025-02-13 16:41:36,479][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 103
[2025-02-13 16:41:57,160][rgc][INFO] - 	Updating weights of batch 103
[2025-02-13 16:41:57,219][rgc][INFO] - Batch 103, avg loss per batch: 4.809534471126378
[2025-02-13 16:41:57,221][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 104
[2025-02-13 16:42:17,919][rgc][INFO] - 	Updating weights of batch 104
[2025-02-13 16:42:17,970][rgc][INFO] - Batch 104, avg loss per batch: 2.7142998588181917
[2025-02-13 16:42:17,971][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 105
[2025-02-13 16:42:38,710][rgc][INFO] - 	Updating weights of batch 105
[2025-02-13 16:42:38,789][rgc][INFO] - Batch 105, avg loss per batch: 1.1779406313298317
[2025-02-13 16:42:38,791][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 106
[2025-02-13 16:42:59,462][rgc][INFO] - 	Updating weights of batch 106
[2025-02-13 16:42:59,511][rgc][INFO] - Batch 106, avg loss per batch: 2.8316952866784613
[2025-02-13 16:42:59,512][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 107
[2025-02-13 16:43:20,260][rgc][INFO] - 	Updating weights of batch 107
[2025-02-13 16:43:20,315][rgc][INFO] - Batch 107, avg loss per batch: 2.0397633109754434
[2025-02-13 16:43:20,317][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 108
[2025-02-13 16:43:40,990][rgc][INFO] - 	Updating weights of batch 108
[2025-02-13 16:43:41,040][rgc][INFO] - Batch 108, avg loss per batch: 3.1369755861323005
[2025-02-13 16:43:41,041][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 109
[2025-02-13 16:44:01,767][rgc][INFO] - 	Updating weights of batch 109
[2025-02-13 16:44:01,821][rgc][INFO] - Batch 109, avg loss per batch: 5.424296702654507
[2025-02-13 16:44:01,823][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 110
[2025-02-13 16:44:22,558][rgc][INFO] - 	Updating weights of batch 110
[2025-02-13 16:44:22,620][rgc][INFO] - Batch 110, avg loss per batch: 1.9691264132626427
[2025-02-13 16:44:22,620][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 111
[2025-02-13 16:44:43,353][rgc][INFO] - 	Updating weights of batch 111
[2025-02-13 16:44:43,417][rgc][INFO] - Batch 111, avg loss per batch: 1.5464095778240026
[2025-02-13 16:44:43,417][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 112
[2025-02-13 16:45:04,101][rgc][INFO] - 	Updating weights of batch 112
[2025-02-13 16:45:04,154][rgc][INFO] - Batch 112, avg loss per batch: 2.164119413469245
[2025-02-13 16:45:04,155][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 113
[2025-02-13 16:45:24,842][rgc][INFO] - 	Updating weights of batch 113
[2025-02-13 16:45:24,894][rgc][INFO] - Batch 113, avg loss per batch: 1.9743737163784987
[2025-02-13 16:45:24,895][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 114
[2025-02-13 16:45:45,577][rgc][INFO] - 	Updating weights of batch 114
[2025-02-13 16:45:45,650][rgc][INFO] - Batch 114, avg loss per batch: 2.5254111760257683
[2025-02-13 16:45:45,651][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 115
[2025-02-13 16:46:06,413][rgc][INFO] - 	Updating weights of batch 115
[2025-02-13 16:46:06,473][rgc][INFO] - Batch 115, avg loss per batch: 2.182299421198991
[2025-02-13 16:46:06,474][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 116
[2025-02-13 16:46:27,175][rgc][INFO] - 	Updating weights of batch 116
[2025-02-13 16:46:27,229][rgc][INFO] - Batch 116, avg loss per batch: 1.0714856855065849
[2025-02-13 16:46:27,230][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 117
[2025-02-13 16:46:47,986][rgc][INFO] - 	Updating weights of batch 117
[2025-02-13 16:46:48,040][rgc][INFO] - Batch 117, avg loss per batch: 2.6414388587556825
[2025-02-13 16:46:48,041][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 118
[2025-02-13 16:47:08,807][rgc][INFO] - 	Updating weights of batch 118
[2025-02-13 16:47:08,860][rgc][INFO] - Batch 118, avg loss per batch: 2.011427470805786
[2025-02-13 16:47:08,861][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 119
[2025-02-13 16:47:29,597][rgc][INFO] - 	Updating weights of batch 119
[2025-02-13 16:47:29,655][rgc][INFO] - Batch 119, avg loss per batch: 2.2583434869053693
[2025-02-13 16:47:29,656][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 120
[2025-02-13 16:47:50,414][rgc][INFO] - 	Updating weights of batch 120
[2025-02-13 16:47:50,466][rgc][INFO] - Batch 120, avg loss per batch: 2.098149729790513
[2025-02-13 16:47:50,466][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 121
[2025-02-13 16:48:11,153][rgc][INFO] - 	Updating weights of batch 121
[2025-02-13 16:48:11,204][rgc][INFO] - Batch 121, avg loss per batch: 1.9677788456681782
[2025-02-13 16:48:11,205][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 122
[2025-02-13 16:48:31,890][rgc][INFO] - 	Updating weights of batch 122
[2025-02-13 16:48:31,943][rgc][INFO] - Batch 122, avg loss per batch: 2.6985468214649813
[2025-02-13 16:48:31,944][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 123
[2025-02-13 16:48:52,701][rgc][INFO] - 	Updating weights of batch 123
[2025-02-13 16:48:52,751][rgc][INFO] - Batch 123, avg loss per batch: 2.180951507540793
[2025-02-13 16:48:52,752][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 124
[2025-02-13 16:49:13,517][rgc][INFO] - 	Updating weights of batch 124
[2025-02-13 16:49:13,568][rgc][INFO] - Batch 124, avg loss per batch: 3.4416751776179857
[2025-02-13 16:49:13,569][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 125
[2025-02-13 16:49:34,332][rgc][INFO] - 	Updating weights of batch 125
[2025-02-13 16:49:34,382][rgc][INFO] - Batch 125, avg loss per batch: 3.0290076308563574
[2025-02-13 16:49:34,382][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 126
[2025-02-13 16:49:55,155][rgc][INFO] - 	Updating weights of batch 126
[2025-02-13 16:49:55,206][rgc][INFO] - Batch 126, avg loss per batch: 3.8529941698756973
[2025-02-13 16:49:55,207][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 127
[2025-02-13 16:50:15,954][rgc][INFO] - 	Updating weights of batch 127
[2025-02-13 16:50:16,008][rgc][INFO] - Batch 127, avg loss per batch: 2.3255484090959566
[2025-02-13 16:50:16,009][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 128
[2025-02-13 16:50:36,747][rgc][INFO] - 	Updating weights of batch 128
[2025-02-13 16:50:36,799][rgc][INFO] - Batch 128, avg loss per batch: 3.9356513359155834
[2025-02-13 16:50:36,799][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 129
[2025-02-13 16:50:57,573][rgc][INFO] - 	Updating weights of batch 129
[2025-02-13 16:50:57,623][rgc][INFO] - Batch 129, avg loss per batch: 3.5884905563640794
[2025-02-13 16:50:57,623][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 130
[2025-02-13 16:51:18,284][rgc][INFO] - 	Updating weights of batch 130
[2025-02-13 16:51:18,332][rgc][INFO] - Batch 130, avg loss per batch: 4.316866704965059
[2025-02-13 16:51:18,333][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 131
[2025-02-13 16:51:39,020][rgc][INFO] - 	Updating weights of batch 131
[2025-02-13 16:51:39,070][rgc][INFO] - Batch 131, avg loss per batch: 3.997270332922877
[2025-02-13 16:51:39,071][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 132
[2025-02-13 16:51:59,752][rgc][INFO] - 	Updating weights of batch 132
[2025-02-13 16:51:59,805][rgc][INFO] - Batch 132, avg loss per batch: 3.2533010613998545
[2025-02-13 16:51:59,806][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 133
[2025-02-13 16:52:20,491][rgc][INFO] - 	Updating weights of batch 133
[2025-02-13 16:52:20,544][rgc][INFO] - Batch 133, avg loss per batch: 2.3394282535555435
[2025-02-13 16:52:20,545][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 134
[2025-02-13 16:52:41,248][rgc][INFO] - 	Updating weights of batch 134
[2025-02-13 16:52:41,303][rgc][INFO] - Batch 134, avg loss per batch: 2.424934285297017
[2025-02-13 16:52:41,304][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 135
[2025-02-13 16:53:02,065][rgc][INFO] - 	Updating weights of batch 135
[2025-02-13 16:53:02,117][rgc][INFO] - Batch 135, avg loss per batch: 2.916423212625026
[2025-02-13 16:53:02,119][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 136
[2025-02-13 16:53:22,875][rgc][INFO] - 	Updating weights of batch 136
[2025-02-13 16:53:22,932][rgc][INFO] - Batch 136, avg loss per batch: 4.884043444178083
[2025-02-13 16:53:22,934][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 137
[2025-02-13 16:53:43,686][rgc][INFO] - 	Updating weights of batch 137
[2025-02-13 16:53:43,741][rgc][INFO] - Batch 137, avg loss per batch: 1.3312311419777845
[2025-02-13 16:53:43,743][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 138
[2025-02-13 16:54:04,437][rgc][INFO] - 	Updating weights of batch 138
[2025-02-13 16:54:04,492][rgc][INFO] - Batch 138, avg loss per batch: 1.31495745786372
[2025-02-13 16:54:04,492][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 139
[2025-02-13 16:54:25,256][rgc][INFO] - 	Updating weights of batch 139
[2025-02-13 16:54:25,315][rgc][INFO] - Batch 139, avg loss per batch: 1.923352357804188
[2025-02-13 16:54:25,316][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 140
[2025-02-13 16:54:46,085][rgc][INFO] - 	Updating weights of batch 140
[2025-02-13 16:54:46,134][rgc][INFO] - Batch 140, avg loss per batch: 3.360964534634665
[2025-02-13 16:54:46,135][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 141
[2025-02-13 16:55:06,877][rgc][INFO] - 	Updating weights of batch 141
[2025-02-13 16:55:06,946][rgc][INFO] - Batch 141, avg loss per batch: 3.284009604634742
[2025-02-13 16:55:06,948][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 142
[2025-02-13 16:55:27,684][rgc][INFO] - 	Updating weights of batch 142
[2025-02-13 16:55:27,760][rgc][INFO] - Batch 142, avg loss per batch: 1.7167318861885057
[2025-02-13 16:55:27,761][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 143
[2025-02-13 16:55:48,479][rgc][INFO] - 	Updating weights of batch 143
[2025-02-13 16:55:48,532][rgc][INFO] - Batch 143, avg loss per batch: 2.7064608352256547
[2025-02-13 16:55:48,533][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 144
[2025-02-13 16:56:09,219][rgc][INFO] - 	Updating weights of batch 144
[2025-02-13 16:56:09,271][rgc][INFO] - Batch 144, avg loss per batch: 3.081708519715407
[2025-02-13 16:56:09,272][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 145
[2025-02-13 16:56:30,006][rgc][INFO] - 	Updating weights of batch 145
[2025-02-13 16:56:30,058][rgc][INFO] - Batch 145, avg loss per batch: 5.418568986401291
[2025-02-13 16:56:30,059][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 146
[2025-02-13 16:56:50,786][rgc][INFO] - 	Updating weights of batch 146
[2025-02-13 16:56:50,835][rgc][INFO] - Batch 146, avg loss per batch: 2.9790280016775066
[2025-02-13 16:56:50,836][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 147
[2025-02-13 16:57:11,502][rgc][INFO] - 	Updating weights of batch 147
[2025-02-13 16:57:11,552][rgc][INFO] - Batch 147, avg loss per batch: 3.540497007493997
[2025-02-13 16:57:11,553][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 148
[2025-02-13 16:57:32,307][rgc][INFO] - 	Updating weights of batch 148
[2025-02-13 16:57:32,360][rgc][INFO] - Batch 148, avg loss per batch: 2.856501071274124
[2025-02-13 16:57:32,361][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 149
[2025-02-13 16:57:53,115][rgc][INFO] - 	Updating weights of batch 149
[2025-02-13 16:57:53,169][rgc][INFO] - Batch 149, avg loss per batch: 2.1758466185819003
[2025-02-13 16:57:53,170][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 150
[2025-02-13 16:58:13,906][rgc][INFO] - 	Updating weights of batch 150
[2025-02-13 16:58:13,958][rgc][INFO] - Batch 150, avg loss per batch: 4.303764444645388
[2025-02-13 16:58:13,958][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 151
[2025-02-13 16:58:34,694][rgc][INFO] - 	Updating weights of batch 151
[2025-02-13 16:58:34,753][rgc][INFO] - Batch 151, avg loss per batch: 3.015717613224092
[2025-02-13 16:58:34,753][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 152
[2025-02-13 16:58:55,490][rgc][INFO] - 	Updating weights of batch 152
[2025-02-13 16:58:55,539][rgc][INFO] - Batch 152, avg loss per batch: 4.172887952517597
[2025-02-13 16:58:55,540][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 153
[2025-02-13 16:59:16,281][rgc][INFO] - 	Updating weights of batch 153
[2025-02-13 16:59:16,331][rgc][INFO] - Batch 153, avg loss per batch: 2.53620378206568
[2025-02-13 16:59:16,332][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 154
[2025-02-13 16:59:37,050][rgc][INFO] - 	Updating weights of batch 154
[2025-02-13 16:59:37,103][rgc][INFO] - Batch 154, avg loss per batch: 5.281072455150348
[2025-02-13 16:59:37,104][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 155
[2025-02-13 16:59:57,880][rgc][INFO] - 	Updating weights of batch 155
[2025-02-13 16:59:57,931][rgc][INFO] - Batch 155, avg loss per batch: 2.0887091765365344
[2025-02-13 16:59:57,932][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 156
[2025-02-13 17:00:18,679][rgc][INFO] - 	Updating weights of batch 156
[2025-02-13 17:00:18,733][rgc][INFO] - Batch 156, avg loss per batch: 1.5181825503000024
[2025-02-13 17:00:18,734][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 157
[2025-02-13 17:00:39,500][rgc][INFO] - 	Updating weights of batch 157
[2025-02-13 17:00:39,550][rgc][INFO] - Batch 157, avg loss per batch: 4.60911690169668
[2025-02-13 17:00:39,550][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 158
[2025-02-13 17:01:00,261][rgc][INFO] - 	Updating weights of batch 158
[2025-02-13 17:01:00,311][rgc][INFO] - Batch 158, avg loss per batch: 2.7032685503704457
[2025-02-13 17:01:00,312][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 159
[2025-02-13 17:01:21,005][rgc][INFO] - 	Updating weights of batch 159
[2025-02-13 17:01:21,054][rgc][INFO] - Batch 159, avg loss per batch: 2.633104489849573
[2025-02-13 17:01:21,055][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 160
[2025-02-13 17:01:41,817][rgc][INFO] - 	Updating weights of batch 160
[2025-02-13 17:01:41,868][rgc][INFO] - Batch 160, avg loss per batch: 3.147003140030671
[2025-02-13 17:01:41,869][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 161
[2025-02-13 17:02:02,621][rgc][INFO] - 	Updating weights of batch 161
[2025-02-13 17:02:02,684][rgc][INFO] - Batch 161, avg loss per batch: 1.8772041982044199
[2025-02-13 17:02:02,685][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 162
[2025-02-13 17:02:23,426][rgc][INFO] - 	Updating weights of batch 162
[2025-02-13 17:02:23,476][rgc][INFO] - Batch 162, avg loss per batch: 2.6097368697250896
[2025-02-13 17:02:23,477][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 163
[2025-02-13 17:02:44,146][rgc][INFO] - 	Updating weights of batch 163
[2025-02-13 17:02:44,197][rgc][INFO] - Batch 163, avg loss per batch: 2.065583246483397
[2025-02-13 17:02:44,197][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 164
[2025-02-13 17:03:04,884][rgc][INFO] - 	Updating weights of batch 164
[2025-02-13 17:03:04,936][rgc][INFO] - Batch 164, avg loss per batch: 2.0403555455900992
[2025-02-13 17:03:04,937][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 165
[2025-02-13 17:03:25,676][rgc][INFO] - 	Updating weights of batch 165
[2025-02-13 17:03:25,735][rgc][INFO] - Batch 165, avg loss per batch: 4.856283195924812
[2025-02-13 17:03:25,736][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 166
[2025-02-13 17:03:46,394][rgc][INFO] - 	Updating weights of batch 166
[2025-02-13 17:03:46,456][rgc][INFO] - Batch 166, avg loss per batch: 4.61769553902709
[2025-02-13 17:03:46,457][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 167
[2025-02-13 17:04:07,223][rgc][INFO] - 	Updating weights of batch 167
[2025-02-13 17:04:07,275][rgc][INFO] - Batch 167, avg loss per batch: 3.358029795535623
[2025-02-13 17:04:07,276][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 168
[2025-02-13 17:04:28,013][rgc][INFO] - 	Updating weights of batch 168
[2025-02-13 17:04:28,078][rgc][INFO] - Batch 168, avg loss per batch: 2.3958971202379074
[2025-02-13 17:04:28,078][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 169
[2025-02-13 17:04:48,827][rgc][INFO] - 	Updating weights of batch 169
[2025-02-13 17:04:48,891][rgc][INFO] - Batch 169, avg loss per batch: 2.17328443470252
[2025-02-13 17:04:48,892][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 170
[2025-02-13 17:05:09,560][rgc][INFO] - 	Updating weights of batch 170
[2025-02-13 17:05:09,610][rgc][INFO] - Batch 170, avg loss per batch: 1.6305942133993492
[2025-02-13 17:05:09,611][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 171
[2025-02-13 17:05:30,379][rgc][INFO] - 	Updating weights of batch 171
[2025-02-13 17:05:30,429][rgc][INFO] - Batch 171, avg loss per batch: 1.8279736379329274
[2025-02-13 17:05:30,429][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 172
[2025-02-13 17:05:51,116][rgc][INFO] - 	Updating weights of batch 172
[2025-02-13 17:05:51,169][rgc][INFO] - Batch 172, avg loss per batch: 2.5219322689396164
[2025-02-13 17:05:51,169][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 173
[2025-02-13 17:06:11,909][rgc][INFO] - 	Updating weights of batch 173
[2025-02-13 17:06:11,983][rgc][INFO] - Batch 173, avg loss per batch: 2.4172688799494644
[2025-02-13 17:06:11,983][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 174
[2025-02-13 17:06:32,717][rgc][INFO] - 	Updating weights of batch 174
[2025-02-13 17:06:32,777][rgc][INFO] - Batch 174, avg loss per batch: 2.2778073694496843
[2025-02-13 17:06:32,779][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 175
[2025-02-13 17:06:53,463][rgc][INFO] - 	Updating weights of batch 175
[2025-02-13 17:06:53,517][rgc][INFO] - Batch 175, avg loss per batch: 3.6642342450278136
[2025-02-13 17:06:53,518][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 176
[2025-02-13 17:07:14,248][rgc][INFO] - 	Updating weights of batch 176
[2025-02-13 17:07:14,300][rgc][INFO] - Batch 176, avg loss per batch: 2.776284665062888
[2025-02-13 17:07:14,300][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 177
[2025-02-13 17:07:35,018][rgc][INFO] - 	Updating weights of batch 177
[2025-02-13 17:07:35,072][rgc][INFO] - Batch 177, avg loss per batch: 2.6171773001545646
[2025-02-13 17:07:35,072][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 178
[2025-02-13 17:07:55,809][rgc][INFO] - 	Updating weights of batch 178
[2025-02-13 17:07:55,861][rgc][INFO] - Batch 178, avg loss per batch: 1.8843412577940133
[2025-02-13 17:07:55,862][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 179
[2025-02-13 17:08:16,587][rgc][INFO] - 	Updating weights of batch 179
[2025-02-13 17:08:16,640][rgc][INFO] - Batch 179, avg loss per batch: 3.7177740341946928
[2025-02-13 17:08:16,641][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 180
[2025-02-13 17:08:37,312][rgc][INFO] - 	Updating weights of batch 180
[2025-02-13 17:08:37,368][rgc][INFO] - Batch 180, avg loss per batch: 2.9019086901729767
[2025-02-13 17:08:37,369][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 181
[2025-02-13 17:08:58,033][rgc][INFO] - 	Updating weights of batch 181
[2025-02-13 17:08:58,083][rgc][INFO] - Batch 181, avg loss per batch: 2.846279862011363
[2025-02-13 17:08:58,084][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 182
[2025-02-13 17:09:18,835][rgc][INFO] - 	Updating weights of batch 182
[2025-02-13 17:09:18,886][rgc][INFO] - Batch 182, avg loss per batch: 3.5417925415648286
[2025-02-13 17:09:18,887][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 183
[2025-02-13 17:09:39,625][rgc][INFO] - 	Updating weights of batch 183
[2025-02-13 17:09:39,700][rgc][INFO] - Batch 183, avg loss per batch: 5.155454698097303
[2025-02-13 17:09:39,701][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 184
[2025-02-13 17:10:00,465][rgc][INFO] - 	Updating weights of batch 184
[2025-02-13 17:10:00,517][rgc][INFO] - Batch 184, avg loss per batch: 3.669651502983354
[2025-02-13 17:10:00,518][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 185
[2025-02-13 17:10:21,237][rgc][INFO] - 	Updating weights of batch 185
[2025-02-13 17:10:21,290][rgc][INFO] - Batch 185, avg loss per batch: 1.5888927520382683
[2025-02-13 17:10:21,291][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 186
[2025-02-13 17:10:42,031][rgc][INFO] - 	Updating weights of batch 186
[2025-02-13 17:10:42,091][rgc][INFO] - Batch 186, avg loss per batch: 5.203509424261491
[2025-02-13 17:10:42,092][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 187
[2025-02-13 17:11:02,856][rgc][INFO] - 	Updating weights of batch 187
[2025-02-13 17:11:02,905][rgc][INFO] - Batch 187, avg loss per batch: 3.3946264780712463
[2025-02-13 17:11:02,906][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 188
[2025-02-13 17:11:23,641][rgc][INFO] - 	Updating weights of batch 188
[2025-02-13 17:11:23,691][rgc][INFO] - Batch 188, avg loss per batch: 1.8316415320498605
[2025-02-13 17:11:23,693][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 189
[2025-02-13 17:11:44,419][rgc][INFO] - 	Updating weights of batch 189
[2025-02-13 17:11:44,499][rgc][INFO] - Batch 189, avg loss per batch: 2.8278168204859346
[2025-02-13 17:11:44,500][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 190
[2025-02-13 17:12:05,270][rgc][INFO] - 	Updating weights of batch 190
[2025-02-13 17:12:05,324][rgc][INFO] - Batch 190, avg loss per batch: 2.1531730508381086
[2025-02-13 17:12:05,325][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 191
[2025-02-13 17:12:26,009][rgc][INFO] - 	Updating weights of batch 191
[2025-02-13 17:12:26,076][rgc][INFO] - Batch 191, avg loss per batch: 2.4362841373304134
[2025-02-13 17:12:26,077][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 192
[2025-02-13 17:12:46,812][rgc][INFO] - 	Updating weights of batch 192
[2025-02-13 17:12:46,889][rgc][INFO] - Batch 192, avg loss per batch: 1.8980891934517203
[2025-02-13 17:12:46,890][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 193
[2025-02-13 17:13:07,642][rgc][INFO] - 	Updating weights of batch 193
[2025-02-13 17:13:07,696][rgc][INFO] - Batch 193, avg loss per batch: 1.449262397960721
[2025-02-13 17:13:07,698][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 194
[2025-02-13 17:13:28,392][rgc][INFO] - 	Updating weights of batch 194
[2025-02-13 17:13:28,444][rgc][INFO] - Batch 194, avg loss per batch: 2.150175569331033
[2025-02-13 17:13:28,445][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 195
[2025-02-13 17:13:49,185][rgc][INFO] - 	Updating weights of batch 195
[2025-02-13 17:13:49,240][rgc][INFO] - Batch 195, avg loss per batch: 3.0476096817555445
[2025-02-13 17:13:49,242][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 196
[2025-02-13 17:14:09,901][rgc][INFO] - 	Updating weights of batch 196
[2025-02-13 17:14:09,953][rgc][INFO] - Batch 196, avg loss per batch: 2.079643921485923
[2025-02-13 17:14:09,954][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 197
[2025-02-13 17:14:30,674][rgc][INFO] - 	Updating weights of batch 197
[2025-02-13 17:14:30,729][rgc][INFO] - Batch 197, avg loss per batch: 1.492590644376905
[2025-02-13 17:14:30,730][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 198
[2025-02-13 17:14:51,414][rgc][INFO] - 	Updating weights of batch 198
[2025-02-13 17:14:51,467][rgc][INFO] - Batch 198, avg loss per batch: 0.8474359057101809
[2025-02-13 17:14:51,467][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 199
[2025-02-13 17:15:12,166][rgc][INFO] - 	Updating weights of batch 199
[2025-02-13 17:15:12,220][rgc][INFO] - Batch 199, avg loss per batch: 4.5546508624950635
[2025-02-13 17:15:23,730][rgc][INFO] - AVG rho on val data: 0.2821533308309624
[2025-02-13 17:15:23,730][rgc][INFO] - AVG mae on val data: 0.5238801613489941
[2025-02-13 17:15:34,924][rgc][INFO] - AVG rho on test data: 0.31044775043514916
[2025-02-13 17:15:34,924][rgc][INFO] - AVG mae on test data: 0.5615347261457926
[2025-02-13 17:15:47,769][rgc][INFO] - AVG rho on train data: 0.22601620426719254
[2025-02-13 17:15:47,769][rgc][INFO] - AVG mae on train data: 0.5462967163065644
[2025-02-13 17:15:47,770][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 17:15:47,771][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 200
[2025-02-13 17:16:08,527][rgc][INFO] - 	Updating weights of batch 200
[2025-02-13 17:16:08,581][rgc][INFO] - Batch 200, avg loss per batch: 5.014979977877605
[2025-02-13 17:16:08,582][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 201
[2025-02-13 17:16:29,339][rgc][INFO] - 	Updating weights of batch 201
[2025-02-13 17:16:29,393][rgc][INFO] - Batch 201, avg loss per batch: 3.785220316571692
[2025-02-13 17:16:29,394][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 202
[2025-02-13 17:16:50,096][rgc][INFO] - 	Updating weights of batch 202
[2025-02-13 17:16:50,156][rgc][INFO] - Batch 202, avg loss per batch: 2.82686806903565
[2025-02-13 17:16:50,157][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 203
[2025-02-13 17:17:10,854][rgc][INFO] - 	Updating weights of batch 203
[2025-02-13 17:17:10,909][rgc][INFO] - Batch 203, avg loss per batch: 2.744972070338476
[2025-02-13 17:17:10,910][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 204
[2025-02-13 17:17:31,655][rgc][INFO] - 	Updating weights of batch 204
[2025-02-13 17:17:31,708][rgc][INFO] - Batch 204, avg loss per batch: 1.909018141298905
[2025-02-13 17:17:31,709][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 205
[2025-02-13 17:17:52,443][rgc][INFO] - 	Updating weights of batch 205
[2025-02-13 17:17:52,503][rgc][INFO] - Batch 205, avg loss per batch: 2.17602034404976
[2025-02-13 17:17:52,504][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 206
[2025-02-13 17:18:13,259][rgc][INFO] - 	Updating weights of batch 206
[2025-02-13 17:18:13,312][rgc][INFO] - Batch 206, avg loss per batch: 2.46111668497144
[2025-02-13 17:18:13,313][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 207
[2025-02-13 17:18:34,064][rgc][INFO] - 	Updating weights of batch 207
[2025-02-13 17:18:34,121][rgc][INFO] - Batch 207, avg loss per batch: 1.5651079913878339
[2025-02-13 17:18:34,121][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 208
[2025-02-13 17:18:54,808][rgc][INFO] - 	Updating weights of batch 208
[2025-02-13 17:18:54,862][rgc][INFO] - Batch 208, avg loss per batch: 2.77481636375117
[2025-02-13 17:18:54,863][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 209
[2025-02-13 17:19:15,647][rgc][INFO] - 	Updating weights of batch 209
[2025-02-13 17:19:15,710][rgc][INFO] - Batch 209, avg loss per batch: 3.72614464647933
[2025-02-13 17:19:15,711][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 210
[2025-02-13 17:19:36,410][rgc][INFO] - 	Updating weights of batch 210
[2025-02-13 17:19:36,495][rgc][INFO] - Batch 210, avg loss per batch: 2.259550354997586
[2025-02-13 17:19:36,497][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 211
[2025-02-13 17:19:57,183][rgc][INFO] - 	Updating weights of batch 211
[2025-02-13 17:19:57,237][rgc][INFO] - Batch 211, avg loss per batch: 2.5604549029578028
[2025-02-13 17:19:57,238][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 212
[2025-02-13 17:20:17,923][rgc][INFO] - 	Updating weights of batch 212
[2025-02-13 17:20:18,002][rgc][INFO] - Batch 212, avg loss per batch: 5.013983027417447
[2025-02-13 17:20:18,003][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 213
[2025-02-13 17:20:38,742][rgc][INFO] - 	Updating weights of batch 213
[2025-02-13 17:20:38,795][rgc][INFO] - Batch 213, avg loss per batch: 4.22702154192183
[2025-02-13 17:20:38,796][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 214
[2025-02-13 17:20:59,486][rgc][INFO] - 	Updating weights of batch 214
[2025-02-13 17:20:59,536][rgc][INFO] - Batch 214, avg loss per batch: 4.21498930750065
[2025-02-13 17:20:59,537][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 215
[2025-02-13 17:21:20,280][rgc][INFO] - 	Updating weights of batch 215
[2025-02-13 17:21:20,360][rgc][INFO] - Batch 215, avg loss per batch: 3.0778998736669276
[2025-02-13 17:21:20,361][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 216
[2025-02-13 17:21:41,045][rgc][INFO] - 	Updating weights of batch 216
[2025-02-13 17:21:41,099][rgc][INFO] - Batch 216, avg loss per batch: 2.1231054261917675
[2025-02-13 17:21:41,100][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 217
[2025-02-13 17:22:01,793][rgc][INFO] - 	Updating weights of batch 217
[2025-02-13 17:22:01,845][rgc][INFO] - Batch 217, avg loss per batch: 2.2668850580095175
[2025-02-13 17:22:01,846][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 218
[2025-02-13 17:22:22,529][rgc][INFO] - 	Updating weights of batch 218
[2025-02-13 17:22:22,578][rgc][INFO] - Batch 218, avg loss per batch: 4.1520989981151635
[2025-02-13 17:22:22,579][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 219
[2025-02-13 17:22:43,266][rgc][INFO] - 	Updating weights of batch 219
[2025-02-13 17:22:43,318][rgc][INFO] - Batch 219, avg loss per batch: 3.259488874072092
[2025-02-13 17:22:43,319][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 220
[2025-02-13 17:23:04,084][rgc][INFO] - 	Updating weights of batch 220
[2025-02-13 17:23:04,137][rgc][INFO] - Batch 220, avg loss per batch: 2.0465008067985373
[2025-02-13 17:23:04,137][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 221
[2025-02-13 17:23:24,871][rgc][INFO] - 	Updating weights of batch 221
[2025-02-13 17:23:24,923][rgc][INFO] - Batch 221, avg loss per batch: 3.5499290684071685
[2025-02-13 17:23:24,924][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 222
[2025-02-13 17:23:45,645][rgc][INFO] - 	Updating weights of batch 222
[2025-02-13 17:23:45,695][rgc][INFO] - Batch 222, avg loss per batch: 1.466410115293742
[2025-02-13 17:23:45,696][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 223
[2025-02-13 17:24:06,457][rgc][INFO] - 	Updating weights of batch 223
[2025-02-13 17:24:06,507][rgc][INFO] - Batch 223, avg loss per batch: 2.4880660554655964
[2025-02-13 17:24:06,507][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 224
[2025-02-13 17:24:27,251][rgc][INFO] - 	Updating weights of batch 224
[2025-02-13 17:24:27,301][rgc][INFO] - Batch 224, avg loss per batch: 1.6726605851367864
[2025-02-13 17:24:27,302][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 225
[2025-02-13 17:24:47,977][rgc][INFO] - 	Updating weights of batch 225
[2025-02-13 17:24:48,028][rgc][INFO] - Batch 225, avg loss per batch: 3.20161535694679
[2025-02-13 17:24:48,029][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 226
[2025-02-13 17:25:08,797][rgc][INFO] - 	Updating weights of batch 226
[2025-02-13 17:25:08,848][rgc][INFO] - Batch 226, avg loss per batch: 3.548482700003669
[2025-02-13 17:25:08,849][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 227
[2025-02-13 17:25:29,587][rgc][INFO] - 	Updating weights of batch 227
[2025-02-13 17:25:29,640][rgc][INFO] - Batch 227, avg loss per batch: 4.690161634796597
[2025-02-13 17:25:29,640][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 228
[2025-02-13 17:25:50,380][rgc][INFO] - 	Updating weights of batch 228
[2025-02-13 17:25:50,460][rgc][INFO] - Batch 228, avg loss per batch: 2.7414995441318
[2025-02-13 17:25:50,461][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 229
[2025-02-13 17:26:11,194][rgc][INFO] - 	Updating weights of batch 229
[2025-02-13 17:26:11,250][rgc][INFO] - Batch 229, avg loss per batch: 2.8531588096679132
[2025-02-13 17:26:11,251][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 230
[2025-02-13 17:26:31,973][rgc][INFO] - 	Updating weights of batch 230
[2025-02-13 17:26:32,025][rgc][INFO] - Batch 230, avg loss per batch: 1.8594130451766664
[2025-02-13 17:26:32,025][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 231
[2025-02-13 17:26:52,744][rgc][INFO] - 	Updating weights of batch 231
[2025-02-13 17:26:52,797][rgc][INFO] - Batch 231, avg loss per batch: 3.4349376259746354
[2025-02-13 17:26:52,798][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 232
[2025-02-13 17:27:13,517][rgc][INFO] - 	Updating weights of batch 232
[2025-02-13 17:27:13,571][rgc][INFO] - Batch 232, avg loss per batch: 2.641220768951844
[2025-02-13 17:27:13,572][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 233
[2025-02-13 17:27:34,250][rgc][INFO] - 	Updating weights of batch 233
[2025-02-13 17:27:34,304][rgc][INFO] - Batch 233, avg loss per batch: 4.222987539464084
[2025-02-13 17:27:34,305][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 234
[2025-02-13 17:27:55,036][rgc][INFO] - 	Updating weights of batch 234
[2025-02-13 17:27:55,092][rgc][INFO] - Batch 234, avg loss per batch: 1.8345938699552455
[2025-02-13 17:27:55,092][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 235
[2025-02-13 17:28:15,780][rgc][INFO] - 	Updating weights of batch 235
[2025-02-13 17:28:15,831][rgc][INFO] - Batch 235, avg loss per batch: 3.077879068478145
[2025-02-13 17:28:15,832][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 236
[2025-02-13 17:28:36,527][rgc][INFO] - 	Updating weights of batch 236
[2025-02-13 17:28:36,579][rgc][INFO] - Batch 236, avg loss per batch: 5.46487116838746
[2025-02-13 17:28:36,580][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 237
[2025-02-13 17:28:57,314][rgc][INFO] - 	Updating weights of batch 237
[2025-02-13 17:28:57,371][rgc][INFO] - Batch 237, avg loss per batch: 1.6100349721193181
[2025-02-13 17:28:57,372][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 238
[2025-02-13 17:29:18,150][rgc][INFO] - 	Updating weights of batch 238
[2025-02-13 17:29:18,206][rgc][INFO] - Batch 238, avg loss per batch: 3.984228693266226
[2025-02-13 17:29:18,207][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 239
[2025-02-13 17:29:38,874][rgc][INFO] - 	Updating weights of batch 239
[2025-02-13 17:29:38,927][rgc][INFO] - Batch 239, avg loss per batch: 1.9148912700552734
[2025-02-13 17:29:38,928][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 240
[2025-02-13 17:29:59,654][rgc][INFO] - 	Updating weights of batch 240
[2025-02-13 17:29:59,717][rgc][INFO] - Batch 240, avg loss per batch: 3.9608755286426924
[2025-02-13 17:29:59,718][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 241
[2025-02-13 17:30:20,405][rgc][INFO] - 	Updating weights of batch 241
[2025-02-13 17:30:20,476][rgc][INFO] - Batch 241, avg loss per batch: 3.3998488308008072
[2025-02-13 17:30:20,477][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 242
[2025-02-13 17:30:41,227][rgc][INFO] - 	Updating weights of batch 242
[2025-02-13 17:30:41,279][rgc][INFO] - Batch 242, avg loss per batch: 3.176818546918317
[2025-02-13 17:30:41,280][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 243
[2025-02-13 17:31:01,997][rgc][INFO] - 	Updating weights of batch 243
[2025-02-13 17:31:02,074][rgc][INFO] - Batch 243, avg loss per batch: 2.0029246686122644
[2025-02-13 17:31:02,076][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 244
[2025-02-13 17:31:22,758][rgc][INFO] - 	Updating weights of batch 244
[2025-02-13 17:31:22,813][rgc][INFO] - Batch 244, avg loss per batch: 2.4642129184047095
[2025-02-13 17:31:22,814][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 245
[2025-02-13 17:31:43,536][rgc][INFO] - 	Updating weights of batch 245
[2025-02-13 17:31:43,602][rgc][INFO] - Batch 245, avg loss per batch: 2.4769643481108554
[2025-02-13 17:31:43,604][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 246
[2025-02-13 17:32:04,283][rgc][INFO] - 	Updating weights of batch 246
[2025-02-13 17:32:04,337][rgc][INFO] - Batch 246, avg loss per batch: 1.902584262965402
[2025-02-13 17:32:04,338][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 247
[2025-02-13 17:32:25,005][rgc][INFO] - 	Updating weights of batch 247
[2025-02-13 17:32:25,059][rgc][INFO] - Batch 247, avg loss per batch: 2.9871252219495767
[2025-02-13 17:32:25,060][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 248
[2025-02-13 17:32:45,809][rgc][INFO] - 	Updating weights of batch 248
[2025-02-13 17:32:45,870][rgc][INFO] - Batch 248, avg loss per batch: 2.531648458928062
[2025-02-13 17:32:45,871][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 249
[2025-02-13 17:33:06,639][rgc][INFO] - 	Updating weights of batch 249
[2025-02-13 17:33:06,696][rgc][INFO] - Batch 249, avg loss per batch: 3.2041125693347285
[2025-02-13 17:33:06,696][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 250
[2025-02-13 17:33:27,382][rgc][INFO] - 	Updating weights of batch 250
[2025-02-13 17:33:27,440][rgc][INFO] - Batch 250, avg loss per batch: 2.854079958072117
[2025-02-13 17:33:27,441][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 251
[2025-02-13 17:33:48,182][rgc][INFO] - 	Updating weights of batch 251
[2025-02-13 17:33:48,238][rgc][INFO] - Batch 251, avg loss per batch: 2.5283529960394673
[2025-02-13 17:33:48,239][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 252
[2025-02-13 17:34:08,980][rgc][INFO] - 	Updating weights of batch 252
[2025-02-13 17:34:09,033][rgc][INFO] - Batch 252, avg loss per batch: 1.9440601916744207
[2025-02-13 17:34:09,035][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 253
[2025-02-13 17:34:29,770][rgc][INFO] - 	Updating weights of batch 253
[2025-02-13 17:34:29,849][rgc][INFO] - Batch 253, avg loss per batch: 3.4644298171563963
[2025-02-13 17:34:29,850][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 254
[2025-02-13 17:34:50,516][rgc][INFO] - 	Updating weights of batch 254
[2025-02-13 17:34:50,571][rgc][INFO] - Batch 254, avg loss per batch: 3.2234361270825937
[2025-02-13 17:34:50,573][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 255
[2025-02-13 17:35:11,310][rgc][INFO] - 	Updating weights of batch 255
[2025-02-13 17:35:11,361][rgc][INFO] - Batch 255, avg loss per batch: 1.6634570858569593
[2025-02-13 17:35:11,362][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 256
[2025-02-13 17:35:32,069][rgc][INFO] - 	Updating weights of batch 256
[2025-02-13 17:35:32,132][rgc][INFO] - Batch 256, avg loss per batch: 2.9104744420365045
[2025-02-13 17:35:32,133][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 257
[2025-02-13 17:35:52,823][rgc][INFO] - 	Updating weights of batch 257
[2025-02-13 17:35:52,875][rgc][INFO] - Batch 257, avg loss per batch: 6.986033870917722
[2025-02-13 17:35:52,876][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 258
[2025-02-13 17:36:13,615][rgc][INFO] - 	Updating weights of batch 258
[2025-02-13 17:36:13,664][rgc][INFO] - Batch 258, avg loss per batch: 2.8270523166815433
[2025-02-13 17:36:13,665][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 259
[2025-02-13 17:36:34,330][rgc][INFO] - 	Updating weights of batch 259
[2025-02-13 17:36:34,381][rgc][INFO] - Batch 259, avg loss per batch: 2.7838950545992773
[2025-02-13 17:36:34,381][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 260
[2025-02-13 17:36:55,072][rgc][INFO] - 	Updating weights of batch 260
[2025-02-13 17:36:55,137][rgc][INFO] - Batch 260, avg loss per batch: 3.678492830845183
[2025-02-13 17:36:55,138][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 261
[2025-02-13 17:37:15,873][rgc][INFO] - 	Updating weights of batch 261
[2025-02-13 17:37:15,929][rgc][INFO] - Batch 261, avg loss per batch: 2.3811334722006583
[2025-02-13 17:37:15,929][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 262
[2025-02-13 17:37:36,675][rgc][INFO] - 	Updating weights of batch 262
[2025-02-13 17:37:36,729][rgc][INFO] - Batch 262, avg loss per batch: 3.9592889857684646
[2025-02-13 17:37:36,730][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 263
[2025-02-13 17:37:57,477][rgc][INFO] - 	Updating weights of batch 263
[2025-02-13 17:37:57,531][rgc][INFO] - Batch 263, avg loss per batch: 1.5329124837769739
[2025-02-13 17:37:57,532][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 264
[2025-02-13 17:38:18,226][rgc][INFO] - 	Updating weights of batch 264
[2025-02-13 17:38:18,282][rgc][INFO] - Batch 264, avg loss per batch: 2.456959847122058
[2025-02-13 17:38:18,283][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 265
[2025-02-13 17:38:39,010][rgc][INFO] - 	Updating weights of batch 265
[2025-02-13 17:38:39,063][rgc][INFO] - Batch 265, avg loss per batch: 1.6590754771084442
[2025-02-13 17:38:39,064][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 266
[2025-02-13 17:38:59,741][rgc][INFO] - 	Updating weights of batch 266
[2025-02-13 17:38:59,794][rgc][INFO] - Batch 266, avg loss per batch: 2.8702751501456385
[2025-02-13 17:38:59,794][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 267
[2025-02-13 17:39:20,480][rgc][INFO] - 	Updating weights of batch 267
[2025-02-13 17:39:20,534][rgc][INFO] - Batch 267, avg loss per batch: 1.7946514134892222
[2025-02-13 17:39:20,534][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 268
[2025-02-13 17:39:41,228][rgc][INFO] - 	Updating weights of batch 268
[2025-02-13 17:39:41,280][rgc][INFO] - Batch 268, avg loss per batch: 3.393947344631421
[2025-02-13 17:39:41,281][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 269
[2025-02-13 17:40:02,025][rgc][INFO] - 	Updating weights of batch 269
[2025-02-13 17:40:02,082][rgc][INFO] - Batch 269, avg loss per batch: 2.748784552736747
[2025-02-13 17:40:02,083][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 270
[2025-02-13 17:40:22,812][rgc][INFO] - 	Updating weights of batch 270
[2025-02-13 17:40:22,867][rgc][INFO] - Batch 270, avg loss per batch: 3.332122022118682
[2025-02-13 17:40:22,867][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 271
[2025-02-13 17:40:43,535][rgc][INFO] - 	Updating weights of batch 271
[2025-02-13 17:40:43,593][rgc][INFO] - Batch 271, avg loss per batch: 4.016512441393504
[2025-02-13 17:40:43,594][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 272
[2025-02-13 17:41:04,334][rgc][INFO] - 	Updating weights of batch 272
[2025-02-13 17:41:04,398][rgc][INFO] - Batch 272, avg loss per batch: 2.296062810451327
[2025-02-13 17:41:04,399][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 273
[2025-02-13 17:41:25,149][rgc][INFO] - 	Updating weights of batch 273
[2025-02-13 17:41:25,206][rgc][INFO] - Batch 273, avg loss per batch: 1.9286759728878105
[2025-02-13 17:41:25,207][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 274
[2025-02-13 17:41:45,978][rgc][INFO] - 	Updating weights of batch 274
[2025-02-13 17:41:46,034][rgc][INFO] - Batch 274, avg loss per batch: 2.0978778914996106
[2025-02-13 17:41:46,035][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 275
[2025-02-13 17:42:06,756][rgc][INFO] - 	Updating weights of batch 275
[2025-02-13 17:42:06,811][rgc][INFO] - Batch 275, avg loss per batch: 2.339605317432885
[2025-02-13 17:42:06,812][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 276
[2025-02-13 17:42:27,539][rgc][INFO] - 	Updating weights of batch 276
[2025-02-13 17:42:27,588][rgc][INFO] - Batch 276, avg loss per batch: 1.1382251335676536
[2025-02-13 17:42:27,589][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 277
[2025-02-13 17:42:48,328][rgc][INFO] - 	Updating weights of batch 277
[2025-02-13 17:42:48,403][rgc][INFO] - Batch 277, avg loss per batch: 2.0266789987216725
[2025-02-13 17:42:48,404][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 278
[2025-02-13 17:43:09,155][rgc][INFO] - 	Updating weights of batch 278
[2025-02-13 17:43:09,206][rgc][INFO] - Batch 278, avg loss per batch: 2.2715366530644694
[2025-02-13 17:43:09,206][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 279
[2025-02-13 17:43:29,901][rgc][INFO] - 	Updating weights of batch 279
[2025-02-13 17:43:29,966][rgc][INFO] - Batch 279, avg loss per batch: 2.965478631866192
[2025-02-13 17:43:29,967][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 280
[2025-02-13 17:43:50,683][rgc][INFO] - 	Updating weights of batch 280
[2025-02-13 17:43:50,735][rgc][INFO] - Batch 280, avg loss per batch: 2.798610187762903
[2025-02-13 17:43:50,736][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 281
[2025-02-13 17:44:11,461][rgc][INFO] - 	Updating weights of batch 281
[2025-02-13 17:44:11,515][rgc][INFO] - Batch 281, avg loss per batch: 2.844299639486067
[2025-02-13 17:44:11,516][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 282
[2025-02-13 17:44:32,248][rgc][INFO] - 	Updating weights of batch 282
[2025-02-13 17:44:32,302][rgc][INFO] - Batch 282, avg loss per batch: 4.161912773312012
[2025-02-13 17:44:32,303][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 283
[2025-02-13 17:44:53,065][rgc][INFO] - 	Updating weights of batch 283
[2025-02-13 17:44:53,118][rgc][INFO] - Batch 283, avg loss per batch: 2.988559834402381
[2025-02-13 17:44:53,119][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 284
[2025-02-13 17:45:13,842][rgc][INFO] - 	Updating weights of batch 284
[2025-02-13 17:45:13,923][rgc][INFO] - Batch 284, avg loss per batch: 1.6016726111336583
[2025-02-13 17:45:13,924][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 285
[2025-02-13 17:45:34,662][rgc][INFO] - 	Updating weights of batch 285
[2025-02-13 17:45:34,713][rgc][INFO] - Batch 285, avg loss per batch: 2.4899949346125956
[2025-02-13 17:45:34,714][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 286
[2025-02-13 17:45:55,402][rgc][INFO] - 	Updating weights of batch 286
[2025-02-13 17:45:55,454][rgc][INFO] - Batch 286, avg loss per batch: 2.2802256149830162
[2025-02-13 17:45:55,456][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 287
[2025-02-13 17:46:16,117][rgc][INFO] - 	Updating weights of batch 287
[2025-02-13 17:46:16,165][rgc][INFO] - Batch 287, avg loss per batch: 1.997485826133049
[2025-02-13 17:46:16,166][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 288
[2025-02-13 17:46:36,841][rgc][INFO] - 	Updating weights of batch 288
[2025-02-13 17:46:36,891][rgc][INFO] - Batch 288, avg loss per batch: 2.9159260865154253
[2025-02-13 17:46:36,892][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 289
[2025-02-13 17:46:57,550][rgc][INFO] - 	Updating weights of batch 289
[2025-02-13 17:46:57,604][rgc][INFO] - Batch 289, avg loss per batch: 3.113006477744973
[2025-02-13 17:46:57,605][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 290
[2025-02-13 17:47:18,284][rgc][INFO] - 	Updating weights of batch 290
[2025-02-13 17:47:18,333][rgc][INFO] - Batch 290, avg loss per batch: 1.136624471527968
[2025-02-13 17:47:18,334][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 291
[2025-02-13 17:47:39,055][rgc][INFO] - 	Updating weights of batch 291
[2025-02-13 17:47:39,106][rgc][INFO] - Batch 291, avg loss per batch: 4.563909837388024
[2025-02-13 17:47:39,107][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 292
[2025-02-13 17:47:59,789][rgc][INFO] - 	Updating weights of batch 292
[2025-02-13 17:47:59,843][rgc][INFO] - Batch 292, avg loss per batch: 2.539681275748938
[2025-02-13 17:47:59,843][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 293
[2025-02-13 17:48:20,541][rgc][INFO] - 	Updating weights of batch 293
[2025-02-13 17:48:20,592][rgc][INFO] - Batch 293, avg loss per batch: 1.4641977693032766
[2025-02-13 17:48:20,593][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 294
[2025-02-13 17:48:41,294][rgc][INFO] - 	Updating weights of batch 294
[2025-02-13 17:48:41,348][rgc][INFO] - Batch 294, avg loss per batch: 3.006915595928971
[2025-02-13 17:48:41,348][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 295
[2025-02-13 17:49:02,109][rgc][INFO] - 	Updating weights of batch 295
[2025-02-13 17:49:02,161][rgc][INFO] - Batch 295, avg loss per batch: 2.5301401604400646
[2025-02-13 17:49:02,162][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 296
[2025-02-13 17:49:22,846][rgc][INFO] - 	Updating weights of batch 296
[2025-02-13 17:49:22,905][rgc][INFO] - Batch 296, avg loss per batch: 2.78419929798063
[2025-02-13 17:49:22,906][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 297
[2025-02-13 17:49:43,655][rgc][INFO] - 	Updating weights of batch 297
[2025-02-13 17:49:43,712][rgc][INFO] - Batch 297, avg loss per batch: 3.115401595824954
[2025-02-13 17:49:43,713][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 298
[2025-02-13 17:50:04,490][rgc][INFO] - 	Updating weights of batch 298
[2025-02-13 17:50:04,542][rgc][INFO] - Batch 298, avg loss per batch: 3.193143785135589
[2025-02-13 17:50:04,543][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 299
[2025-02-13 17:50:25,292][rgc][INFO] - 	Updating weights of batch 299
[2025-02-13 17:50:25,345][rgc][INFO] - Batch 299, avg loss per batch: 2.0392225931303187
[2025-02-13 17:50:36,870][rgc][INFO] - AVG rho on val data: 0.277404895157498
[2025-02-13 17:50:36,870][rgc][INFO] - AVG mae on val data: 0.5259036512180749
[2025-02-13 17:50:48,074][rgc][INFO] - AVG rho on test data: 0.30554481579304527
[2025-02-13 17:50:48,075][rgc][INFO] - AVG mae on test data: 0.5594272451191191
[2025-02-13 17:51:00,913][rgc][INFO] - AVG rho on train data: 0.2222151249524848
[2025-02-13 17:51:00,913][rgc][INFO] - AVG mae on train data: 0.5475763554072199
[2025-02-13 17:51:00,915][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 17:51:00,920][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 300
[2025-02-13 17:51:21,682][rgc][INFO] - 	Updating weights of batch 300
[2025-02-13 17:51:21,742][rgc][INFO] - Batch 300, avg loss per batch: 2.61562449179686
[2025-02-13 17:51:21,742][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 301
[2025-02-13 17:51:42,483][rgc][INFO] - 	Updating weights of batch 301
[2025-02-13 17:51:42,537][rgc][INFO] - Batch 301, avg loss per batch: 2.280811791533692
[2025-02-13 17:51:42,538][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 302
[2025-02-13 17:52:03,214][rgc][INFO] - 	Updating weights of batch 302
[2025-02-13 17:52:03,265][rgc][INFO] - Batch 302, avg loss per batch: 3.165822095630544
[2025-02-13 17:52:03,266][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 303
[2025-02-13 17:52:23,940][rgc][INFO] - 	Updating weights of batch 303
[2025-02-13 17:52:24,023][rgc][INFO] - Batch 303, avg loss per batch: 1.6725477265948028
[2025-02-13 17:52:24,024][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 304
[2025-02-13 17:52:44,753][rgc][INFO] - 	Updating weights of batch 304
[2025-02-13 17:52:44,806][rgc][INFO] - Batch 304, avg loss per batch: 2.356048600211744
[2025-02-13 17:52:44,807][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 305
[2025-02-13 17:53:05,510][rgc][INFO] - 	Updating weights of batch 305
[2025-02-13 17:53:05,559][rgc][INFO] - Batch 305, avg loss per batch: 3.1800967762359478
[2025-02-13 17:53:05,560][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 306
[2025-02-13 17:53:26,302][rgc][INFO] - 	Updating weights of batch 306
[2025-02-13 17:53:26,360][rgc][INFO] - Batch 306, avg loss per batch: 2.670341453597533
[2025-02-13 17:53:26,360][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 307
[2025-02-13 17:53:47,106][rgc][INFO] - 	Updating weights of batch 307
[2025-02-13 17:53:47,188][rgc][INFO] - Batch 307, avg loss per batch: 2.9912157072869543
[2025-02-13 17:53:47,190][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 308
[2025-02-13 17:54:07,923][rgc][INFO] - 	Updating weights of batch 308
[2025-02-13 17:54:07,976][rgc][INFO] - Batch 308, avg loss per batch: 3.106648563495243
[2025-02-13 17:54:07,977][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 309
[2025-02-13 17:54:28,665][rgc][INFO] - 	Updating weights of batch 309
[2025-02-13 17:54:28,717][rgc][INFO] - Batch 309, avg loss per batch: 3.764603029273315
[2025-02-13 17:54:28,718][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 310
[2025-02-13 17:54:49,420][rgc][INFO] - 	Updating weights of batch 310
[2025-02-13 17:54:49,475][rgc][INFO] - Batch 310, avg loss per batch: 3.1408387852225506
[2025-02-13 17:54:49,476][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 311
[2025-02-13 17:55:10,211][rgc][INFO] - 	Updating weights of batch 311
[2025-02-13 17:55:10,268][rgc][INFO] - Batch 311, avg loss per batch: 2.9739996610467747
[2025-02-13 17:55:10,269][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 312
[2025-02-13 17:55:30,957][rgc][INFO] - 	Updating weights of batch 312
[2025-02-13 17:55:31,009][rgc][INFO] - Batch 312, avg loss per batch: 2.9208500698956366
[2025-02-13 17:55:31,010][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 313
[2025-02-13 17:55:51,680][rgc][INFO] - 	Updating weights of batch 313
[2025-02-13 17:55:51,731][rgc][INFO] - Batch 313, avg loss per batch: 2.274232430216459
[2025-02-13 17:55:51,732][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 314
[2025-02-13 17:56:12,472][rgc][INFO] - 	Updating weights of batch 314
[2025-02-13 17:56:12,523][rgc][INFO] - Batch 314, avg loss per batch: 2.7226065427985393
[2025-02-13 17:56:12,524][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 315
[2025-02-13 17:56:33,264][rgc][INFO] - 	Updating weights of batch 315
[2025-02-13 17:56:33,318][rgc][INFO] - Batch 315, avg loss per batch: 3.181277297870681
[2025-02-13 17:56:33,319][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 316
[2025-02-13 17:56:54,069][rgc][INFO] - 	Updating weights of batch 316
[2025-02-13 17:56:54,123][rgc][INFO] - Batch 316, avg loss per batch: 2.126131172563524
[2025-02-13 17:56:54,124][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 317
[2025-02-13 17:57:14,891][rgc][INFO] - 	Updating weights of batch 317
[2025-02-13 17:57:14,953][rgc][INFO] - Batch 317, avg loss per batch: 2.472755168974249
[2025-02-13 17:57:14,953][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 318
[2025-02-13 17:57:35,701][rgc][INFO] - 	Updating weights of batch 318
[2025-02-13 17:57:35,754][rgc][INFO] - Batch 318, avg loss per batch: 2.024583573507821
[2025-02-13 17:57:35,755][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 319
[2025-02-13 17:57:56,450][rgc][INFO] - 	Updating weights of batch 319
[2025-02-13 17:57:56,506][rgc][INFO] - Batch 319, avg loss per batch: 2.329080158414924
[2025-02-13 17:57:56,507][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 320
[2025-02-13 17:58:17,247][rgc][INFO] - 	Updating weights of batch 320
[2025-02-13 17:58:17,306][rgc][INFO] - Batch 320, avg loss per batch: 1.4068409442317944
[2025-02-13 17:58:17,307][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 321
[2025-02-13 17:58:38,050][rgc][INFO] - 	Updating weights of batch 321
[2025-02-13 17:58:38,115][rgc][INFO] - Batch 321, avg loss per batch: 4.696276114522099
[2025-02-13 17:58:38,116][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 322
[2025-02-13 17:58:58,865][rgc][INFO] - 	Updating weights of batch 322
[2025-02-13 17:58:58,916][rgc][INFO] - Batch 322, avg loss per batch: 3.765428243043588
[2025-02-13 17:58:58,917][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 323
[2025-02-13 17:59:19,595][rgc][INFO] - 	Updating weights of batch 323
[2025-02-13 17:59:19,645][rgc][INFO] - Batch 323, avg loss per batch: 3.009849829294644
[2025-02-13 17:59:19,646][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 324
[2025-02-13 17:59:40,410][rgc][INFO] - 	Updating weights of batch 324
[2025-02-13 17:59:40,486][rgc][INFO] - Batch 324, avg loss per batch: 3.621656571248478
[2025-02-13 17:59:40,487][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 325
[2025-02-13 18:00:01,162][rgc][INFO] - 	Updating weights of batch 325
[2025-02-13 18:00:01,220][rgc][INFO] - Batch 325, avg loss per batch: 2.427252339319004
[2025-02-13 18:00:01,222][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 326
[2025-02-13 18:00:21,879][rgc][INFO] - 	Updating weights of batch 326
[2025-02-13 18:00:21,934][rgc][INFO] - Batch 326, avg loss per batch: 1.7263832519039544
[2025-02-13 18:00:21,935][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 327
[2025-02-13 18:00:42,608][rgc][INFO] - 	Updating weights of batch 327
[2025-02-13 18:00:42,660][rgc][INFO] - Batch 327, avg loss per batch: 2.127757170866647
[2025-02-13 18:00:42,660][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 328
[2025-02-13 18:01:03,404][rgc][INFO] - 	Updating weights of batch 328
[2025-02-13 18:01:03,465][rgc][INFO] - Batch 328, avg loss per batch: 1.670471388273258
[2025-02-13 18:01:03,466][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 329
[2025-02-13 18:01:24,152][rgc][INFO] - 	Updating weights of batch 329
[2025-02-13 18:01:24,208][rgc][INFO] - Batch 329, avg loss per batch: 3.1290152595033627
[2025-02-13 18:01:24,210][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 330
[2025-02-13 18:01:44,907][rgc][INFO] - 	Updating weights of batch 330
[2025-02-13 18:01:44,957][rgc][INFO] - Batch 330, avg loss per batch: 2.172806695684339
[2025-02-13 18:01:44,958][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 331
[2025-02-13 18:02:05,690][rgc][INFO] - 	Updating weights of batch 331
[2025-02-13 18:02:05,743][rgc][INFO] - Batch 331, avg loss per batch: 1.395364684726876
[2025-02-13 18:02:05,744][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 332
[2025-02-13 18:02:26,493][rgc][INFO] - 	Updating weights of batch 332
[2025-02-13 18:02:26,546][rgc][INFO] - Batch 332, avg loss per batch: 2.3662586618211257
[2025-02-13 18:02:26,547][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 333
[2025-02-13 18:02:47,300][rgc][INFO] - 	Updating weights of batch 333
[2025-02-13 18:02:47,351][rgc][INFO] - Batch 333, avg loss per batch: 2.460024004804607
[2025-02-13 18:02:47,352][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 334
[2025-02-13 18:03:08,116][rgc][INFO] - 	Updating weights of batch 334
[2025-02-13 18:03:08,169][rgc][INFO] - Batch 334, avg loss per batch: 3.008052861251848
[2025-02-13 18:03:08,170][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 335
[2025-02-13 18:03:28,863][rgc][INFO] - 	Updating weights of batch 335
[2025-02-13 18:03:28,918][rgc][INFO] - Batch 335, avg loss per batch: 3.371187396170157
[2025-02-13 18:03:28,920][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 336
[2025-02-13 18:03:49,649][rgc][INFO] - 	Updating weights of batch 336
[2025-02-13 18:03:49,705][rgc][INFO] - Batch 336, avg loss per batch: 2.648113328053502
[2025-02-13 18:03:49,706][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 337
[2025-02-13 18:04:10,371][rgc][INFO] - 	Updating weights of batch 337
[2025-02-13 18:04:10,429][rgc][INFO] - Batch 337, avg loss per batch: 3.1385528170704458
[2025-02-13 18:04:10,430][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 338
[2025-02-13 18:04:31,165][rgc][INFO] - 	Updating weights of batch 338
[2025-02-13 18:04:31,230][rgc][INFO] - Batch 338, avg loss per batch: 3.354455187276721
[2025-02-13 18:04:31,231][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 339
[2025-02-13 18:04:51,905][rgc][INFO] - 	Updating weights of batch 339
[2025-02-13 18:04:51,958][rgc][INFO] - Batch 339, avg loss per batch: 2.7591650605848184
[2025-02-13 18:04:51,959][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 340
[2025-02-13 18:05:12,680][rgc][INFO] - 	Updating weights of batch 340
[2025-02-13 18:05:12,737][rgc][INFO] - Batch 340, avg loss per batch: 3.558294537298919
[2025-02-13 18:05:12,738][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 341
[2025-02-13 18:05:33,503][rgc][INFO] - 	Updating weights of batch 341
[2025-02-13 18:05:33,556][rgc][INFO] - Batch 341, avg loss per batch: 2.6892867873543302
[2025-02-13 18:05:33,557][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 342
[2025-02-13 18:05:54,310][rgc][INFO] - 	Updating weights of batch 342
[2025-02-13 18:05:54,372][rgc][INFO] - Batch 342, avg loss per batch: 1.718109589274479
[2025-02-13 18:05:54,373][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 343
[2025-02-13 18:06:15,125][rgc][INFO] - 	Updating weights of batch 343
[2025-02-13 18:06:15,182][rgc][INFO] - Batch 343, avg loss per batch: 2.659319957871244
[2025-02-13 18:06:15,182][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 344
[2025-02-13 18:06:35,920][rgc][INFO] - 	Updating weights of batch 344
[2025-02-13 18:06:35,975][rgc][INFO] - Batch 344, avg loss per batch: 1.1989702946702347
[2025-02-13 18:06:35,976][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 345
[2025-02-13 18:06:56,715][rgc][INFO] - 	Updating weights of batch 345
[2025-02-13 18:06:56,774][rgc][INFO] - Batch 345, avg loss per batch: 1.8566054534792684
[2025-02-13 18:06:56,774][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 346
[2025-02-13 18:07:17,457][rgc][INFO] - 	Updating weights of batch 346
[2025-02-13 18:07:17,510][rgc][INFO] - Batch 346, avg loss per batch: 3.460329229866736
[2025-02-13 18:07:17,510][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 347
[2025-02-13 18:07:38,270][rgc][INFO] - 	Updating weights of batch 347
[2025-02-13 18:07:38,321][rgc][INFO] - Batch 347, avg loss per batch: 2.4177465020685216
[2025-02-13 18:07:38,322][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 348
[2025-02-13 18:07:59,063][rgc][INFO] - 	Updating weights of batch 348
[2025-02-13 18:07:59,112][rgc][INFO] - Batch 348, avg loss per batch: 1.2577931841749868
[2025-02-13 18:07:59,113][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 349
[2025-02-13 18:08:19,786][rgc][INFO] - 	Updating weights of batch 349
[2025-02-13 18:08:19,842][rgc][INFO] - Batch 349, avg loss per batch: 2.175625606430429
[2025-02-13 18:08:19,843][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 350
[2025-02-13 18:08:40,585][rgc][INFO] - 	Updating weights of batch 350
[2025-02-13 18:08:40,641][rgc][INFO] - Batch 350, avg loss per batch: 2.5949855637308135
[2025-02-13 18:08:40,642][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 351
[2025-02-13 18:09:01,398][rgc][INFO] - 	Updating weights of batch 351
[2025-02-13 18:09:01,449][rgc][INFO] - Batch 351, avg loss per batch: 3.6255175479856674
[2025-02-13 18:09:01,450][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 352
[2025-02-13 18:09:22,212][rgc][INFO] - 	Updating weights of batch 352
[2025-02-13 18:09:22,268][rgc][INFO] - Batch 352, avg loss per batch: 3.7633564796993895
[2025-02-13 18:09:22,269][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 353
[2025-02-13 18:09:43,005][rgc][INFO] - 	Updating weights of batch 353
[2025-02-13 18:09:43,058][rgc][INFO] - Batch 353, avg loss per batch: 1.9098918826685498
[2025-02-13 18:09:43,058][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 354
[2025-02-13 18:10:03,808][rgc][INFO] - 	Updating weights of batch 354
[2025-02-13 18:10:03,883][rgc][INFO] - Batch 354, avg loss per batch: 3.094653664390575
[2025-02-13 18:10:03,885][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 355
[2025-02-13 18:10:24,546][rgc][INFO] - 	Updating weights of batch 355
[2025-02-13 18:10:24,603][rgc][INFO] - Batch 355, avg loss per batch: 2.308463666079116
[2025-02-13 18:10:24,604][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 356
[2025-02-13 18:10:45,333][rgc][INFO] - 	Updating weights of batch 356
[2025-02-13 18:10:45,381][rgc][INFO] - Batch 356, avg loss per batch: 2.428383851143327
[2025-02-13 18:10:45,381][rgc][INFO] - 	Applying batch grad function of epoch 3 and batch 357
[2025-02-13 18:11:06,111][rgc][INFO] - 	Updating weights of batch 357
[2025-02-13 18:11:06,189][rgc][INFO] - Batch 357, avg loss per batch: 1.6427838451166528
[2025-02-13 18:11:06,198][rgc][INFO] - ================= Epoch 3, loss: 981.5857737360184 ===============
[2025-02-13 18:11:06,199][rgc][INFO] - Visualizing histograms
[2025-02-13 18:11:29,077][rgc][INFO] - AVG rho on val data: 0.27437744964000843
[2025-02-13 18:11:29,077][rgc][INFO] - AVG Mean Absolute Error on val data: 0.5257756398013698
[2025-02-13 18:11:40,251][rgc][INFO] - AVG rho on test data: 0.30335524636932043
[2025-02-13 18:11:40,251][rgc][INFO] - AVG Mean Absolute Error on test data: 0.5621566221133576
[2025-02-13 18:11:53,098][rgc][INFO] - AVG rho on train data: 0.22267217150328547
[2025-02-13 18:11:53,098][rgc][INFO] - AVG Mean Absolute Error on train data: 0.5476807697834287
[2025-02-13 18:11:53,099][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 18:11:53,114][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 0
[2025-02-13 18:12:13,836][rgc][INFO] - 	Updating weights of batch 0
[2025-02-13 18:12:13,887][rgc][INFO] - Batch 0, avg loss per batch: 4.469878166922725
[2025-02-13 18:12:13,888][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 1
[2025-02-13 18:12:34,581][rgc][INFO] - 	Updating weights of batch 1
[2025-02-13 18:12:34,633][rgc][INFO] - Batch 1, avg loss per batch: 2.347095874737747
[2025-02-13 18:12:34,634][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 2
[2025-02-13 18:12:55,297][rgc][INFO] - 	Updating weights of batch 2
[2025-02-13 18:12:55,347][rgc][INFO] - Batch 2, avg loss per batch: 2.0048838076455415
[2025-02-13 18:12:55,348][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 3
[2025-02-13 18:13:16,032][rgc][INFO] - 	Updating weights of batch 3
[2025-02-13 18:13:16,084][rgc][INFO] - Batch 3, avg loss per batch: 2.4932290742505523
[2025-02-13 18:13:16,085][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 4
[2025-02-13 18:13:36,772][rgc][INFO] - 	Updating weights of batch 4
[2025-02-13 18:13:36,824][rgc][INFO] - Batch 4, avg loss per batch: 2.7344079456789343
[2025-02-13 18:13:36,825][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 5
[2025-02-13 18:13:57,566][rgc][INFO] - 	Updating weights of batch 5
[2025-02-13 18:13:57,617][rgc][INFO] - Batch 5, avg loss per batch: 2.5080121316663133
[2025-02-13 18:13:57,617][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 6
[2025-02-13 18:14:18,302][rgc][INFO] - 	Updating weights of batch 6
[2025-02-13 18:14:18,365][rgc][INFO] - Batch 6, avg loss per batch: 2.414555861175741
[2025-02-13 18:14:18,366][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 7
[2025-02-13 18:14:39,052][rgc][INFO] - 	Updating weights of batch 7
[2025-02-13 18:14:39,102][rgc][INFO] - Batch 7, avg loss per batch: 2.623855469401148
[2025-02-13 18:14:39,103][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 8
[2025-02-13 18:14:59,853][rgc][INFO] - 	Updating weights of batch 8
[2025-02-13 18:14:59,909][rgc][INFO] - Batch 8, avg loss per batch: 2.143795115246704
[2025-02-13 18:14:59,910][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 9
[2025-02-13 18:15:20,651][rgc][INFO] - 	Updating weights of batch 9
[2025-02-13 18:15:20,722][rgc][INFO] - Batch 9, avg loss per batch: 0.9305261124216577
[2025-02-13 18:15:20,723][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 10
[2025-02-13 18:15:41,434][rgc][INFO] - 	Updating weights of batch 10
[2025-02-13 18:15:41,489][rgc][INFO] - Batch 10, avg loss per batch: 1.6499076030881887
[2025-02-13 18:15:41,489][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 11
[2025-02-13 18:16:02,239][rgc][INFO] - 	Updating weights of batch 11
[2025-02-13 18:16:02,288][rgc][INFO] - Batch 11, avg loss per batch: 2.3596137263230617
[2025-02-13 18:16:02,289][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 12
[2025-02-13 18:16:23,033][rgc][INFO] - 	Updating weights of batch 12
[2025-02-13 18:16:23,084][rgc][INFO] - Batch 12, avg loss per batch: 2.957181865543884
[2025-02-13 18:16:23,084][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 13
[2025-02-13 18:16:43,759][rgc][INFO] - 	Updating weights of batch 13
[2025-02-13 18:16:43,811][rgc][INFO] - Batch 13, avg loss per batch: 1.5337626973947331
[2025-02-13 18:16:43,812][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 14
[2025-02-13 18:17:04,530][rgc][INFO] - 	Updating weights of batch 14
[2025-02-13 18:17:04,590][rgc][INFO] - Batch 14, avg loss per batch: 1.2002385108247224
[2025-02-13 18:17:04,591][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 15
[2025-02-13 18:17:25,315][rgc][INFO] - 	Updating weights of batch 15
[2025-02-13 18:17:25,365][rgc][INFO] - Batch 15, avg loss per batch: 2.786218072003988
[2025-02-13 18:17:25,366][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 16
[2025-02-13 18:17:46,043][rgc][INFO] - 	Updating weights of batch 16
[2025-02-13 18:17:46,097][rgc][INFO] - Batch 16, avg loss per batch: 1.9929897729223036
[2025-02-13 18:17:46,098][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 17
[2025-02-13 18:18:06,850][rgc][INFO] - 	Updating weights of batch 17
[2025-02-13 18:18:06,927][rgc][INFO] - Batch 17, avg loss per batch: 2.5158608295628038
[2025-02-13 18:18:06,929][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 18
[2025-02-13 18:18:27,661][rgc][INFO] - 	Updating weights of batch 18
[2025-02-13 18:18:27,713][rgc][INFO] - Batch 18, avg loss per batch: 1.4962651864918972
[2025-02-13 18:18:27,714][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 19
[2025-02-13 18:18:48,414][rgc][INFO] - 	Updating weights of batch 19
[2025-02-13 18:18:48,465][rgc][INFO] - Batch 19, avg loss per batch: 2.523861711324923
[2025-02-13 18:18:48,466][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 20
[2025-02-13 18:19:09,217][rgc][INFO] - 	Updating weights of batch 20
[2025-02-13 18:19:09,265][rgc][INFO] - Batch 20, avg loss per batch: 2.36524907112293
[2025-02-13 18:19:09,266][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 21
[2025-02-13 18:19:30,017][rgc][INFO] - 	Updating weights of batch 21
[2025-02-13 18:19:30,069][rgc][INFO] - Batch 21, avg loss per batch: 1.4675089200608487
[2025-02-13 18:19:30,070][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 22
[2025-02-13 18:19:50,810][rgc][INFO] - 	Updating weights of batch 22
[2025-02-13 18:19:50,864][rgc][INFO] - Batch 22, avg loss per batch: 2.1244436830070605
[2025-02-13 18:19:50,865][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 23
[2025-02-13 18:20:11,588][rgc][INFO] - 	Updating weights of batch 23
[2025-02-13 18:20:11,642][rgc][INFO] - Batch 23, avg loss per batch: 1.7177852025530727
[2025-02-13 18:20:11,643][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 24
[2025-02-13 18:20:32,379][rgc][INFO] - 	Updating weights of batch 24
[2025-02-13 18:20:32,431][rgc][INFO] - Batch 24, avg loss per batch: 1.9521308339629684
[2025-02-13 18:20:32,432][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 25
[2025-02-13 18:20:53,182][rgc][INFO] - 	Updating weights of batch 25
[2025-02-13 18:20:53,235][rgc][INFO] - Batch 25, avg loss per batch: 3.701550922918654
[2025-02-13 18:20:53,236][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 26
[2025-02-13 18:21:13,985][rgc][INFO] - 	Updating weights of batch 26
[2025-02-13 18:21:14,038][rgc][INFO] - Batch 26, avg loss per batch: 3.757026582409149
[2025-02-13 18:21:14,039][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 27
[2025-02-13 18:21:34,794][rgc][INFO] - 	Updating weights of batch 27
[2025-02-13 18:21:34,870][rgc][INFO] - Batch 27, avg loss per batch: 3.5224323740007497
[2025-02-13 18:21:34,870][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 28
[2025-02-13 18:21:55,598][rgc][INFO] - 	Updating weights of batch 28
[2025-02-13 18:21:55,653][rgc][INFO] - Batch 28, avg loss per batch: 2.1444113746339664
[2025-02-13 18:21:55,655][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 29
[2025-02-13 18:22:16,322][rgc][INFO] - 	Updating weights of batch 29
[2025-02-13 18:22:16,377][rgc][INFO] - Batch 29, avg loss per batch: 4.542287539769088
[2025-02-13 18:22:16,378][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 30
[2025-02-13 18:22:37,127][rgc][INFO] - 	Updating weights of batch 30
[2025-02-13 18:22:37,197][rgc][INFO] - Batch 30, avg loss per batch: 4.212761891895225
[2025-02-13 18:22:37,198][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 31
[2025-02-13 18:22:57,946][rgc][INFO] - 	Updating weights of batch 31
[2025-02-13 18:22:58,000][rgc][INFO] - Batch 31, avg loss per batch: 2.0185093590671874
[2025-02-13 18:22:58,000][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 32
[2025-02-13 18:23:18,734][rgc][INFO] - 	Updating weights of batch 32
[2025-02-13 18:23:18,808][rgc][INFO] - Batch 32, avg loss per batch: 3.1614542875721767
[2025-02-13 18:23:18,809][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 33
[2025-02-13 18:23:39,570][rgc][INFO] - 	Updating weights of batch 33
[2025-02-13 18:23:39,622][rgc][INFO] - Batch 33, avg loss per batch: 5.176053386158099
[2025-02-13 18:23:39,622][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 34
[2025-02-13 18:24:00,318][rgc][INFO] - 	Updating weights of batch 34
[2025-02-13 18:24:00,368][rgc][INFO] - Batch 34, avg loss per batch: 2.485355213112684
[2025-02-13 18:24:00,369][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 35
[2025-02-13 18:24:21,097][rgc][INFO] - 	Updating weights of batch 35
[2025-02-13 18:24:21,149][rgc][INFO] - Batch 35, avg loss per batch: 1.6254568625949255
[2025-02-13 18:24:21,150][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 36
[2025-02-13 18:24:41,912][rgc][INFO] - 	Updating weights of batch 36
[2025-02-13 18:24:41,967][rgc][INFO] - Batch 36, avg loss per batch: 1.5354564444694379
[2025-02-13 18:24:41,968][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 37
[2025-02-13 18:25:02,705][rgc][INFO] - 	Updating weights of batch 37
[2025-02-13 18:25:02,761][rgc][INFO] - Batch 37, avg loss per batch: 1.8896831160587104
[2025-02-13 18:25:02,762][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 38
[2025-02-13 18:25:23,457][rgc][INFO] - 	Updating weights of batch 38
[2025-02-13 18:25:23,516][rgc][INFO] - Batch 38, avg loss per batch: 1.7634428111513083
[2025-02-13 18:25:23,517][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 39
[2025-02-13 18:25:44,209][rgc][INFO] - 	Updating weights of batch 39
[2025-02-13 18:25:44,265][rgc][INFO] - Batch 39, avg loss per batch: 2.3266146370262404
[2025-02-13 18:25:44,266][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 40
[2025-02-13 18:26:05,006][rgc][INFO] - 	Updating weights of batch 40
[2025-02-13 18:26:05,062][rgc][INFO] - Batch 40, avg loss per batch: 3.888186162781299
[2025-02-13 18:26:05,063][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 41
[2025-02-13 18:26:25,823][rgc][INFO] - 	Updating weights of batch 41
[2025-02-13 18:26:25,876][rgc][INFO] - Batch 41, avg loss per batch: 1.61416163287684
[2025-02-13 18:26:25,877][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 42
[2025-02-13 18:26:46,549][rgc][INFO] - 	Updating weights of batch 42
[2025-02-13 18:26:46,605][rgc][INFO] - Batch 42, avg loss per batch: 3.3377733037346697
[2025-02-13 18:26:46,606][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 43
[2025-02-13 18:27:07,282][rgc][INFO] - 	Updating weights of batch 43
[2025-02-13 18:27:07,333][rgc][INFO] - Batch 43, avg loss per batch: 1.2814404883362485
[2025-02-13 18:27:07,335][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 44
[2025-02-13 18:27:28,093][rgc][INFO] - 	Updating weights of batch 44
[2025-02-13 18:27:28,164][rgc][INFO] - Batch 44, avg loss per batch: 4.240481743909276
[2025-02-13 18:27:28,166][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 45
[2025-02-13 18:27:48,917][rgc][INFO] - 	Updating weights of batch 45
[2025-02-13 18:27:48,970][rgc][INFO] - Batch 45, avg loss per batch: 2.0365773656152264
[2025-02-13 18:27:48,971][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 46
[2025-02-13 18:28:09,654][rgc][INFO] - 	Updating weights of batch 46
[2025-02-13 18:28:09,708][rgc][INFO] - Batch 46, avg loss per batch: 2.2500233530796456
[2025-02-13 18:28:09,709][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 47
[2025-02-13 18:28:30,452][rgc][INFO] - 	Updating weights of batch 47
[2025-02-13 18:28:30,506][rgc][INFO] - Batch 47, avg loss per batch: 3.106488156064035
[2025-02-13 18:28:30,507][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 48
[2025-02-13 18:28:51,239][rgc][INFO] - 	Updating weights of batch 48
[2025-02-13 18:28:51,287][rgc][INFO] - Batch 48, avg loss per batch: 1.717328100493344
[2025-02-13 18:28:51,289][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 49
[2025-02-13 18:29:11,966][rgc][INFO] - 	Updating weights of batch 49
[2025-02-13 18:29:12,022][rgc][INFO] - Batch 49, avg loss per batch: 5.119695145514462
[2025-02-13 18:29:12,023][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 50
[2025-02-13 18:29:32,776][rgc][INFO] - 	Updating weights of batch 50
[2025-02-13 18:29:32,833][rgc][INFO] - Batch 50, avg loss per batch: 2.0545940972800603
[2025-02-13 18:29:32,834][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 51
[2025-02-13 18:29:53,533][rgc][INFO] - 	Updating weights of batch 51
[2025-02-13 18:29:53,591][rgc][INFO] - Batch 51, avg loss per batch: 4.967775482549866
[2025-02-13 18:29:53,592][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 52
[2025-02-13 18:30:14,357][rgc][INFO] - 	Updating weights of batch 52
[2025-02-13 18:30:14,412][rgc][INFO] - Batch 52, avg loss per batch: 3.5019806965332085
[2025-02-13 18:30:14,413][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 53
[2025-02-13 18:30:35,162][rgc][INFO] - 	Updating weights of batch 53
[2025-02-13 18:30:35,216][rgc][INFO] - Batch 53, avg loss per batch: 4.462630227930531
[2025-02-13 18:30:35,217][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 54
[2025-02-13 18:30:55,891][rgc][INFO] - 	Updating weights of batch 54
[2025-02-13 18:30:55,947][rgc][INFO] - Batch 54, avg loss per batch: 2.5807673737995445
[2025-02-13 18:30:55,948][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 55
[2025-02-13 18:31:16,621][rgc][INFO] - 	Updating weights of batch 55
[2025-02-13 18:31:16,674][rgc][INFO] - Batch 55, avg loss per batch: 2.669381558335564
[2025-02-13 18:31:16,675][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 56
[2025-02-13 18:31:37,346][rgc][INFO] - 	Updating weights of batch 56
[2025-02-13 18:31:37,400][rgc][INFO] - Batch 56, avg loss per batch: 2.2764943914889932
[2025-02-13 18:31:37,401][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 57
[2025-02-13 18:31:58,120][rgc][INFO] - 	Updating weights of batch 57
[2025-02-13 18:31:58,170][rgc][INFO] - Batch 57, avg loss per batch: 2.8320539889821106
[2025-02-13 18:31:58,171][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 58
[2025-02-13 18:32:18,890][rgc][INFO] - 	Updating weights of batch 58
[2025-02-13 18:32:18,941][rgc][INFO] - Batch 58, avg loss per batch: 2.4971975624598195
[2025-02-13 18:32:18,942][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 59
[2025-02-13 18:32:39,701][rgc][INFO] - 	Updating weights of batch 59
[2025-02-13 18:32:39,753][rgc][INFO] - Batch 59, avg loss per batch: 2.377840995215192
[2025-02-13 18:32:39,754][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 60
[2025-02-13 18:33:00,503][rgc][INFO] - 	Updating weights of batch 60
[2025-02-13 18:33:00,554][rgc][INFO] - Batch 60, avg loss per batch: 2.2341121055537863
[2025-02-13 18:33:00,554][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 61
[2025-02-13 18:33:21,301][rgc][INFO] - 	Updating weights of batch 61
[2025-02-13 18:33:21,355][rgc][INFO] - Batch 61, avg loss per batch: 3.417884027138281
[2025-02-13 18:33:21,357][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 62
[2025-02-13 18:33:42,088][rgc][INFO] - 	Updating weights of batch 62
[2025-02-13 18:33:42,138][rgc][INFO] - Batch 62, avg loss per batch: 3.709938152842059
[2025-02-13 18:33:42,138][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 63
[2025-02-13 18:34:02,914][rgc][INFO] - 	Updating weights of batch 63
[2025-02-13 18:34:02,965][rgc][INFO] - Batch 63, avg loss per batch: 2.4747986032304787
[2025-02-13 18:34:02,966][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 64
[2025-02-13 18:34:23,749][rgc][INFO] - 	Updating weights of batch 64
[2025-02-13 18:34:23,800][rgc][INFO] - Batch 64, avg loss per batch: 1.9985453577886059
[2025-02-13 18:34:23,801][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 65
[2025-02-13 18:34:44,506][rgc][INFO] - 	Updating weights of batch 65
[2025-02-13 18:34:44,560][rgc][INFO] - Batch 65, avg loss per batch: 2.7101172024495757
[2025-02-13 18:34:44,561][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 66
[2025-02-13 18:35:05,319][rgc][INFO] - 	Updating weights of batch 66
[2025-02-13 18:35:05,369][rgc][INFO] - Batch 66, avg loss per batch: 1.5329899878269113
[2025-02-13 18:35:05,370][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 67
[2025-02-13 18:35:26,123][rgc][INFO] - 	Updating weights of batch 67
[2025-02-13 18:35:26,175][rgc][INFO] - Batch 67, avg loss per batch: 2.9835251806892624
[2025-02-13 18:35:26,176][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 68
[2025-02-13 18:35:46,920][rgc][INFO] - 	Updating weights of batch 68
[2025-02-13 18:35:46,970][rgc][INFO] - Batch 68, avg loss per batch: 2.5432105952346395
[2025-02-13 18:35:46,971][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 69
[2025-02-13 18:36:07,715][rgc][INFO] - 	Updating weights of batch 69
[2025-02-13 18:36:07,764][rgc][INFO] - Batch 69, avg loss per batch: 2.9032416061433
[2025-02-13 18:36:07,764][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 70
[2025-02-13 18:36:28,514][rgc][INFO] - 	Updating weights of batch 70
[2025-02-13 18:36:28,565][rgc][INFO] - Batch 70, avg loss per batch: 2.639591271467066
[2025-02-13 18:36:28,566][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 71
[2025-02-13 18:36:49,255][rgc][INFO] - 	Updating weights of batch 71
[2025-02-13 18:36:49,302][rgc][INFO] - Batch 71, avg loss per batch: 1.497440037909787
[2025-02-13 18:36:49,303][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 72
[2025-02-13 18:37:10,059][rgc][INFO] - 	Updating weights of batch 72
[2025-02-13 18:37:10,111][rgc][INFO] - Batch 72, avg loss per batch: 2.5818223249125447
[2025-02-13 18:37:10,112][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 73
[2025-02-13 18:37:30,840][rgc][INFO] - 	Updating weights of batch 73
[2025-02-13 18:37:30,890][rgc][INFO] - Batch 73, avg loss per batch: 2.7162429728018758
[2025-02-13 18:37:30,890][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 74
[2025-02-13 18:37:51,586][rgc][INFO] - 	Updating weights of batch 74
[2025-02-13 18:37:51,638][rgc][INFO] - Batch 74, avg loss per batch: 2.8928220572568923
[2025-02-13 18:37:51,639][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 75
[2025-02-13 18:38:12,375][rgc][INFO] - 	Updating weights of batch 75
[2025-02-13 18:38:12,428][rgc][INFO] - Batch 75, avg loss per batch: 3.1806496992317306
[2025-02-13 18:38:12,429][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 76
[2025-02-13 18:38:33,180][rgc][INFO] - 	Updating weights of batch 76
[2025-02-13 18:38:33,232][rgc][INFO] - Batch 76, avg loss per batch: 2.3503583380185638
[2025-02-13 18:38:33,232][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 77
[2025-02-13 18:38:53,971][rgc][INFO] - 	Updating weights of batch 77
[2025-02-13 18:38:54,021][rgc][INFO] - Batch 77, avg loss per batch: 1.6022055051316664
[2025-02-13 18:38:54,022][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 78
[2025-02-13 18:39:14,775][rgc][INFO] - 	Updating weights of batch 78
[2025-02-13 18:39:14,843][rgc][INFO] - Batch 78, avg loss per batch: 1.6983589845492308
[2025-02-13 18:39:14,844][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 79
[2025-02-13 18:39:35,584][rgc][INFO] - 	Updating weights of batch 79
[2025-02-13 18:39:35,631][rgc][INFO] - Batch 79, avg loss per batch: 3.2340021287746286
[2025-02-13 18:39:35,632][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 80
[2025-02-13 18:39:56,368][rgc][INFO] - 	Updating weights of batch 80
[2025-02-13 18:39:56,420][rgc][INFO] - Batch 80, avg loss per batch: 3.487495873685183
[2025-02-13 18:39:56,421][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 81
[2025-02-13 18:40:17,098][rgc][INFO] - 	Updating weights of batch 81
[2025-02-13 18:40:17,156][rgc][INFO] - Batch 81, avg loss per batch: 2.8537185268018925
[2025-02-13 18:40:17,158][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 82
[2025-02-13 18:40:37,836][rgc][INFO] - 	Updating weights of batch 82
[2025-02-13 18:40:37,911][rgc][INFO] - Batch 82, avg loss per batch: 2.743322746441112
[2025-02-13 18:40:37,912][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 83
[2025-02-13 18:40:58,662][rgc][INFO] - 	Updating weights of batch 83
[2025-02-13 18:40:58,712][rgc][INFO] - Batch 83, avg loss per batch: 2.120967423319763
[2025-02-13 18:40:58,713][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 84
[2025-02-13 18:41:19,482][rgc][INFO] - 	Updating weights of batch 84
[2025-02-13 18:41:19,533][rgc][INFO] - Batch 84, avg loss per batch: 2.768040083022036
[2025-02-13 18:41:19,534][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 85
[2025-02-13 18:41:40,211][rgc][INFO] - 	Updating weights of batch 85
[2025-02-13 18:41:40,265][rgc][INFO] - Batch 85, avg loss per batch: 4.162343169378989
[2025-02-13 18:41:40,266][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 86
[2025-02-13 18:42:00,949][rgc][INFO] - 	Updating weights of batch 86
[2025-02-13 18:42:01,000][rgc][INFO] - Batch 86, avg loss per batch: 4.109579290128911
[2025-02-13 18:42:01,001][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 87
[2025-02-13 18:42:21,768][rgc][INFO] - 	Updating weights of batch 87
[2025-02-13 18:42:21,820][rgc][INFO] - Batch 87, avg loss per batch: 4.02836908979125
[2025-02-13 18:42:21,821][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 88
[2025-02-13 18:42:42,514][rgc][INFO] - 	Updating weights of batch 88
[2025-02-13 18:42:42,569][rgc][INFO] - Batch 88, avg loss per batch: 2.167371580452304
[2025-02-13 18:42:42,570][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 89
[2025-02-13 18:43:03,321][rgc][INFO] - 	Updating weights of batch 89
[2025-02-13 18:43:03,376][rgc][INFO] - Batch 89, avg loss per batch: 2.3062946768555963
[2025-02-13 18:43:03,377][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 90
[2025-02-13 18:43:24,100][rgc][INFO] - 	Updating weights of batch 90
[2025-02-13 18:43:24,165][rgc][INFO] - Batch 90, avg loss per batch: 3.121055276425354
[2025-02-13 18:43:24,166][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 91
[2025-02-13 18:43:44,873][rgc][INFO] - 	Updating weights of batch 91
[2025-02-13 18:43:44,924][rgc][INFO] - Batch 91, avg loss per batch: 2.437128041711922
[2025-02-13 18:43:44,925][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 92
[2025-02-13 18:44:05,657][rgc][INFO] - 	Updating weights of batch 92
[2025-02-13 18:44:05,707][rgc][INFO] - Batch 92, avg loss per batch: 2.0720347873826896
[2025-02-13 18:44:05,707][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 93
[2025-02-13 18:44:26,444][rgc][INFO] - 	Updating weights of batch 93
[2025-02-13 18:44:26,496][rgc][INFO] - Batch 93, avg loss per batch: 1.8839608656975135
[2025-02-13 18:44:26,497][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 94
[2025-02-13 18:44:47,248][rgc][INFO] - 	Updating weights of batch 94
[2025-02-13 18:44:47,300][rgc][INFO] - Batch 94, avg loss per batch: 2.6377689892610534
[2025-02-13 18:44:47,302][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 95
[2025-02-13 18:45:08,059][rgc][INFO] - 	Updating weights of batch 95
[2025-02-13 18:45:08,124][rgc][INFO] - Batch 95, avg loss per batch: 3.1980729721916354
[2025-02-13 18:45:08,125][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 96
[2025-02-13 18:45:28,849][rgc][INFO] - 	Updating weights of batch 96
[2025-02-13 18:45:28,916][rgc][INFO] - Batch 96, avg loss per batch: 2.4619571827380105
[2025-02-13 18:45:28,917][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 97
[2025-02-13 18:45:49,652][rgc][INFO] - 	Updating weights of batch 97
[2025-02-13 18:45:49,709][rgc][INFO] - Batch 97, avg loss per batch: 1.361038733718447
[2025-02-13 18:45:49,709][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 98
[2025-02-13 18:46:10,403][rgc][INFO] - 	Updating weights of batch 98
[2025-02-13 18:46:10,467][rgc][INFO] - Batch 98, avg loss per batch: 3.058835985843632
[2025-02-13 18:46:10,468][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 99
[2025-02-13 18:46:31,166][rgc][INFO] - 	Updating weights of batch 99
[2025-02-13 18:46:31,217][rgc][INFO] - Batch 99, avg loss per batch: 3.374317229364917
[2025-02-13 18:46:42,728][rgc][INFO] - AVG rho on val data: 0.27140034141634145
[2025-02-13 18:46:42,728][rgc][INFO] - AVG mae on val data: 0.5264321277071342
[2025-02-13 18:46:53,897][rgc][INFO] - AVG rho on test data: 0.30543747488276357
[2025-02-13 18:46:53,898][rgc][INFO] - AVG mae on test data: 0.5611809583172154
[2025-02-13 18:47:06,748][rgc][INFO] - AVG rho on train data: 0.22104361372008033
[2025-02-13 18:47:06,748][rgc][INFO] - AVG mae on train data: 0.5473155897215705
[2025-02-13 18:47:06,749][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 18:47:06,751][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 100
[2025-02-13 18:47:27,462][rgc][INFO] - 	Updating weights of batch 100
[2025-02-13 18:47:27,517][rgc][INFO] - Batch 100, avg loss per batch: 2.8216075310604776
[2025-02-13 18:47:27,517][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 101
[2025-02-13 18:47:48,267][rgc][INFO] - 	Updating weights of batch 101
[2025-02-13 18:47:48,320][rgc][INFO] - Batch 101, avg loss per batch: 3.7406317751623535
[2025-02-13 18:47:48,321][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 102
[2025-02-13 18:48:09,077][rgc][INFO] - 	Updating weights of batch 102
[2025-02-13 18:48:09,132][rgc][INFO] - Batch 102, avg loss per batch: 3.188506967653216
[2025-02-13 18:48:09,133][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 103
[2025-02-13 18:48:29,865][rgc][INFO] - 	Updating weights of batch 103
[2025-02-13 18:48:29,920][rgc][INFO] - Batch 103, avg loss per batch: 2.7486856300415834
[2025-02-13 18:48:29,921][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 104
[2025-02-13 18:48:50,645][rgc][INFO] - 	Updating weights of batch 104
[2025-02-13 18:48:50,698][rgc][INFO] - Batch 104, avg loss per batch: 1.939270345976904
[2025-02-13 18:48:50,698][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 105
[2025-02-13 18:49:11,436][rgc][INFO] - 	Updating weights of batch 105
[2025-02-13 18:49:11,489][rgc][INFO] - Batch 105, avg loss per batch: 2.4900954284512893
[2025-02-13 18:49:11,490][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 106
[2025-02-13 18:49:32,222][rgc][INFO] - 	Updating weights of batch 106
[2025-02-13 18:49:32,275][rgc][INFO] - Batch 106, avg loss per batch: 2.4428991417904538
[2025-02-13 18:49:32,275][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 107
[2025-02-13 18:49:53,044][rgc][INFO] - 	Updating weights of batch 107
[2025-02-13 18:49:53,097][rgc][INFO] - Batch 107, avg loss per batch: 3.473960187658101
[2025-02-13 18:49:53,098][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 108
[2025-02-13 18:50:13,860][rgc][INFO] - 	Updating weights of batch 108
[2025-02-13 18:50:13,927][rgc][INFO] - Batch 108, avg loss per batch: 4.540211183100547
[2025-02-13 18:50:13,927][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 109
[2025-02-13 18:50:34,669][rgc][INFO] - 	Updating weights of batch 109
[2025-02-13 18:50:34,722][rgc][INFO] - Batch 109, avg loss per batch: 1.904931806596628
[2025-02-13 18:50:34,723][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 110
[2025-02-13 18:50:55,456][rgc][INFO] - 	Updating weights of batch 110
[2025-02-13 18:50:55,507][rgc][INFO] - Batch 110, avg loss per batch: 2.14061022292817
[2025-02-13 18:50:55,508][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 111
[2025-02-13 18:51:16,264][rgc][INFO] - 	Updating weights of batch 111
[2025-02-13 18:51:16,316][rgc][INFO] - Batch 111, avg loss per batch: 4.270006482255761
[2025-02-13 18:51:16,318][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 112
[2025-02-13 18:51:37,063][rgc][INFO] - 	Updating weights of batch 112
[2025-02-13 18:51:37,115][rgc][INFO] - Batch 112, avg loss per batch: 2.018915880004741
[2025-02-13 18:51:37,116][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 113
[2025-02-13 18:51:57,846][rgc][INFO] - 	Updating weights of batch 113
[2025-02-13 18:51:57,897][rgc][INFO] - Batch 113, avg loss per batch: 3.035669144590016
[2025-02-13 18:51:57,897][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 114
[2025-02-13 18:52:18,622][rgc][INFO] - 	Updating weights of batch 114
[2025-02-13 18:52:18,673][rgc][INFO] - Batch 114, avg loss per batch: 1.8144326244817046
[2025-02-13 18:52:18,674][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 115
[2025-02-13 18:52:39,434][rgc][INFO] - 	Updating weights of batch 115
[2025-02-13 18:52:39,488][rgc][INFO] - Batch 115, avg loss per batch: 4.348467537745359
[2025-02-13 18:52:39,489][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 116
[2025-02-13 18:53:00,241][rgc][INFO] - 	Updating weights of batch 116
[2025-02-13 18:53:00,294][rgc][INFO] - Batch 116, avg loss per batch: 1.572676882196212
[2025-02-13 18:53:00,295][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 117
[2025-02-13 18:53:21,062][rgc][INFO] - 	Updating weights of batch 117
[2025-02-13 18:53:21,117][rgc][INFO] - Batch 117, avg loss per batch: 2.869550747498818
[2025-02-13 18:53:21,119][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 118
[2025-02-13 18:53:41,885][rgc][INFO] - 	Updating weights of batch 118
[2025-02-13 18:53:41,938][rgc][INFO] - Batch 118, avg loss per batch: 2.6693383454697326
[2025-02-13 18:53:41,939][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 119
[2025-02-13 18:54:02,711][rgc][INFO] - 	Updating weights of batch 119
[2025-02-13 18:54:02,769][rgc][INFO] - Batch 119, avg loss per batch: 4.614984447900401
[2025-02-13 18:54:02,770][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 120
[2025-02-13 18:54:23,476][rgc][INFO] - 	Updating weights of batch 120
[2025-02-13 18:54:23,530][rgc][INFO] - Batch 120, avg loss per batch: 1.6980183069581327
[2025-02-13 18:54:23,532][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 121
[2025-02-13 18:54:44,269][rgc][INFO] - 	Updating weights of batch 121
[2025-02-13 18:54:44,320][rgc][INFO] - Batch 121, avg loss per batch: 3.6471344034668913
[2025-02-13 18:54:44,321][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 122
[2025-02-13 18:55:05,086][rgc][INFO] - 	Updating weights of batch 122
[2025-02-13 18:55:05,154][rgc][INFO] - Batch 122, avg loss per batch: 2.4033101883132173
[2025-02-13 18:55:05,155][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 123
[2025-02-13 18:55:25,935][rgc][INFO] - 	Updating weights of batch 123
[2025-02-13 18:55:25,988][rgc][INFO] - Batch 123, avg loss per batch: 2.0335462817016197
[2025-02-13 18:55:25,989][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 124
[2025-02-13 18:55:46,690][rgc][INFO] - 	Updating weights of batch 124
[2025-02-13 18:55:46,744][rgc][INFO] - Batch 124, avg loss per batch: 1.8893522825289744
[2025-02-13 18:55:46,745][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 125
[2025-02-13 18:56:07,486][rgc][INFO] - 	Updating weights of batch 125
[2025-02-13 18:56:07,542][rgc][INFO] - Batch 125, avg loss per batch: 1.307015700867363
[2025-02-13 18:56:07,543][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 126
[2025-02-13 18:56:28,269][rgc][INFO] - 	Updating weights of batch 126
[2025-02-13 18:56:28,320][rgc][INFO] - Batch 126, avg loss per batch: 3.0269781631913384
[2025-02-13 18:56:28,321][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 127
[2025-02-13 18:56:49,083][rgc][INFO] - 	Updating weights of batch 127
[2025-02-13 18:56:49,132][rgc][INFO] - Batch 127, avg loss per batch: 1.9406690343935153
[2025-02-13 18:56:49,133][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 128
[2025-02-13 18:57:09,901][rgc][INFO] - 	Updating weights of batch 128
[2025-02-13 18:57:09,950][rgc][INFO] - Batch 128, avg loss per batch: 3.3954899790103226
[2025-02-13 18:57:09,950][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 129
[2025-02-13 18:57:30,707][rgc][INFO] - 	Updating weights of batch 129
[2025-02-13 18:57:30,763][rgc][INFO] - Batch 129, avg loss per batch: 2.7827627628127205
[2025-02-13 18:57:30,764][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 130
[2025-02-13 18:57:51,446][rgc][INFO] - 	Updating weights of batch 130
[2025-02-13 18:57:51,496][rgc][INFO] - Batch 130, avg loss per batch: 2.0068386530015405
[2025-02-13 18:57:51,497][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 131
[2025-02-13 18:58:12,254][rgc][INFO] - 	Updating weights of batch 131
[2025-02-13 18:58:12,305][rgc][INFO] - Batch 131, avg loss per batch: 3.5306971115074646
[2025-02-13 18:58:12,306][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 132
[2025-02-13 18:58:33,041][rgc][INFO] - 	Updating weights of batch 132
[2025-02-13 18:58:33,097][rgc][INFO] - Batch 132, avg loss per batch: 2.746040008360402
[2025-02-13 18:58:33,098][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 133
[2025-02-13 18:58:53,782][rgc][INFO] - 	Updating weights of batch 133
[2025-02-13 18:58:53,831][rgc][INFO] - Batch 133, avg loss per batch: 3.3853113388249945
[2025-02-13 18:58:53,832][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 134
[2025-02-13 18:59:14,576][rgc][INFO] - 	Updating weights of batch 134
[2025-02-13 18:59:14,626][rgc][INFO] - Batch 134, avg loss per batch: 2.1376464183792367
[2025-02-13 18:59:14,626][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 135
[2025-02-13 18:59:35,386][rgc][INFO] - 	Updating weights of batch 135
[2025-02-13 18:59:35,460][rgc][INFO] - Batch 135, avg loss per batch: 1.7542235296325444
[2025-02-13 18:59:35,462][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 136
[2025-02-13 18:59:56,156][rgc][INFO] - 	Updating weights of batch 136
[2025-02-13 18:59:56,205][rgc][INFO] - Batch 136, avg loss per batch: 2.6646154653101672
[2025-02-13 18:59:56,206][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 137
[2025-02-13 19:00:16,948][rgc][INFO] - 	Updating weights of batch 137
[2025-02-13 19:00:17,000][rgc][INFO] - Batch 137, avg loss per batch: 3.6936259034156076
[2025-02-13 19:00:17,000][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 138
[2025-02-13 19:00:37,691][rgc][INFO] - 	Updating weights of batch 138
[2025-02-13 19:00:37,745][rgc][INFO] - Batch 138, avg loss per batch: 3.894448196221834
[2025-02-13 19:00:37,746][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 139
[2025-02-13 19:00:58,472][rgc][INFO] - 	Updating weights of batch 139
[2025-02-13 19:00:58,524][rgc][INFO] - Batch 139, avg loss per batch: 3.613544559257546
[2025-02-13 19:00:58,524][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 140
[2025-02-13 19:01:19,201][rgc][INFO] - 	Updating weights of batch 140
[2025-02-13 19:01:19,263][rgc][INFO] - Batch 140, avg loss per batch: 2.892705292283397
[2025-02-13 19:01:19,264][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 141
[2025-02-13 19:01:39,995][rgc][INFO] - 	Updating weights of batch 141
[2025-02-13 19:01:40,048][rgc][INFO] - Batch 141, avg loss per batch: 2.9364348209735
[2025-02-13 19:01:40,049][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 142
[2025-02-13 19:02:00,769][rgc][INFO] - 	Updating weights of batch 142
[2025-02-13 19:02:00,821][rgc][INFO] - Batch 142, avg loss per batch: 3.0109323912127213
[2025-02-13 19:02:00,822][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 143
[2025-02-13 19:02:21,562][rgc][INFO] - 	Updating weights of batch 143
[2025-02-13 19:02:21,611][rgc][INFO] - Batch 143, avg loss per batch: 1.8966448307520425
[2025-02-13 19:02:21,612][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 144
[2025-02-13 19:02:42,341][rgc][INFO] - 	Updating weights of batch 144
[2025-02-13 19:02:42,394][rgc][INFO] - Batch 144, avg loss per batch: 2.4141500490538386
[2025-02-13 19:02:42,395][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 145
[2025-02-13 19:03:03,055][rgc][INFO] - 	Updating weights of batch 145
[2025-02-13 19:03:03,104][rgc][INFO] - Batch 145, avg loss per batch: 2.380108597543499
[2025-02-13 19:03:03,105][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 146
[2025-02-13 19:03:23,857][rgc][INFO] - 	Updating weights of batch 146
[2025-02-13 19:03:23,911][rgc][INFO] - Batch 146, avg loss per batch: 3.157164769803244
[2025-02-13 19:03:23,912][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 147
[2025-02-13 19:03:44,607][rgc][INFO] - 	Updating weights of batch 147
[2025-02-13 19:03:44,659][rgc][INFO] - Batch 147, avg loss per batch: 5.09793839095192
[2025-02-13 19:03:44,660][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 148
[2025-02-13 19:04:05,392][rgc][INFO] - 	Updating weights of batch 148
[2025-02-13 19:04:05,446][rgc][INFO] - Batch 148, avg loss per batch: 3.671923748431662
[2025-02-13 19:04:05,446][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 149
[2025-02-13 19:04:26,187][rgc][INFO] - 	Updating weights of batch 149
[2025-02-13 19:04:26,235][rgc][INFO] - Batch 149, avg loss per batch: 2.1355425632569514
[2025-02-13 19:04:26,236][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 150
[2025-02-13 19:04:46,982][rgc][INFO] - 	Updating weights of batch 150
[2025-02-13 19:04:47,033][rgc][INFO] - Batch 150, avg loss per batch: 2.6691193926954964
[2025-02-13 19:04:47,035][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 151
[2025-02-13 19:05:07,793][rgc][INFO] - 	Updating weights of batch 151
[2025-02-13 19:05:07,842][rgc][INFO] - Batch 151, avg loss per batch: 3.1536220008868456
[2025-02-13 19:05:07,843][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 152
[2025-02-13 19:05:28,574][rgc][INFO] - 	Updating weights of batch 152
[2025-02-13 19:05:28,625][rgc][INFO] - Batch 152, avg loss per batch: 3.047726107325267
[2025-02-13 19:05:28,626][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 153
[2025-02-13 19:05:49,373][rgc][INFO] - 	Updating weights of batch 153
[2025-02-13 19:05:49,429][rgc][INFO] - Batch 153, avg loss per batch: 2.1112542740190987
[2025-02-13 19:05:49,431][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 154
[2025-02-13 19:06:10,121][rgc][INFO] - 	Updating weights of batch 154
[2025-02-13 19:06:10,180][rgc][INFO] - Batch 154, avg loss per batch: 3.39096695027379
[2025-02-13 19:06:10,181][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 155
[2025-02-13 19:06:30,972][rgc][INFO] - 	Updating weights of batch 155
[2025-02-13 19:06:31,057][rgc][INFO] - Batch 155, avg loss per batch: 3.305952974892324
[2025-02-13 19:06:31,062][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 156
[2025-02-13 19:06:51,873][rgc][INFO] - 	Updating weights of batch 156
[2025-02-13 19:06:51,939][rgc][INFO] - Batch 156, avg loss per batch: 3.129388933043684
[2025-02-13 19:06:51,941][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 157
[2025-02-13 19:07:12,754][rgc][INFO] - 	Updating weights of batch 157
[2025-02-13 19:07:12,832][rgc][INFO] - Batch 157, avg loss per batch: 1.692311317544719
[2025-02-13 19:07:12,836][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 158
[2025-02-13 19:07:33,567][rgc][INFO] - 	Updating weights of batch 158
[2025-02-13 19:07:33,625][rgc][INFO] - Batch 158, avg loss per batch: 4.677427764322658
[2025-02-13 19:07:33,626][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 159
[2025-02-13 19:07:54,397][rgc][INFO] - 	Updating weights of batch 159
[2025-02-13 19:07:54,452][rgc][INFO] - Batch 159, avg loss per batch: 4.6552896514362745
[2025-02-13 19:07:54,453][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 160
[2025-02-13 19:08:15,137][rgc][INFO] - 	Updating weights of batch 160
[2025-02-13 19:08:15,217][rgc][INFO] - Batch 160, avg loss per batch: 4.903949421373699
[2025-02-13 19:08:15,218][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 161
[2025-02-13 19:08:35,948][rgc][INFO] - 	Updating weights of batch 161
[2025-02-13 19:08:36,002][rgc][INFO] - Batch 161, avg loss per batch: 6.924835973698767
[2025-02-13 19:08:36,003][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 162
[2025-02-13 19:08:56,756][rgc][INFO] - 	Updating weights of batch 162
[2025-02-13 19:08:56,809][rgc][INFO] - Batch 162, avg loss per batch: 1.6177213555257624
[2025-02-13 19:08:56,810][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 163
[2025-02-13 19:09:17,490][rgc][INFO] - 	Updating weights of batch 163
[2025-02-13 19:09:17,548][rgc][INFO] - Batch 163, avg loss per batch: 4.679900856297896
[2025-02-13 19:09:17,549][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 164
[2025-02-13 19:09:38,269][rgc][INFO] - 	Updating weights of batch 164
[2025-02-13 19:09:38,319][rgc][INFO] - Batch 164, avg loss per batch: 2.4435806765106443
[2025-02-13 19:09:38,320][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 165
[2025-02-13 19:09:59,046][rgc][INFO] - 	Updating weights of batch 165
[2025-02-13 19:09:59,102][rgc][INFO] - Batch 165, avg loss per batch: 1.4024603192441416
[2025-02-13 19:09:59,103][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 166
[2025-02-13 19:10:19,874][rgc][INFO] - 	Updating weights of batch 166
[2025-02-13 19:10:19,935][rgc][INFO] - Batch 166, avg loss per batch: 3.5114198533046643
[2025-02-13 19:10:19,935][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 167
[2025-02-13 19:10:40,694][rgc][INFO] - 	Updating weights of batch 167
[2025-02-13 19:10:40,748][rgc][INFO] - Batch 167, avg loss per batch: 3.8432579535365945
[2025-02-13 19:10:40,749][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 168
[2025-02-13 19:11:01,470][rgc][INFO] - 	Updating weights of batch 168
[2025-02-13 19:11:01,522][rgc][INFO] - Batch 168, avg loss per batch: 2.5322592170205995
[2025-02-13 19:11:01,522][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 169
[2025-02-13 19:11:22,267][rgc][INFO] - 	Updating weights of batch 169
[2025-02-13 19:11:22,332][rgc][INFO] - Batch 169, avg loss per batch: 2.027363888589675
[2025-02-13 19:11:22,333][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 170
[2025-02-13 19:11:43,088][rgc][INFO] - 	Updating weights of batch 170
[2025-02-13 19:11:43,166][rgc][INFO] - Batch 170, avg loss per batch: 2.516566610337882
[2025-02-13 19:11:43,167][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 171
[2025-02-13 19:12:03,916][rgc][INFO] - 	Updating weights of batch 171
[2025-02-13 19:12:03,968][rgc][INFO] - Batch 171, avg loss per batch: 2.858465355990032
[2025-02-13 19:12:03,968][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 172
[2025-02-13 19:12:24,715][rgc][INFO] - 	Updating weights of batch 172
[2025-02-13 19:12:24,798][rgc][INFO] - Batch 172, avg loss per batch: 2.3949975785697903
[2025-02-13 19:12:24,799][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 173
[2025-02-13 19:12:45,505][rgc][INFO] - 	Updating weights of batch 173
[2025-02-13 19:12:45,562][rgc][INFO] - Batch 173, avg loss per batch: 2.664547300800909
[2025-02-13 19:12:45,563][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 174
[2025-02-13 19:13:06,317][rgc][INFO] - 	Updating weights of batch 174
[2025-02-13 19:13:06,392][rgc][INFO] - Batch 174, avg loss per batch: 3.3819469370651003
[2025-02-13 19:13:06,393][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 175
[2025-02-13 19:13:27,132][rgc][INFO] - 	Updating weights of batch 175
[2025-02-13 19:13:27,185][rgc][INFO] - Batch 175, avg loss per batch: 1.7633080262972292
[2025-02-13 19:13:27,186][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 176
[2025-02-13 19:13:47,941][rgc][INFO] - 	Updating weights of batch 176
[2025-02-13 19:13:47,991][rgc][INFO] - Batch 176, avg loss per batch: 4.78033396191831
[2025-02-13 19:13:47,992][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 177
[2025-02-13 19:14:08,770][rgc][INFO] - 	Updating weights of batch 177
[2025-02-13 19:14:08,832][rgc][INFO] - Batch 177, avg loss per batch: 2.66498793203586
[2025-02-13 19:14:08,833][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 178
[2025-02-13 19:14:29,595][rgc][INFO] - 	Updating weights of batch 178
[2025-02-13 19:14:29,644][rgc][INFO] - Batch 178, avg loss per batch: 3.3130623522766642
[2025-02-13 19:14:29,645][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 179
[2025-02-13 19:14:50,339][rgc][INFO] - 	Updating weights of batch 179
[2025-02-13 19:14:50,390][rgc][INFO] - Batch 179, avg loss per batch: 3.041128863828405
[2025-02-13 19:14:50,391][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 180
[2025-02-13 19:15:11,087][rgc][INFO] - 	Updating weights of batch 180
[2025-02-13 19:15:11,142][rgc][INFO] - Batch 180, avg loss per batch: 2.046351862251709
[2025-02-13 19:15:11,143][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 181
[2025-02-13 19:15:31,867][rgc][INFO] - 	Updating weights of batch 181
[2025-02-13 19:15:31,915][rgc][INFO] - Batch 181, avg loss per batch: 4.074087835357991
[2025-02-13 19:15:31,916][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 182
[2025-02-13 19:15:52,666][rgc][INFO] - 	Updating weights of batch 182
[2025-02-13 19:15:52,741][rgc][INFO] - Batch 182, avg loss per batch: 2.8783438014793976
[2025-02-13 19:15:52,743][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 183
[2025-02-13 19:16:13,429][rgc][INFO] - 	Updating weights of batch 183
[2025-02-13 19:16:13,480][rgc][INFO] - Batch 183, avg loss per batch: 1.9349822765468008
[2025-02-13 19:16:13,481][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 184
[2025-02-13 19:16:34,169][rgc][INFO] - 	Updating weights of batch 184
[2025-02-13 19:16:34,221][rgc][INFO] - Batch 184, avg loss per batch: 3.1701191818804553
[2025-02-13 19:16:34,222][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 185
[2025-02-13 19:16:54,908][rgc][INFO] - 	Updating weights of batch 185
[2025-02-13 19:16:54,956][rgc][INFO] - Batch 185, avg loss per batch: 2.2665201976012757
[2025-02-13 19:16:54,957][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 186
[2025-02-13 19:17:15,637][rgc][INFO] - 	Updating weights of batch 186
[2025-02-13 19:17:15,691][rgc][INFO] - Batch 186, avg loss per batch: 3.160563050771535
[2025-02-13 19:17:15,693][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 187
[2025-02-13 19:17:36,451][rgc][INFO] - 	Updating weights of batch 187
[2025-02-13 19:17:36,508][rgc][INFO] - Batch 187, avg loss per batch: 2.4865033497039994
[2025-02-13 19:17:36,509][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 188
[2025-02-13 19:17:57,232][rgc][INFO] - 	Updating weights of batch 188
[2025-02-13 19:17:57,288][rgc][INFO] - Batch 188, avg loss per batch: 2.472984572525488
[2025-02-13 19:17:57,289][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 189
[2025-02-13 19:18:18,045][rgc][INFO] - 	Updating weights of batch 189
[2025-02-13 19:18:18,098][rgc][INFO] - Batch 189, avg loss per batch: 2.469613086279761
[2025-02-13 19:18:18,099][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 190
[2025-02-13 19:18:38,827][rgc][INFO] - 	Updating weights of batch 190
[2025-02-13 19:18:38,878][rgc][INFO] - Batch 190, avg loss per batch: 2.9926794874543283
[2025-02-13 19:18:38,879][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 191
[2025-02-13 19:18:59,642][rgc][INFO] - 	Updating weights of batch 191
[2025-02-13 19:18:59,704][rgc][INFO] - Batch 191, avg loss per batch: 2.180233126192475
[2025-02-13 19:18:59,705][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 192
[2025-02-13 19:19:20,449][rgc][INFO] - 	Updating weights of batch 192
[2025-02-13 19:19:20,505][rgc][INFO] - Batch 192, avg loss per batch: 2.290511044939538
[2025-02-13 19:19:20,506][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 193
[2025-02-13 19:19:41,223][rgc][INFO] - 	Updating weights of batch 193
[2025-02-13 19:19:41,280][rgc][INFO] - Batch 193, avg loss per batch: 2.5106046751196875
[2025-02-13 19:19:41,281][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 194
[2025-02-13 19:20:02,017][rgc][INFO] - 	Updating weights of batch 194
[2025-02-13 19:20:02,072][rgc][INFO] - Batch 194, avg loss per batch: 2.5501916859483313
[2025-02-13 19:20:02,073][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 195
[2025-02-13 19:20:22,839][rgc][INFO] - 	Updating weights of batch 195
[2025-02-13 19:20:22,891][rgc][INFO] - Batch 195, avg loss per batch: 5.427273156495168
[2025-02-13 19:20:22,892][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 196
[2025-02-13 19:20:43,617][rgc][INFO] - 	Updating weights of batch 196
[2025-02-13 19:20:43,670][rgc][INFO] - Batch 196, avg loss per batch: 3.4903993683492933
[2025-02-13 19:20:43,671][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 197
[2025-02-13 19:21:04,392][rgc][INFO] - 	Updating weights of batch 197
[2025-02-13 19:21:04,442][rgc][INFO] - Batch 197, avg loss per batch: 1.0700558624614427
[2025-02-13 19:21:04,443][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 198
[2025-02-13 19:21:25,118][rgc][INFO] - 	Updating weights of batch 198
[2025-02-13 19:21:25,166][rgc][INFO] - Batch 198, avg loss per batch: 2.6470767007980385
[2025-02-13 19:21:25,167][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 199
[2025-02-13 19:21:45,830][rgc][INFO] - 	Updating weights of batch 199
[2025-02-13 19:21:45,904][rgc][INFO] - Batch 199, avg loss per batch: 4.709338428308385
[2025-02-13 19:21:57,466][rgc][INFO] - AVG rho on val data: 0.2690606685274404
[2025-02-13 19:21:57,466][rgc][INFO] - AVG mae on val data: 0.526824876313299
[2025-02-13 19:22:08,630][rgc][INFO] - AVG rho on test data: 0.31123652072746183
[2025-02-13 19:22:08,631][rgc][INFO] - AVG mae on test data: 0.5618124885746186
[2025-02-13 19:22:21,460][rgc][INFO] - AVG rho on train data: 0.22398025580924985
[2025-02-13 19:22:21,461][rgc][INFO] - AVG mae on train data: 0.5463859005442673
[2025-02-13 19:22:21,461][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 19:22:21,462][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 200
[2025-02-13 19:22:42,203][rgc][INFO] - 	Updating weights of batch 200
[2025-02-13 19:22:42,283][rgc][INFO] - Batch 200, avg loss per batch: 2.3194324758793625
[2025-02-13 19:22:42,285][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 201
[2025-02-13 19:23:03,037][rgc][INFO] - 	Updating weights of batch 201
[2025-02-13 19:23:03,087][rgc][INFO] - Batch 201, avg loss per batch: 1.929539762813391
[2025-02-13 19:23:03,087][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 202
[2025-02-13 19:23:23,835][rgc][INFO] - 	Updating weights of batch 202
[2025-02-13 19:23:23,918][rgc][INFO] - Batch 202, avg loss per batch: 2.8678219661831017
[2025-02-13 19:23:23,919][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 203
[2025-02-13 19:23:44,662][rgc][INFO] - 	Updating weights of batch 203
[2025-02-13 19:23:44,713][rgc][INFO] - Batch 203, avg loss per batch: 1.4626358158703885
[2025-02-13 19:23:44,714][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 204
[2025-02-13 19:24:05,442][rgc][INFO] - 	Updating weights of batch 204
[2025-02-13 19:24:05,496][rgc][INFO] - Batch 204, avg loss per batch: 3.470185159639918
[2025-02-13 19:24:05,497][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 205
[2025-02-13 19:24:26,242][rgc][INFO] - 	Updating weights of batch 205
[2025-02-13 19:24:26,306][rgc][INFO] - Batch 205, avg loss per batch: 2.1312832606632384
[2025-02-13 19:24:26,307][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 206
[2025-02-13 19:24:47,092][rgc][INFO] - 	Updating weights of batch 206
[2025-02-13 19:24:47,152][rgc][INFO] - Batch 206, avg loss per batch: 1.1981661057624036
[2025-02-13 19:24:47,154][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 207
[2025-02-13 19:25:07,907][rgc][INFO] - 	Updating weights of batch 207
[2025-02-13 19:25:07,957][rgc][INFO] - Batch 207, avg loss per batch: 3.2888912803742185
[2025-02-13 19:25:07,959][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 208
[2025-02-13 19:25:28,729][rgc][INFO] - 	Updating weights of batch 208
[2025-02-13 19:25:28,781][rgc][INFO] - Batch 208, avg loss per batch: 2.804099870511571
[2025-02-13 19:25:28,782][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 209
[2025-02-13 19:25:49,547][rgc][INFO] - 	Updating weights of batch 209
[2025-02-13 19:25:49,600][rgc][INFO] - Batch 209, avg loss per batch: 2.691252550849996
[2025-02-13 19:25:49,601][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 210
[2025-02-13 19:26:10,362][rgc][INFO] - 	Updating weights of batch 210
[2025-02-13 19:26:10,414][rgc][INFO] - Batch 210, avg loss per batch: 1.602507245751385
[2025-02-13 19:26:10,415][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 211
[2025-02-13 19:26:31,154][rgc][INFO] - 	Updating weights of batch 211
[2025-02-13 19:26:31,208][rgc][INFO] - Batch 211, avg loss per batch: 1.6563610258091792
[2025-02-13 19:26:31,209][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 212
[2025-02-13 19:26:51,962][rgc][INFO] - 	Updating weights of batch 212
[2025-02-13 19:26:52,040][rgc][INFO] - Batch 212, avg loss per batch: 1.617224713282011
[2025-02-13 19:26:52,041][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 213
[2025-02-13 19:27:12,812][rgc][INFO] - 	Updating weights of batch 213
[2025-02-13 19:27:12,862][rgc][INFO] - Batch 213, avg loss per batch: 1.700338191977212
[2025-02-13 19:27:12,863][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 214
[2025-02-13 19:27:33,616][rgc][INFO] - 	Updating weights of batch 214
[2025-02-13 19:27:33,668][rgc][INFO] - Batch 214, avg loss per batch: 4.5443982880969065
[2025-02-13 19:27:33,668][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 215
[2025-02-13 19:27:54,414][rgc][INFO] - 	Updating weights of batch 215
[2025-02-13 19:27:54,463][rgc][INFO] - Batch 215, avg loss per batch: 2.9571082695029878
[2025-02-13 19:27:54,464][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 216
[2025-02-13 19:28:15,206][rgc][INFO] - 	Updating weights of batch 216
[2025-02-13 19:28:15,303][rgc][INFO] - Batch 216, avg loss per batch: 3.611393331959443
[2025-02-13 19:28:15,306][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 217
[2025-02-13 19:28:36,084][rgc][INFO] - 	Updating weights of batch 217
[2025-02-13 19:28:36,143][rgc][INFO] - Batch 217, avg loss per batch: 3.2057322201990406
[2025-02-13 19:28:36,145][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 218
[2025-02-13 19:28:56,916][rgc][INFO] - 	Updating weights of batch 218
[2025-02-13 19:28:56,986][rgc][INFO] - Batch 218, avg loss per batch: 2.8715683238467866
[2025-02-13 19:28:56,987][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 219
[2025-02-13 19:29:17,700][rgc][INFO] - 	Updating weights of batch 219
[2025-02-13 19:29:17,753][rgc][INFO] - Batch 219, avg loss per batch: 2.05490580365975
[2025-02-13 19:29:17,754][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 220
[2025-02-13 19:29:38,508][rgc][INFO] - 	Updating weights of batch 220
[2025-02-13 19:29:38,565][rgc][INFO] - Batch 220, avg loss per batch: 2.968413428591107
[2025-02-13 19:29:38,566][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 221
[2025-02-13 19:29:59,334][rgc][INFO] - 	Updating weights of batch 221
[2025-02-13 19:29:59,391][rgc][INFO] - Batch 221, avg loss per batch: 2.097151128709372
[2025-02-13 19:29:59,392][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 222
[2025-02-13 19:30:20,170][rgc][INFO] - 	Updating weights of batch 222
[2025-02-13 19:30:20,224][rgc][INFO] - Batch 222, avg loss per batch: 2.845290245345906
[2025-02-13 19:30:20,224][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 223
[2025-02-13 19:30:40,929][rgc][INFO] - 	Updating weights of batch 223
[2025-02-13 19:30:40,992][rgc][INFO] - Batch 223, avg loss per batch: 1.3162778582554118
[2025-02-13 19:30:40,993][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 224
[2025-02-13 19:31:01,755][rgc][INFO] - 	Updating weights of batch 224
[2025-02-13 19:31:01,814][rgc][INFO] - Batch 224, avg loss per batch: 2.565108811253922
[2025-02-13 19:31:01,815][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 225
[2025-02-13 19:31:22,559][rgc][INFO] - 	Updating weights of batch 225
[2025-02-13 19:31:22,617][rgc][INFO] - Batch 225, avg loss per batch: 4.871784635509023
[2025-02-13 19:31:22,618][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 226
[2025-02-13 19:31:43,364][rgc][INFO] - 	Updating weights of batch 226
[2025-02-13 19:31:43,417][rgc][INFO] - Batch 226, avg loss per batch: 1.9209370573707418
[2025-02-13 19:31:43,417][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 227
[2025-02-13 19:32:04,178][rgc][INFO] - 	Updating weights of batch 227
[2025-02-13 19:32:04,235][rgc][INFO] - Batch 227, avg loss per batch: 1.7400417359305846
[2025-02-13 19:32:04,236][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 228
[2025-02-13 19:32:24,991][rgc][INFO] - 	Updating weights of batch 228
[2025-02-13 19:32:25,045][rgc][INFO] - Batch 228, avg loss per batch: 2.221064924866791
[2025-02-13 19:32:25,046][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 229
[2025-02-13 19:32:45,769][rgc][INFO] - 	Updating weights of batch 229
[2025-02-13 19:32:45,824][rgc][INFO] - Batch 229, avg loss per batch: 3.033640109767603
[2025-02-13 19:32:45,825][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 230
[2025-02-13 19:33:06,515][rgc][INFO] - 	Updating weights of batch 230
[2025-02-13 19:33:06,569][rgc][INFO] - Batch 230, avg loss per batch: 2.6732056265568285
[2025-02-13 19:33:06,570][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 231
[2025-02-13 19:33:27,310][rgc][INFO] - 	Updating weights of batch 231
[2025-02-13 19:33:27,366][rgc][INFO] - Batch 231, avg loss per batch: 2.042221222961349
[2025-02-13 19:33:27,367][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 232
[2025-02-13 19:33:48,124][rgc][INFO] - 	Updating weights of batch 232
[2025-02-13 19:33:48,183][rgc][INFO] - Batch 232, avg loss per batch: 2.6886455641828433
[2025-02-13 19:33:48,184][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 233
[2025-02-13 19:34:08,923][rgc][INFO] - 	Updating weights of batch 233
[2025-02-13 19:34:08,979][rgc][INFO] - Batch 233, avg loss per batch: 2.922151074005179
[2025-02-13 19:34:08,980][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 234
[2025-02-13 19:34:29,736][rgc][INFO] - 	Updating weights of batch 234
[2025-02-13 19:34:29,794][rgc][INFO] - Batch 234, avg loss per batch: 3.870241371150888
[2025-02-13 19:34:29,795][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 235
[2025-02-13 19:34:50,504][rgc][INFO] - 	Updating weights of batch 235
[2025-02-13 19:34:50,560][rgc][INFO] - Batch 235, avg loss per batch: 2.220552768175456
[2025-02-13 19:34:50,560][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 236
[2025-02-13 19:35:11,264][rgc][INFO] - 	Updating weights of batch 236
[2025-02-13 19:35:11,315][rgc][INFO] - Batch 236, avg loss per batch: 1.0918379451427414
[2025-02-13 19:35:11,316][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 237
[2025-02-13 19:35:32,064][rgc][INFO] - 	Updating weights of batch 237
[2025-02-13 19:35:32,123][rgc][INFO] - Batch 237, avg loss per batch: 5.590901244400535
[2025-02-13 19:35:32,124][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 238
[2025-02-13 19:35:52,876][rgc][INFO] - 	Updating weights of batch 238
[2025-02-13 19:35:52,929][rgc][INFO] - Batch 238, avg loss per batch: 3.098056588268938
[2025-02-13 19:35:52,930][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 239
[2025-02-13 19:36:13,703][rgc][INFO] - 	Updating weights of batch 239
[2025-02-13 19:36:13,762][rgc][INFO] - Batch 239, avg loss per batch: 2.7007751597894716
[2025-02-13 19:36:13,763][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 240
[2025-02-13 19:36:34,535][rgc][INFO] - 	Updating weights of batch 240
[2025-02-13 19:36:34,593][rgc][INFO] - Batch 240, avg loss per batch: 2.879837329097641
[2025-02-13 19:36:34,593][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 241
[2025-02-13 19:36:55,300][rgc][INFO] - 	Updating weights of batch 241
[2025-02-13 19:36:55,356][rgc][INFO] - Batch 241, avg loss per batch: 3.297883576726078
[2025-02-13 19:36:55,357][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 242
[2025-02-13 19:37:16,068][rgc][INFO] - 	Updating weights of batch 242
[2025-02-13 19:37:16,125][rgc][INFO] - Batch 242, avg loss per batch: 2.5235906971147353
[2025-02-13 19:37:16,126][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 243
[2025-02-13 19:37:36,869][rgc][INFO] - 	Updating weights of batch 243
[2025-02-13 19:37:36,924][rgc][INFO] - Batch 243, avg loss per batch: 1.599709215900352
[2025-02-13 19:37:36,925][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 244
[2025-02-13 19:37:57,666][rgc][INFO] - 	Updating weights of batch 244
[2025-02-13 19:37:57,721][rgc][INFO] - Batch 244, avg loss per batch: 2.038335023734776
[2025-02-13 19:37:57,722][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 245
[2025-02-13 19:38:18,488][rgc][INFO] - 	Updating weights of batch 245
[2025-02-13 19:38:18,543][rgc][INFO] - Batch 245, avg loss per batch: 4.056816406264685
[2025-02-13 19:38:18,543][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 246
[2025-02-13 19:38:39,237][rgc][INFO] - 	Updating weights of batch 246
[2025-02-13 19:38:39,293][rgc][INFO] - Batch 246, avg loss per batch: 3.904491232716098
[2025-02-13 19:38:39,294][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 247
[2025-02-13 19:39:00,061][rgc][INFO] - 	Updating weights of batch 247
[2025-02-13 19:39:00,117][rgc][INFO] - Batch 247, avg loss per batch: 3.2218804607340368
[2025-02-13 19:39:00,119][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 248
[2025-02-13 19:39:20,827][rgc][INFO] - 	Updating weights of batch 248
[2025-02-13 19:39:20,882][rgc][INFO] - Batch 248, avg loss per batch: 2.7435424503866144
[2025-02-13 19:39:20,883][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 249
[2025-02-13 19:39:41,649][rgc][INFO] - 	Updating weights of batch 249
[2025-02-13 19:39:41,705][rgc][INFO] - Batch 249, avg loss per batch: 2.7046442726925344
[2025-02-13 19:39:41,706][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 250
[2025-02-13 19:40:02,489][rgc][INFO] - 	Updating weights of batch 250
[2025-02-13 19:40:02,544][rgc][INFO] - Batch 250, avg loss per batch: 3.6653686542856665
[2025-02-13 19:40:02,545][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 251
[2025-02-13 19:40:23,251][rgc][INFO] - 	Updating weights of batch 251
[2025-02-13 19:40:23,306][rgc][INFO] - Batch 251, avg loss per batch: 2.9930506759563964
[2025-02-13 19:40:23,307][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 252
[2025-02-13 19:40:44,061][rgc][INFO] - 	Updating weights of batch 252
[2025-02-13 19:40:44,115][rgc][INFO] - Batch 252, avg loss per batch: 3.5742023434204313
[2025-02-13 19:40:44,116][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 253
[2025-02-13 19:41:04,877][rgc][INFO] - 	Updating weights of batch 253
[2025-02-13 19:41:04,933][rgc][INFO] - Batch 253, avg loss per batch: 3.4093482026302437
[2025-02-13 19:41:04,934][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 254
[2025-02-13 19:41:25,614][rgc][INFO] - 	Updating weights of batch 254
[2025-02-13 19:41:25,666][rgc][INFO] - Batch 254, avg loss per batch: 4.077280893539352
[2025-02-13 19:41:25,667][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 255
[2025-02-13 19:41:46,411][rgc][INFO] - 	Updating weights of batch 255
[2025-02-13 19:41:46,465][rgc][INFO] - Batch 255, avg loss per batch: 1.895468084259461
[2025-02-13 19:41:46,466][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 256
[2025-02-13 19:42:07,146][rgc][INFO] - 	Updating weights of batch 256
[2025-02-13 19:42:07,201][rgc][INFO] - Batch 256, avg loss per batch: 1.7238400506397946
[2025-02-13 19:42:07,202][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 257
[2025-02-13 19:42:27,957][rgc][INFO] - 	Updating weights of batch 257
[2025-02-13 19:42:28,009][rgc][INFO] - Batch 257, avg loss per batch: 3.481971357037195
[2025-02-13 19:42:28,010][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 258
[2025-02-13 19:42:48,760][rgc][INFO] - 	Updating weights of batch 258
[2025-02-13 19:42:48,816][rgc][INFO] - Batch 258, avg loss per batch: 2.955976547026946
[2025-02-13 19:42:48,817][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 259
[2025-02-13 19:43:09,591][rgc][INFO] - 	Updating weights of batch 259
[2025-02-13 19:43:09,647][rgc][INFO] - Batch 259, avg loss per batch: 5.248263425864542
[2025-02-13 19:43:09,647][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 260
[2025-02-13 19:43:30,403][rgc][INFO] - 	Updating weights of batch 260
[2025-02-13 19:43:30,456][rgc][INFO] - Batch 260, avg loss per batch: 1.3579645142771235
[2025-02-13 19:43:30,457][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 261
[2025-02-13 19:43:51,202][rgc][INFO] - 	Updating weights of batch 261
[2025-02-13 19:43:51,256][rgc][INFO] - Batch 261, avg loss per batch: 1.8473711053719843
[2025-02-13 19:43:51,257][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 262
[2025-02-13 19:44:11,998][rgc][INFO] - 	Updating weights of batch 262
[2025-02-13 19:44:12,053][rgc][INFO] - Batch 262, avg loss per batch: 3.8605498736646977
[2025-02-13 19:44:12,054][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 263
[2025-02-13 19:44:32,804][rgc][INFO] - 	Updating weights of batch 263
[2025-02-13 19:44:32,860][rgc][INFO] - Batch 263, avg loss per batch: 2.10011652905453
[2025-02-13 19:44:32,860][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 264
[2025-02-13 19:44:53,617][rgc][INFO] - 	Updating weights of batch 264
[2025-02-13 19:44:53,672][rgc][INFO] - Batch 264, avg loss per batch: 2.0426392400485387
[2025-02-13 19:44:53,672][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 265
[2025-02-13 19:45:14,439][rgc][INFO] - 	Updating weights of batch 265
[2025-02-13 19:45:14,494][rgc][INFO] - Batch 265, avg loss per batch: 3.669012356369747
[2025-02-13 19:45:14,494][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 266
[2025-02-13 19:45:35,245][rgc][INFO] - 	Updating weights of batch 266
[2025-02-13 19:45:35,298][rgc][INFO] - Batch 266, avg loss per batch: 2.008973885177856
[2025-02-13 19:45:35,299][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 267
[2025-02-13 19:45:56,058][rgc][INFO] - 	Updating weights of batch 267
[2025-02-13 19:45:56,113][rgc][INFO] - Batch 267, avg loss per batch: 2.5935924021868937
[2025-02-13 19:45:56,114][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 268
[2025-02-13 19:46:16,865][rgc][INFO] - 	Updating weights of batch 268
[2025-02-13 19:46:16,922][rgc][INFO] - Batch 268, avg loss per batch: 3.1069526814605855
[2025-02-13 19:46:16,922][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 269
[2025-02-13 19:46:37,600][rgc][INFO] - 	Updating weights of batch 269
[2025-02-13 19:46:37,655][rgc][INFO] - Batch 269, avg loss per batch: 2.362129367634352
[2025-02-13 19:46:37,656][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 270
[2025-02-13 19:46:58,407][rgc][INFO] - 	Updating weights of batch 270
[2025-02-13 19:46:58,462][rgc][INFO] - Batch 270, avg loss per batch: 3.3349527307555036
[2025-02-13 19:46:58,463][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 271
[2025-02-13 19:47:19,211][rgc][INFO] - 	Updating weights of batch 271
[2025-02-13 19:47:19,264][rgc][INFO] - Batch 271, avg loss per batch: 3.0932563032984213
[2025-02-13 19:47:19,265][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 272
[2025-02-13 19:47:40,031][rgc][INFO] - 	Updating weights of batch 272
[2025-02-13 19:47:40,087][rgc][INFO] - Batch 272, avg loss per batch: 1.6931389803553134
[2025-02-13 19:47:40,087][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 273
[2025-02-13 19:48:00,855][rgc][INFO] - 	Updating weights of batch 273
[2025-02-13 19:48:00,912][rgc][INFO] - Batch 273, avg loss per batch: 5.460077523704627
[2025-02-13 19:48:00,914][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 274
[2025-02-13 19:48:21,655][rgc][INFO] - 	Updating weights of batch 274
[2025-02-13 19:48:21,709][rgc][INFO] - Batch 274, avg loss per batch: 1.8689489564476425
[2025-02-13 19:48:21,710][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 275
[2025-02-13 19:48:42,445][rgc][INFO] - 	Updating weights of batch 275
[2025-02-13 19:48:42,498][rgc][INFO] - Batch 275, avg loss per batch: 0.8544124233628239
[2025-02-13 19:48:42,499][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 276
[2025-02-13 19:49:03,238][rgc][INFO] - 	Updating weights of batch 276
[2025-02-13 19:49:03,291][rgc][INFO] - Batch 276, avg loss per batch: 2.675557268911868
[2025-02-13 19:49:03,292][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 277
[2025-02-13 19:49:24,060][rgc][INFO] - 	Updating weights of batch 277
[2025-02-13 19:49:24,116][rgc][INFO] - Batch 277, avg loss per batch: 3.093414827957308
[2025-02-13 19:49:24,117][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 278
[2025-02-13 19:49:44,855][rgc][INFO] - 	Updating weights of batch 278
[2025-02-13 19:49:44,910][rgc][INFO] - Batch 278, avg loss per batch: 3.332908140524632
[2025-02-13 19:49:44,910][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 279
[2025-02-13 19:50:05,680][rgc][INFO] - 	Updating weights of batch 279
[2025-02-13 19:50:05,736][rgc][INFO] - Batch 279, avg loss per batch: 1.8225647760561654
[2025-02-13 19:50:05,737][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 280
[2025-02-13 19:50:26,494][rgc][INFO] - 	Updating weights of batch 280
[2025-02-13 19:50:26,549][rgc][INFO] - Batch 280, avg loss per batch: 2.6802161418658095
[2025-02-13 19:50:26,549][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 281
[2025-02-13 19:50:47,280][rgc][INFO] - 	Updating weights of batch 281
[2025-02-13 19:50:47,336][rgc][INFO] - Batch 281, avg loss per batch: 2.007592739077631
[2025-02-13 19:50:47,336][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 282
[2025-02-13 19:51:08,085][rgc][INFO] - 	Updating weights of batch 282
[2025-02-13 19:51:08,140][rgc][INFO] - Batch 282, avg loss per batch: 1.6557304790043526
[2025-02-13 19:51:08,141][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 283
[2025-02-13 19:51:28,904][rgc][INFO] - 	Updating weights of batch 283
[2025-02-13 19:51:28,960][rgc][INFO] - Batch 283, avg loss per batch: 2.847513987220016
[2025-02-13 19:51:28,961][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 284
[2025-02-13 19:51:49,698][rgc][INFO] - 	Updating weights of batch 284
[2025-02-13 19:51:49,753][rgc][INFO] - Batch 284, avg loss per batch: 2.576005343384306
[2025-02-13 19:51:49,753][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 285
[2025-02-13 19:52:10,510][rgc][INFO] - 	Updating weights of batch 285
[2025-02-13 19:52:10,564][rgc][INFO] - Batch 285, avg loss per batch: 2.5294050318583525
[2025-02-13 19:52:10,565][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 286
[2025-02-13 19:52:31,311][rgc][INFO] - 	Updating weights of batch 286
[2025-02-13 19:52:31,366][rgc][INFO] - Batch 286, avg loss per batch: 2.4355180249157575
[2025-02-13 19:52:31,367][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 287
[2025-02-13 19:52:52,099][rgc][INFO] - 	Updating weights of batch 287
[2025-02-13 19:52:52,152][rgc][INFO] - Batch 287, avg loss per batch: 3.429080602846908
[2025-02-13 19:52:52,153][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 288
[2025-02-13 19:53:12,892][rgc][INFO] - 	Updating weights of batch 288
[2025-02-13 19:53:12,949][rgc][INFO] - Batch 288, avg loss per batch: 2.0226921164099183
[2025-02-13 19:53:12,950][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 289
[2025-02-13 19:53:33,701][rgc][INFO] - 	Updating weights of batch 289
[2025-02-13 19:53:33,756][rgc][INFO] - Batch 289, avg loss per batch: 2.964647457311137
[2025-02-13 19:53:33,757][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 290
[2025-02-13 19:53:54,518][rgc][INFO] - 	Updating weights of batch 290
[2025-02-13 19:53:54,573][rgc][INFO] - Batch 290, avg loss per batch: 3.6759617173017025
[2025-02-13 19:53:54,574][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 291
[2025-02-13 19:54:15,336][rgc][INFO] - 	Updating weights of batch 291
[2025-02-13 19:54:15,391][rgc][INFO] - Batch 291, avg loss per batch: 4.499613882583367
[2025-02-13 19:54:15,392][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 292
[2025-02-13 19:54:36,123][rgc][INFO] - 	Updating weights of batch 292
[2025-02-13 19:54:36,178][rgc][INFO] - Batch 292, avg loss per batch: 4.48963958765455
[2025-02-13 19:54:36,179][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 293
[2025-02-13 19:54:56,940][rgc][INFO] - 	Updating weights of batch 293
[2025-02-13 19:54:56,995][rgc][INFO] - Batch 293, avg loss per batch: 2.4855868344239456
[2025-02-13 19:54:56,995][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 294
[2025-02-13 19:55:17,729][rgc][INFO] - 	Updating weights of batch 294
[2025-02-13 19:55:17,784][rgc][INFO] - Batch 294, avg loss per batch: 3.5816785754860376
[2025-02-13 19:55:17,785][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 295
[2025-02-13 19:55:38,524][rgc][INFO] - 	Updating weights of batch 295
[2025-02-13 19:55:38,579][rgc][INFO] - Batch 295, avg loss per batch: 2.639286500143149
[2025-02-13 19:55:38,580][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 296
[2025-02-13 19:55:59,338][rgc][INFO] - 	Updating weights of batch 296
[2025-02-13 19:55:59,393][rgc][INFO] - Batch 296, avg loss per batch: 2.6595636479221865
[2025-02-13 19:55:59,394][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 297
[2025-02-13 19:56:20,104][rgc][INFO] - 	Updating weights of batch 297
[2025-02-13 19:56:20,160][rgc][INFO] - Batch 297, avg loss per batch: 2.685834215978814
[2025-02-13 19:56:20,161][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 298
[2025-02-13 19:56:40,935][rgc][INFO] - 	Updating weights of batch 298
[2025-02-13 19:56:40,992][rgc][INFO] - Batch 298, avg loss per batch: 4.568731643018473
[2025-02-13 19:56:40,992][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 299
[2025-02-13 19:57:01,743][rgc][INFO] - 	Updating weights of batch 299
[2025-02-13 19:57:01,799][rgc][INFO] - Batch 299, avg loss per batch: 4.4796012596559365
[2025-02-13 19:57:13,372][rgc][INFO] - AVG rho on val data: 0.2695539253162084
[2025-02-13 19:57:13,372][rgc][INFO] - AVG mae on val data: 0.525571517078049
[2025-02-13 19:57:24,550][rgc][INFO] - AVG rho on test data: 0.3077919003266862
[2025-02-13 19:57:24,550][rgc][INFO] - AVG mae on test data: 0.5623458379903081
[2025-02-13 19:57:37,412][rgc][INFO] - AVG rho on train data: 0.21875874082319052
[2025-02-13 19:57:37,413][rgc][INFO] - AVG mae on train data: 0.5461361730091593
[2025-02-13 19:57:37,413][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 19:57:37,415][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 300
[2025-02-13 19:57:58,134][rgc][INFO] - 	Updating weights of batch 300
[2025-02-13 19:57:58,192][rgc][INFO] - Batch 300, avg loss per batch: 2.7797077655203783
[2025-02-13 19:57:58,193][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 301
[2025-02-13 19:58:18,960][rgc][INFO] - 	Updating weights of batch 301
[2025-02-13 19:58:19,020][rgc][INFO] - Batch 301, avg loss per batch: 2.222651847243292
[2025-02-13 19:58:19,021][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 302
[2025-02-13 19:58:39,772][rgc][INFO] - 	Updating weights of batch 302
[2025-02-13 19:58:39,829][rgc][INFO] - Batch 302, avg loss per batch: 4.374383690508952
[2025-02-13 19:58:39,830][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 303
[2025-02-13 19:59:00,602][rgc][INFO] - 	Updating weights of batch 303
[2025-02-13 19:59:00,660][rgc][INFO] - Batch 303, avg loss per batch: 2.327830323346148
[2025-02-13 19:59:00,661][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 304
[2025-02-13 19:59:21,442][rgc][INFO] - 	Updating weights of batch 304
[2025-02-13 19:59:21,498][rgc][INFO] - Batch 304, avg loss per batch: 3.3689553318227006
[2025-02-13 19:59:21,499][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 305
[2025-02-13 19:59:42,276][rgc][INFO] - 	Updating weights of batch 305
[2025-02-13 19:59:42,333][rgc][INFO] - Batch 305, avg loss per batch: 3.077620284132406
[2025-02-13 19:59:42,334][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 306
[2025-02-13 20:00:03,081][rgc][INFO] - 	Updating weights of batch 306
[2025-02-13 20:00:03,138][rgc][INFO] - Batch 306, avg loss per batch: 3.2658997498894067
[2025-02-13 20:00:03,139][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 307
[2025-02-13 20:00:23,891][rgc][INFO] - 	Updating weights of batch 307
[2025-02-13 20:00:23,949][rgc][INFO] - Batch 307, avg loss per batch: 2.2849576459500094
[2025-02-13 20:00:23,950][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 308
[2025-02-13 20:00:44,708][rgc][INFO] - 	Updating weights of batch 308
[2025-02-13 20:00:44,765][rgc][INFO] - Batch 308, avg loss per batch: 2.609269319606771
[2025-02-13 20:00:44,766][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 309
[2025-02-13 20:01:05,525][rgc][INFO] - 	Updating weights of batch 309
[2025-02-13 20:01:05,581][rgc][INFO] - Batch 309, avg loss per batch: 2.1570206595972077
[2025-02-13 20:01:05,581][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 310
[2025-02-13 20:01:26,357][rgc][INFO] - 	Updating weights of batch 310
[2025-02-13 20:01:26,415][rgc][INFO] - Batch 310, avg loss per batch: 3.64816577705852
[2025-02-13 20:01:26,416][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 311
[2025-02-13 20:01:47,162][rgc][INFO] - 	Updating weights of batch 311
[2025-02-13 20:01:47,221][rgc][INFO] - Batch 311, avg loss per batch: 2.6766950124916584
[2025-02-13 20:01:47,222][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 312
[2025-02-13 20:02:07,975][rgc][INFO] - 	Updating weights of batch 312
[2025-02-13 20:02:08,032][rgc][INFO] - Batch 312, avg loss per batch: 1.0513501798385514
[2025-02-13 20:02:08,033][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 313
[2025-02-13 20:02:28,732][rgc][INFO] - 	Updating weights of batch 313
[2025-02-13 20:02:28,788][rgc][INFO] - Batch 313, avg loss per batch: 1.3259554999571024
[2025-02-13 20:02:28,789][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 314
[2025-02-13 20:02:49,491][rgc][INFO] - 	Updating weights of batch 314
[2025-02-13 20:02:49,545][rgc][INFO] - Batch 314, avg loss per batch: 2.210368975319997
[2025-02-13 20:02:49,546][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 315
[2025-02-13 20:03:10,306][rgc][INFO] - 	Updating weights of batch 315
[2025-02-13 20:03:10,361][rgc][INFO] - Batch 315, avg loss per batch: 0.39095571078403546
[2025-02-13 20:03:10,362][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 316
[2025-02-13 20:03:31,068][rgc][INFO] - 	Updating weights of batch 316
[2025-02-13 20:03:31,126][rgc][INFO] - Batch 316, avg loss per batch: 3.1101465350670474
[2025-02-13 20:03:31,127][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 317
[2025-02-13 20:03:51,892][rgc][INFO] - 	Updating weights of batch 317
[2025-02-13 20:03:51,944][rgc][INFO] - Batch 317, avg loss per batch: 2.0859500312307078
[2025-02-13 20:03:51,945][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 318
[2025-02-13 20:04:12,682][rgc][INFO] - 	Updating weights of batch 318
[2025-02-13 20:04:12,738][rgc][INFO] - Batch 318, avg loss per batch: 1.9347043350973907
[2025-02-13 20:04:12,739][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 319
[2025-02-13 20:04:33,490][rgc][INFO] - 	Updating weights of batch 319
[2025-02-13 20:04:33,546][rgc][INFO] - Batch 319, avg loss per batch: 2.5565272412760076
[2025-02-13 20:04:33,547][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 320
[2025-02-13 20:04:54,295][rgc][INFO] - 	Updating weights of batch 320
[2025-02-13 20:04:54,350][rgc][INFO] - Batch 320, avg loss per batch: 4.055237811363614
[2025-02-13 20:04:54,351][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 321
[2025-02-13 20:05:15,061][rgc][INFO] - 	Updating weights of batch 321
[2025-02-13 20:05:15,118][rgc][INFO] - Batch 321, avg loss per batch: 2.2833718009962376
[2025-02-13 20:05:15,118][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 322
[2025-02-13 20:05:35,876][rgc][INFO] - 	Updating weights of batch 322
[2025-02-13 20:05:35,929][rgc][INFO] - Batch 322, avg loss per batch: 2.5937626263207725
[2025-02-13 20:05:35,930][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 323
[2025-02-13 20:05:56,682][rgc][INFO] - 	Updating weights of batch 323
[2025-02-13 20:05:56,736][rgc][INFO] - Batch 323, avg loss per batch: 2.679959976836096
[2025-02-13 20:05:56,736][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 324
[2025-02-13 20:06:17,418][rgc][INFO] - 	Updating weights of batch 324
[2025-02-13 20:06:17,474][rgc][INFO] - Batch 324, avg loss per batch: 2.2769967144522054
[2025-02-13 20:06:17,475][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 325
[2025-02-13 20:06:38,236][rgc][INFO] - 	Updating weights of batch 325
[2025-02-13 20:06:38,292][rgc][INFO] - Batch 325, avg loss per batch: 1.4972638469686266
[2025-02-13 20:06:38,293][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 326
[2025-02-13 20:06:59,057][rgc][INFO] - 	Updating weights of batch 326
[2025-02-13 20:06:59,114][rgc][INFO] - Batch 326, avg loss per batch: 2.4685510817815723
[2025-02-13 20:06:59,114][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 327
[2025-02-13 20:07:19,876][rgc][INFO] - 	Updating weights of batch 327
[2025-02-13 20:07:19,929][rgc][INFO] - Batch 327, avg loss per batch: 2.1751293298854626
[2025-02-13 20:07:19,930][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 328
[2025-02-13 20:07:40,720][rgc][INFO] - 	Updating weights of batch 328
[2025-02-13 20:07:40,776][rgc][INFO] - Batch 328, avg loss per batch: 3.0479981750334884
[2025-02-13 20:07:40,776][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 329
[2025-02-13 20:08:01,473][rgc][INFO] - 	Updating weights of batch 329
[2025-02-13 20:08:01,529][rgc][INFO] - Batch 329, avg loss per batch: 3.991242427129378
[2025-02-13 20:08:01,530][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 330
[2025-02-13 20:08:22,281][rgc][INFO] - 	Updating weights of batch 330
[2025-02-13 20:08:22,336][rgc][INFO] - Batch 330, avg loss per batch: 1.8313725453951037
[2025-02-13 20:08:22,336][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 331
[2025-02-13 20:08:43,089][rgc][INFO] - 	Updating weights of batch 331
[2025-02-13 20:08:43,143][rgc][INFO] - Batch 331, avg loss per batch: 2.3647749338398443
[2025-02-13 20:08:43,144][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 332
[2025-02-13 20:09:03,897][rgc][INFO] - 	Updating weights of batch 332
[2025-02-13 20:09:03,950][rgc][INFO] - Batch 332, avg loss per batch: 2.577202927334247
[2025-02-13 20:09:03,951][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 333
[2025-02-13 20:09:24,696][rgc][INFO] - 	Updating weights of batch 333
[2025-02-13 20:09:24,751][rgc][INFO] - Batch 333, avg loss per batch: 1.8910141911966147
[2025-02-13 20:09:24,751][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 334
[2025-02-13 20:09:45,493][rgc][INFO] - 	Updating weights of batch 334
[2025-02-13 20:09:45,547][rgc][INFO] - Batch 334, avg loss per batch: 1.9552504920402627
[2025-02-13 20:09:45,548][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 335
[2025-02-13 20:10:06,304][rgc][INFO] - 	Updating weights of batch 335
[2025-02-13 20:10:06,361][rgc][INFO] - Batch 335, avg loss per batch: 2.403049452131154
[2025-02-13 20:10:06,362][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 336
[2025-02-13 20:10:27,069][rgc][INFO] - 	Updating weights of batch 336
[2025-02-13 20:10:27,124][rgc][INFO] - Batch 336, avg loss per batch: 2.8069701132306677
[2025-02-13 20:10:27,124][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 337
[2025-02-13 20:10:47,886][rgc][INFO] - 	Updating weights of batch 337
[2025-02-13 20:10:47,943][rgc][INFO] - Batch 337, avg loss per batch: 1.7503855227952048
[2025-02-13 20:10:47,943][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 338
[2025-02-13 20:11:08,676][rgc][INFO] - 	Updating weights of batch 338
[2025-02-13 20:11:08,730][rgc][INFO] - Batch 338, avg loss per batch: 1.7176175470945507
[2025-02-13 20:11:08,731][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 339
[2025-02-13 20:11:29,500][rgc][INFO] - 	Updating weights of batch 339
[2025-02-13 20:11:29,555][rgc][INFO] - Batch 339, avg loss per batch: 4.773302088276669
[2025-02-13 20:11:29,556][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 340
[2025-02-13 20:11:50,313][rgc][INFO] - 	Updating weights of batch 340
[2025-02-13 20:11:50,367][rgc][INFO] - Batch 340, avg loss per batch: 1.4498637860823174
[2025-02-13 20:11:50,367][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 341
[2025-02-13 20:12:11,074][rgc][INFO] - 	Updating weights of batch 341
[2025-02-13 20:12:11,130][rgc][INFO] - Batch 341, avg loss per batch: 2.7664726866298452
[2025-02-13 20:12:11,131][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 342
[2025-02-13 20:12:31,902][rgc][INFO] - 	Updating weights of batch 342
[2025-02-13 20:12:31,957][rgc][INFO] - Batch 342, avg loss per batch: 2.7373354293813055
[2025-02-13 20:12:31,958][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 343
[2025-02-13 20:12:52,708][rgc][INFO] - 	Updating weights of batch 343
[2025-02-13 20:12:52,760][rgc][INFO] - Batch 343, avg loss per batch: 2.5055157893919504
[2025-02-13 20:12:52,761][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 344
[2025-02-13 20:13:13,445][rgc][INFO] - 	Updating weights of batch 344
[2025-02-13 20:13:13,493][rgc][INFO] - Batch 344, avg loss per batch: 2.7239575832872203
[2025-02-13 20:13:13,493][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 345
[2025-02-13 20:13:34,233][rgc][INFO] - 	Updating weights of batch 345
[2025-02-13 20:13:34,288][rgc][INFO] - Batch 345, avg loss per batch: 2.047713564618349
[2025-02-13 20:13:34,289][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 346
[2025-02-13 20:13:55,027][rgc][INFO] - 	Updating weights of batch 346
[2025-02-13 20:13:55,083][rgc][INFO] - Batch 346, avg loss per batch: 1.5723669736827348
[2025-02-13 20:13:55,084][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 347
[2025-02-13 20:14:15,825][rgc][INFO] - 	Updating weights of batch 347
[2025-02-13 20:14:15,879][rgc][INFO] - Batch 347, avg loss per batch: 2.4775512970250086
[2025-02-13 20:14:15,880][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 348
[2025-02-13 20:14:36,654][rgc][INFO] - 	Updating weights of batch 348
[2025-02-13 20:14:36,708][rgc][INFO] - Batch 348, avg loss per batch: 3.8202241241161072
[2025-02-13 20:14:36,709][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 349
[2025-02-13 20:14:57,451][rgc][INFO] - 	Updating weights of batch 349
[2025-02-13 20:14:57,505][rgc][INFO] - Batch 349, avg loss per batch: 2.1789575032452855
[2025-02-13 20:14:57,505][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 350
[2025-02-13 20:15:18,185][rgc][INFO] - 	Updating weights of batch 350
[2025-02-13 20:15:18,242][rgc][INFO] - Batch 350, avg loss per batch: 0.8639463754370482
[2025-02-13 20:15:18,243][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 351
[2025-02-13 20:15:38,991][rgc][INFO] - 	Updating weights of batch 351
[2025-02-13 20:15:39,047][rgc][INFO] - Batch 351, avg loss per batch: 2.1714779230429695
[2025-02-13 20:15:39,048][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 352
[2025-02-13 20:15:59,728][rgc][INFO] - 	Updating weights of batch 352
[2025-02-13 20:15:59,784][rgc][INFO] - Batch 352, avg loss per batch: 1.7012816166927478
[2025-02-13 20:15:59,785][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 353
[2025-02-13 20:16:20,543][rgc][INFO] - 	Updating weights of batch 353
[2025-02-13 20:16:20,597][rgc][INFO] - Batch 353, avg loss per batch: 2.314484085855171
[2025-02-13 20:16:20,598][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 354
[2025-02-13 20:16:41,367][rgc][INFO] - 	Updating weights of batch 354
[2025-02-13 20:16:41,424][rgc][INFO] - Batch 354, avg loss per batch: 2.3039842267165227
[2025-02-13 20:16:41,425][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 355
[2025-02-13 20:17:02,174][rgc][INFO] - 	Updating weights of batch 355
[2025-02-13 20:17:02,230][rgc][INFO] - Batch 355, avg loss per batch: 2.5615759225829016
[2025-02-13 20:17:02,231][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 356
[2025-02-13 20:17:22,911][rgc][INFO] - 	Updating weights of batch 356
[2025-02-13 20:17:22,968][rgc][INFO] - Batch 356, avg loss per batch: 3.5722221757773855
[2025-02-13 20:17:22,968][rgc][INFO] - 	Applying batch grad function of epoch 4 and batch 357
[2025-02-13 20:17:43,709][rgc][INFO] - 	Updating weights of batch 357
[2025-02-13 20:17:43,767][rgc][INFO] - Batch 357, avg loss per batch: 2.240887100324871
[2025-02-13 20:17:43,777][rgc][INFO] - ================= Epoch 4, loss: 981.122473564731 ===============
[2025-02-13 20:17:43,777][rgc][INFO] - Visualizing histograms
[2025-02-13 20:18:06,582][rgc][INFO] - AVG rho on val data: 0.2730611096632384
[2025-02-13 20:18:06,583][rgc][INFO] - AVG Mean Absolute Error on val data: 0.5249131389919658
[2025-02-13 20:18:17,756][rgc][INFO] - AVG rho on test data: 0.3088826285864167
[2025-02-13 20:18:17,756][rgc][INFO] - AVG Mean Absolute Error on test data: 0.5618577344207891
[2025-02-13 20:18:30,624][rgc][INFO] - AVG rho on train data: 0.2209159347905835
[2025-02-13 20:18:30,624][rgc][INFO] - AVG Mean Absolute Error on train data: 0.5464464986150434
[2025-02-13 20:18:30,626][rgc][INFO] - Current best rhos: train 0.23085750998819768, val 0.29143604268683954, test 0.3107578635750144
[2025-02-13 20:18:30,627][rgc][INFO] - Creating Receptive Field Figures ... 
[2025-02-13 20:19:30,201][rgc][INFO] - Finished
